{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Event Driven Reference Architecture Abstract The content of this repository was the source of the event-driven reference architecture in the IBM Garage architecture center . This git repository is maintained on a weekly basis and includes more content not yet formally published to IBM sites. As we are implementing the end to end solution we are updating this main git repository to keep best practices accurate. When content is stable it moves to the IBM Garage architecture center . The modern digital business works in real time; it informs interested parties of things of interest when they happen, it makes sense of, and derives insight from an ever-growing number of sources. It learns, predicts and is intelligent -- it is by nature Event Driven. Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. This architectural pattern can be applied to the systems that transmit events among loosely coupled software components and services. The business value is that you can easily extend EDA with new components that are ready to produce or consume events that already are in place in the overall system. While events are more visible, new business capabilities are addressed, like applying insight in real time when events occur. EDA helps also to improve continous availability of microservice architecture. Events are a way of capturing a statement of fact. Events occur in a continuous stream as things happen in the real and digital worlds. By taking advntage of this continous stream, applications can not only react in real time, but also reason about the future based upon what has happened in the past. For enterprise IT teams, embracing event driven development is foundational to the next generation of digital business applications. IT teams will need to be able to design, develop, deploy and operate event driven solutions, in cloud native styles. While event driven architectures and reactive programming models are not new concepts, the move to Cloud Native architectures with Microservices, Container based workloads and \"server-less\" computing allow us to revisit event driven approaches in this Cloud Native context. Indeed, we could think of event driven as extending the Resilience, Agility and Scale characteristics of \"Cloud Native\" to also be Reactive and Responsive. Two aspects of a cloud-native architecture are essential to developing an event driven architecture: Microservices - These provide the loosely coupled application architecture which enables deployment in highly distributed patterns for Resilience, Agility and Scale. Cloud Native platforms with Containers and \"Serverless deployments\" - These provide the application platform and tools which realize the Resilience, Agility and Scale promise of the microservices architectures. An Event Driven Architecture should provide the following essential event capabilities to the Cloud Native Platform. Being able to communicate and persist events. Being able to take direct action on events. Processing event streams to derive real time insight/intelligence. Providing communication for event driven microservices. This repository represents the root of related content about the Cloud Native Event Driven Architecture. It provides guidance for how to approach the design of event driven solutions, introduces the Cloud Native Event Driven reference architecture and provides reusable coding assets for implementation in a cloud native environment. Target audiences While the content of this repository is mostly technical in nature and is intended for a technical audience, it also introduces methods such as Event Storming which would be used with business leaders to identify key business domain events and actions. You may find it useful to share this information with your business leaders before engaging them in such activities. At a high level this is what you should expect to learn by working through this repository and the related examples. As an architect, you will understand how the event driven architecture provides capabilities which support development of event driven solutions. As a developer, you will understand how to develop event driven applications and develop analytics based on event streams. As a project manager, you may understand all the artifacts which may be required for an event driven solution. The related repositories provide sample code and best practices which you may want to reuse during your future implementations. The reference architecture has been designed to be portable and applicable to Public Cloud, Hybrid cloud and across multiple clouds. Examples given are directly deployable in IBM Public Cloud and with IBM Cloud Private. Concepts Before we start looking at the details of the Event Driven Architecture we will quickly examine the core concepts of being event driven: Events Event streams Commands Loose Coupling Cohesion Read more ... Event sources The modern digital business is driven by events. Events come into the business and events likewise need to be pushed outside of the business. For our Cloud Native Event Driven Architecture we consider event sources to be all of those things which may generate events which are of interest to the business. This could include, events coming from IoT devices, mobile apps, web apps, database triggers or microservices. In general terms, an Event Source , or event producer is any component capable of creating an event notification and publishing it to the event backbone, but let look at some specific types of producer to better understand the opportunity with event driven. Read more ... Event Backbone The Event Backbone is the center of the Event Driven Architecture providing the event communication and persistence layer with the following capabilities: Pub/Sub style event communication between event producers and consumers. An Event Log to persist events for a given period of time. Replay of events. Subscriptions from multiple consumers. Read more ... Taking an Action After an event has occurred is one of the fundamental operations for any event driven solution. IBM Cloud Functions provides a simplified event driven programming model, enabling developers to simply write the action code in the language of their choice and have Cloud Functions manage the computation workload. With this simplified model: A business event of interest would be published to the event backbone. The action for the event would be written as a Cloud Functions action. Cloud Functions would be configured to subscribe to the event and use it as a trigger to start the action . Cloud functions manages the start-up of all required compute resources. Cloud functions managed execution of the action code. Cloud functions manages the shut-down of the computation resources when the action is complete. Read more ... Real time insights/intelligence Processing continuous streaming events to derive real time insights/intelligence is an essential element of modern event driven solutions. Specialized streaming analytics engines provide the means to run stateful analytical and complex event processing workloads across multiple real time event streams while maintaining low latency processing times. Including these engines as part of the Event Driven Architecture enables: Analysis and understanding of real time event streams Extracting real time event data from the stream so that Data Scientists can understand and derive Machine Learning models Running analytical processes, Machine Learning models in line in real time against the event stream. Matching of complex event patterns across multiple streams and time windows to make decisions and take actions Read more ... Event Managed State While the prime focus for an event driven architecture is for processing events, there are cases where we need to persist events for post processing and queries by other applications. With the event backbone we have a builtin Event Log which provides the means to store and reply events published to the backbone, however when we consider the full scope of Event Driven solutions there are other use cases and types of store that we should support. This includes: Event Stores optimized for analytics Event Sourcing as a pattern for recording state changes and updates across distributed systems Command Query Response Separation (CQRS) as an optimization which separates updates and reads across different stores Read more ... Event Driven Cloud Native Apps (Microservices) The event driven architecture must also reach across into our application platform. Developers will build applications which interact with events and are themselves event driven, that is they will both produce and consume events via the event backbone. In this context we can view the Event Backbone as being part of the microservices mesh, providing the Pub/Sub communication between microservices, and therefore enabling the support of loosely coupled event driven microservices. Read more ... Event Storming When it comes to the design of event driven solutions there are some additional methods which can be utilized to help understand the business events and actions that make up a business. Event Storming , is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. It focuses on communication between product owner, domain experts and developers. Insights Storming , is an extension to the event storming workshop and encourages a forward-looking approach to consider the insights, (predictive models) which would make a difference to the business when we look at actions for key business events. What if instead of seeing a system has failed event (events are something that has happened) we could see a predictive or derived event, the system will fail in 3 days , we could take preventative actions. Read more about the Event Storming Methodology Applicability of an EDA EDAs are typically not used for distributed transactional processing because this can lead to increased coupling and performance degradation. But as seen in previous section, using a message backbone to support communication between microservices to ensure data consistency is a viable pattern. The use of EDAs for batch processing is also restricted to cases where the potential for parallelizing batch workloads exist. Most often EDAs are used for event driven applications that require near-realtime situation awareness and decision making. Sample EDA Applications Container shipment solution : this solution presents real time analytics, pub-sub architecture pattern and micro-service communication on Kafka.","title":"Home"},{"location":"#event-driven-reference-architecture","text":"Abstract The content of this repository was the source of the event-driven reference architecture in the IBM Garage architecture center . This git repository is maintained on a weekly basis and includes more content not yet formally published to IBM sites. As we are implementing the end to end solution we are updating this main git repository to keep best practices accurate. When content is stable it moves to the IBM Garage architecture center . The modern digital business works in real time; it informs interested parties of things of interest when they happen, it makes sense of, and derives insight from an ever-growing number of sources. It learns, predicts and is intelligent -- it is by nature Event Driven. Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. This architectural pattern can be applied to the systems that transmit events among loosely coupled software components and services. The business value is that you can easily extend EDA with new components that are ready to produce or consume events that already are in place in the overall system. While events are more visible, new business capabilities are addressed, like applying insight in real time when events occur. EDA helps also to improve continous availability of microservice architecture. Events are a way of capturing a statement of fact. Events occur in a continuous stream as things happen in the real and digital worlds. By taking advntage of this continous stream, applications can not only react in real time, but also reason about the future based upon what has happened in the past. For enterprise IT teams, embracing event driven development is foundational to the next generation of digital business applications. IT teams will need to be able to design, develop, deploy and operate event driven solutions, in cloud native styles. While event driven architectures and reactive programming models are not new concepts, the move to Cloud Native architectures with Microservices, Container based workloads and \"server-less\" computing allow us to revisit event driven approaches in this Cloud Native context. Indeed, we could think of event driven as extending the Resilience, Agility and Scale characteristics of \"Cloud Native\" to also be Reactive and Responsive. Two aspects of a cloud-native architecture are essential to developing an event driven architecture: Microservices - These provide the loosely coupled application architecture which enables deployment in highly distributed patterns for Resilience, Agility and Scale. Cloud Native platforms with Containers and \"Serverless deployments\" - These provide the application platform and tools which realize the Resilience, Agility and Scale promise of the microservices architectures. An Event Driven Architecture should provide the following essential event capabilities to the Cloud Native Platform. Being able to communicate and persist events. Being able to take direct action on events. Processing event streams to derive real time insight/intelligence. Providing communication for event driven microservices. This repository represents the root of related content about the Cloud Native Event Driven Architecture. It provides guidance for how to approach the design of event driven solutions, introduces the Cloud Native Event Driven reference architecture and provides reusable coding assets for implementation in a cloud native environment.","title":"Event Driven Reference Architecture"},{"location":"#target-audiences","text":"While the content of this repository is mostly technical in nature and is intended for a technical audience, it also introduces methods such as Event Storming which would be used with business leaders to identify key business domain events and actions. You may find it useful to share this information with your business leaders before engaging them in such activities. At a high level this is what you should expect to learn by working through this repository and the related examples. As an architect, you will understand how the event driven architecture provides capabilities which support development of event driven solutions. As a developer, you will understand how to develop event driven applications and develop analytics based on event streams. As a project manager, you may understand all the artifacts which may be required for an event driven solution. The related repositories provide sample code and best practices which you may want to reuse during your future implementations. The reference architecture has been designed to be portable and applicable to Public Cloud, Hybrid cloud and across multiple clouds. Examples given are directly deployable in IBM Public Cloud and with IBM Cloud Private.","title":"Target audiences"},{"location":"#concepts","text":"Before we start looking at the details of the Event Driven Architecture we will quickly examine the core concepts of being event driven: Events Event streams Commands Loose Coupling Cohesion Read more ...","title":"Concepts"},{"location":"#event-sources","text":"The modern digital business is driven by events. Events come into the business and events likewise need to be pushed outside of the business. For our Cloud Native Event Driven Architecture we consider event sources to be all of those things which may generate events which are of interest to the business. This could include, events coming from IoT devices, mobile apps, web apps, database triggers or microservices. In general terms, an Event Source , or event producer is any component capable of creating an event notification and publishing it to the event backbone, but let look at some specific types of producer to better understand the opportunity with event driven. Read more ...","title":"Event sources"},{"location":"#event-backbone","text":"The Event Backbone is the center of the Event Driven Architecture providing the event communication and persistence layer with the following capabilities: Pub/Sub style event communication between event producers and consumers. An Event Log to persist events for a given period of time. Replay of events. Subscriptions from multiple consumers. Read more ...","title":"Event Backbone"},{"location":"#taking-an-action","text":"After an event has occurred is one of the fundamental operations for any event driven solution. IBM Cloud Functions provides a simplified event driven programming model, enabling developers to simply write the action code in the language of their choice and have Cloud Functions manage the computation workload. With this simplified model: A business event of interest would be published to the event backbone. The action for the event would be written as a Cloud Functions action. Cloud Functions would be configured to subscribe to the event and use it as a trigger to start the action . Cloud functions manages the start-up of all required compute resources. Cloud functions managed execution of the action code. Cloud functions manages the shut-down of the computation resources when the action is complete. Read more ...","title":"Taking an Action"},{"location":"#real-time-insightsintelligence","text":"Processing continuous streaming events to derive real time insights/intelligence is an essential element of modern event driven solutions. Specialized streaming analytics engines provide the means to run stateful analytical and complex event processing workloads across multiple real time event streams while maintaining low latency processing times. Including these engines as part of the Event Driven Architecture enables: Analysis and understanding of real time event streams Extracting real time event data from the stream so that Data Scientists can understand and derive Machine Learning models Running analytical processes, Machine Learning models in line in real time against the event stream. Matching of complex event patterns across multiple streams and time windows to make decisions and take actions Read more ...","title":"Real time insights/intelligence"},{"location":"#event-managed-state","text":"While the prime focus for an event driven architecture is for processing events, there are cases where we need to persist events for post processing and queries by other applications. With the event backbone we have a builtin Event Log which provides the means to store and reply events published to the backbone, however when we consider the full scope of Event Driven solutions there are other use cases and types of store that we should support. This includes: Event Stores optimized for analytics Event Sourcing as a pattern for recording state changes and updates across distributed systems Command Query Response Separation (CQRS) as an optimization which separates updates and reads across different stores Read more ...","title":"Event Managed State"},{"location":"#event-driven-cloud-native-apps-microservices","text":"The event driven architecture must also reach across into our application platform. Developers will build applications which interact with events and are themselves event driven, that is they will both produce and consume events via the event backbone. In this context we can view the Event Backbone as being part of the microservices mesh, providing the Pub/Sub communication between microservices, and therefore enabling the support of loosely coupled event driven microservices. Read more ...","title":"Event Driven Cloud Native Apps (Microservices)"},{"location":"#event-storming","text":"When it comes to the design of event driven solutions there are some additional methods which can be utilized to help understand the business events and actions that make up a business. Event Storming , is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. It focuses on communication between product owner, domain experts and developers. Insights Storming , is an extension to the event storming workshop and encourages a forward-looking approach to consider the insights, (predictive models) which would make a difference to the business when we look at actions for key business events. What if instead of seeing a system has failed event (events are something that has happened) we could see a predictive or derived event, the system will fail in 3 days , we could take preventative actions. Read more about the Event Storming Methodology","title":"Event Storming"},{"location":"#applicability-of-an-eda","text":"EDAs are typically not used for distributed transactional processing because this can lead to increased coupling and performance degradation. But as seen in previous section, using a message backbone to support communication between microservices to ensure data consistency is a viable pattern. The use of EDAs for batch processing is also restricted to cases where the potential for parallelizing batch workloads exist. Most often EDAs are used for event driven applications that require near-realtime situation awareness and decision making.","title":"Applicability of an EDA"},{"location":"#sample-eda-applications","text":"Container shipment solution : this solution presents real time analytics, pub-sub architecture pattern and micro-service communication on Kafka.","title":"Sample EDA Applications"},{"location":"architecture/","text":"Reference Architecture We defined the starting point for a Cloud Native Event Driven Architecture to be that it supports at least the following important capabilities: Being able to communicate and persist events. Being able to take direct action on events. Processing streams of events to derive real time insight/intelligence. Providing communication mechanism between event driven microservices and functions. With an event backbone providing the connectivity between the capabilities, we can visualize a reference Event Driven Architecture as below: Where: Event sources : generates events and event streams from sources such as IoT devices, web app, mobile app, microservices\u2026 IBM Event Streams : Provides an Event Backbone supporting Pub/Sub communication, an event log, and simple event stream processing based on Apache Kafka . IBM Cloud Functions : Provides a simplified programming model to take action on an event through a \"serverless\" function-based compute model. Streaming Analytics : Provides continuous ingest and analytical processing across multiple event streams. Decision Server Insights: Provides the means to take action on events and event streams through business rules. Event Stores: Provide optimized persistence (data stores), for event sourcing, Command Query Response Separation (CQRS) and analytical use cases. Event Driven Microservices : Applications that run as serverless functions or containerized workloads which are connected via pub/sub event communication through the event backbone. Extended Architecture The event-driven reference architecture provides the framework to support event-driven applications and solutions. The extended architecture provides the connections for: Integration with legacy apps and data resources Integration with analytics or machine learning to derive real-time insights The diagram below shows how these capabilities fit together to form an extended event-driven architecture. In 7. the AI workbench includes tools to do data analysis and visualization, build training and test sets from any datasource and in particular Event Store, and develop models. Models are pushed to streaming analytics component. Integration with analytics and machine learning The extended architecture extends the basic EDA reference architecture with concepts showing how data science, artificial intelligence and machine learning can be incorporated into an event-driven solution. The starting point for data scientists to be able to derive machine learning models or analyze data for trends and behaviors is the existence of the data in a form that they can be consumed. For real-time intelligent solutions, data scientists typically inspect event histories and decision or action records from a system. Then, they reduce this data to some simplified model that scores new event data as it arrives. Getting the data for the data scientist: With real-time event streams, the challenge is in handling unbounded data or a continuous flow of events. To make this consumable for the data scientist you need to capture the relevant data and store it so that it can be pulled into the analysis and model-building process as required. Following our event-driven reference architecture the event stream would be a Kafka topic on the event backbone. From here there are two possibilities for making that event data available and consumable to the data scientist: The event stream or event log can be accessed directly through Kafka and pulled into the analysis process The event stream can be pre-processed by the streaming analytics system and stored for future use in the analysis process. You have a choice of store type to use. Within public IBM cloud object storage Cloud Object Store can be used as a cost-effective historical store. Both approaches are valid, pre-processing through streaming analytics provides opportunity for greater manipulation of the data, or storing data over time windows for complex event processing. However, the more interesting distinction is where you use a predictive (ML model) to score arriving events or stream data in real time. In this case you may use streaming analytics to extract and save the event data for analysis, model building, and model training and also for scoring (executing) a derived model in line in the real time against arriving event data. The event and decision or action data is made available in cloud object storage for model building through streaming analytics. Models may be developed by tuning and parameter fitting, standard form fitting, classification techniques, and text analytics methods. Increasingly artificial intelligence (AI) and machine learning (ML) frameworks are used to discover and train useful predictive models as an alternative to parameterizing existing model types manually. These techniques lead to process and data flows where the predictive model is trained offline using event histories from the event and the decision or action store possibly augmented with some supervisory outcome labelling, as illustrated by the paths from the Event Backbone and Stream Processing store into Learn/Analyze . A model trained in this way includes some \u201cscoring\u201d API that can be invoked with fresh event data to generate a model-based prediction for future behavior and event properties of that specific context. The scoring function is then easily reincorporated into the streaming analytics processing to generate predictions and insights. These combined techniques can lead to the creation of real-time intelligent applications: Event-driven architecture Identification of predictive insights using event storming methodology Developing models for these insights using machine learning Real-time scoring of the insight models using a streaming analytics processing framework These are scalable easily extensible, and adaptable applications responding in near real time to new situations. There are easily extended to build out and evolve from an initial minimal viable product (MVP) because of the loose coupling in the event-driven architecture, , and streams process domains. Data scientist workbench To complete the extended architecture for integration with analytics and machine learning, consider the toolset and frameworks that the data scientist can use to derive the models. Watson Studio provides tools for data scientists, application developers, and subject matter experts to collaboratively and easily work with data to build and train models at scale. For more information see Getting started with Watson Studio. Legacy integration While you create new digital business applications as self-contained systems, you likely need to integrate legacy apps and databases into the event-driven system. Two ways of coming directly into the event-driven architecture are as follows: Where legacy applications are connected with MQ. You can connect directly from MQ to the Kafka in the event backbone. See IBM Event Streams getting started with MQ article . Where databases support the capture of changes to data, you can publish changes as events to Kafka and hence into the event infrastructure. See the confluent blog for more details","title":"Reference diagrams"},{"location":"architecture/#reference-architecture","text":"We defined the starting point for a Cloud Native Event Driven Architecture to be that it supports at least the following important capabilities: Being able to communicate and persist events. Being able to take direct action on events. Processing streams of events to derive real time insight/intelligence. Providing communication mechanism between event driven microservices and functions. With an event backbone providing the connectivity between the capabilities, we can visualize a reference Event Driven Architecture as below: Where: Event sources : generates events and event streams from sources such as IoT devices, web app, mobile app, microservices\u2026 IBM Event Streams : Provides an Event Backbone supporting Pub/Sub communication, an event log, and simple event stream processing based on Apache Kafka . IBM Cloud Functions : Provides a simplified programming model to take action on an event through a \"serverless\" function-based compute model. Streaming Analytics : Provides continuous ingest and analytical processing across multiple event streams. Decision Server Insights: Provides the means to take action on events and event streams through business rules. Event Stores: Provide optimized persistence (data stores), for event sourcing, Command Query Response Separation (CQRS) and analytical use cases. Event Driven Microservices : Applications that run as serverless functions or containerized workloads which are connected via pub/sub event communication through the event backbone.","title":"Reference Architecture"},{"location":"architecture/#extended-architecture","text":"The event-driven reference architecture provides the framework to support event-driven applications and solutions. The extended architecture provides the connections for: Integration with legacy apps and data resources Integration with analytics or machine learning to derive real-time insights The diagram below shows how these capabilities fit together to form an extended event-driven architecture. In 7. the AI workbench includes tools to do data analysis and visualization, build training and test sets from any datasource and in particular Event Store, and develop models. Models are pushed to streaming analytics component.","title":"Extended Architecture"},{"location":"architecture/#integration-with-analytics-and-machine-learning","text":"The extended architecture extends the basic EDA reference architecture with concepts showing how data science, artificial intelligence and machine learning can be incorporated into an event-driven solution. The starting point for data scientists to be able to derive machine learning models or analyze data for trends and behaviors is the existence of the data in a form that they can be consumed. For real-time intelligent solutions, data scientists typically inspect event histories and decision or action records from a system. Then, they reduce this data to some simplified model that scores new event data as it arrives.","title":"Integration with analytics and machine learning"},{"location":"architecture/#getting-the-data-for-the-data-scientist","text":"With real-time event streams, the challenge is in handling unbounded data or a continuous flow of events. To make this consumable for the data scientist you need to capture the relevant data and store it so that it can be pulled into the analysis and model-building process as required. Following our event-driven reference architecture the event stream would be a Kafka topic on the event backbone. From here there are two possibilities for making that event data available and consumable to the data scientist: The event stream or event log can be accessed directly through Kafka and pulled into the analysis process The event stream can be pre-processed by the streaming analytics system and stored for future use in the analysis process. You have a choice of store type to use. Within public IBM cloud object storage Cloud Object Store can be used as a cost-effective historical store. Both approaches are valid, pre-processing through streaming analytics provides opportunity for greater manipulation of the data, or storing data over time windows for complex event processing. However, the more interesting distinction is where you use a predictive (ML model) to score arriving events or stream data in real time. In this case you may use streaming analytics to extract and save the event data for analysis, model building, and model training and also for scoring (executing) a derived model in line in the real time against arriving event data. The event and decision or action data is made available in cloud object storage for model building through streaming analytics. Models may be developed by tuning and parameter fitting, standard form fitting, classification techniques, and text analytics methods. Increasingly artificial intelligence (AI) and machine learning (ML) frameworks are used to discover and train useful predictive models as an alternative to parameterizing existing model types manually. These techniques lead to process and data flows where the predictive model is trained offline using event histories from the event and the decision or action store possibly augmented with some supervisory outcome labelling, as illustrated by the paths from the Event Backbone and Stream Processing store into Learn/Analyze . A model trained in this way includes some \u201cscoring\u201d API that can be invoked with fresh event data to generate a model-based prediction for future behavior and event properties of that specific context. The scoring function is then easily reincorporated into the streaming analytics processing to generate predictions and insights. These combined techniques can lead to the creation of real-time intelligent applications: Event-driven architecture Identification of predictive insights using event storming methodology Developing models for these insights using machine learning Real-time scoring of the insight models using a streaming analytics processing framework These are scalable easily extensible, and adaptable applications responding in near real time to new situations. There are easily extended to build out and evolve from an initial minimal viable product (MVP) because of the loose coupling in the event-driven architecture, , and streams process domains.","title":"Getting the data for the data scientist:"},{"location":"architecture/#data-scientist-workbench","text":"To complete the extended architecture for integration with analytics and machine learning, consider the toolset and frameworks that the data scientist can use to derive the models. Watson Studio provides tools for data scientists, application developers, and subject matter experts to collaboratively and easily work with data to build and train models at scale. For more information see Getting started with Watson Studio.","title":"Data scientist workbench"},{"location":"architecture/#legacy-integration","text":"While you create new digital business applications as self-contained systems, you likely need to integrate legacy apps and databases into the event-driven system. Two ways of coming directly into the event-driven architecture are as follows: Where legacy applications are connected with MQ. You can connect directly from MQ to the Kafka in the event backbone. See IBM Event Streams getting started with MQ article . Where databases support the capture of changes to data, you can publish changes as events to Kafka and hence into the event infrastructure. See the confluent blog for more details","title":"Legacy integration"},{"location":"compendium/","text":"Compendium IBM Event Streams Event Streams presentation Event Streams Samples Validating Event Streams deployment with sample app. Install Event Streams on ICP IBM Event Streams product based on Kafka delivered in ICP catalog Kafka Start by reading Kafka introduction - a must read! Another introduction from Confluent, one of the main contributors of the open source. Kafka summary and deployment on IBM Cloud Private Planning event streams installation Develop Stream Application using Kafka Tutorial on access control, user authentication and authorization from IBM. Kafka on Kubernetes using stateful sets IBM Developer article - learn kafka Using Kafka Connect to connect to enterprise MQ systems - Andrew Schofield Does Apache Kafka do ACID transactions? - Andrew Schofield Spark and Kafka with direct stream, and persistence considerations and best practices Example in scala for processing Tweets with Kafka Streams Microservices and event-driven patterns API for declaring messaging handlers using Reactive Streams Microservice patterns - Chris Richardson Service mesh Stream Analytics Getting started with IBM Streaming Analytics on IBM Cloud Serverless and cloud function Using Cloud functions with event trigger in Kafka https://github.com/IBM/ibm-cloud-functions-message-hub-trigger Serverless IBM Cloud Functions product offering https://www.ibm.com/cloud/functions Integration Interesting article on the evolving hybrid integration reference architecture : How to ensure your integration landscape keeps pace with digital transformation. Companion web site for hybrid integration reference architecture","title":"Compendium"},{"location":"compendium/#compendium","text":"","title":"Compendium"},{"location":"compendium/#ibm-event-streams","text":"Event Streams presentation Event Streams Samples Validating Event Streams deployment with sample app. Install Event Streams on ICP IBM Event Streams product based on Kafka delivered in ICP catalog","title":"IBM Event Streams"},{"location":"compendium/#kafka","text":"Start by reading Kafka introduction - a must read! Another introduction from Confluent, one of the main contributors of the open source. Kafka summary and deployment on IBM Cloud Private Planning event streams installation Develop Stream Application using Kafka Tutorial on access control, user authentication and authorization from IBM. Kafka on Kubernetes using stateful sets IBM Developer article - learn kafka Using Kafka Connect to connect to enterprise MQ systems - Andrew Schofield Does Apache Kafka do ACID transactions? - Andrew Schofield Spark and Kafka with direct stream, and persistence considerations and best practices Example in scala for processing Tweets with Kafka Streams","title":"Kafka"},{"location":"compendium/#microservices-and-event-driven-patterns","text":"API for declaring messaging handlers using Reactive Streams Microservice patterns - Chris Richardson Service mesh","title":"Microservices and event-driven patterns"},{"location":"compendium/#stream-analytics","text":"Getting started with IBM Streaming Analytics on IBM Cloud","title":"Stream Analytics"},{"location":"compendium/#serverless-and-cloud-function","text":"Using Cloud functions with event trigger in Kafka https://github.com/IBM/ibm-cloud-functions-message-hub-trigger Serverless IBM Cloud Functions product offering https://www.ibm.com/cloud/functions","title":"Serverless and cloud function"},{"location":"compendium/#integration","text":"Interesting article on the evolving hybrid integration reference architecture : How to ensure your integration landscape keeps pace with digital transformation. Companion web site for hybrid integration reference architecture","title":"Integration"},{"location":"concepts/","text":"Concepts Events Events are notifications of change of state. Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed. Events are notifications of change of state. Typically, events represent the change of state of something of interest to the business. Events are records of something that has happened. Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past). Event streams An event stream is a continuous unbounded series of events. The start of the stream may have occurred before we started to process the stream. The end of the stream is at some unknown point in the future. Events are ordered by the point in time at which each event occurred. When developing event driven solutions, you will typically see two types of event streams: Event streams whose events are defined and published into a stream as part of a solution. Event streams that connect to a real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems. Command A command , is an instruction to do something . Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed. Events and Messages There is a long history of messaging in IT systems. You can easily see an event driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering: Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed. Events: Events are persisted as a replayable stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.) Loose coupling Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge about who is going to consume those events. Likewise, event consumers don't need to be aware of the event emitters. Because of this, event consuming modules and event producer modules can be implemented in different languages or use technologies that are different and appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly, result in a significant decrease in system complexity. Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must conform to a certain schema that must be known by both the consumer and the producer. One of the most important things that you can do to reduce coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention to the cohesiveness of those modules. Cohesion Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion, a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.","title":"Concepts"},{"location":"concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/#events","text":"Events are notifications of change of state. Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed. Events are notifications of change of state. Typically, events represent the change of state of something of interest to the business. Events are records of something that has happened. Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past).","title":"Events"},{"location":"concepts/#event-streams","text":"An event stream is a continuous unbounded series of events. The start of the stream may have occurred before we started to process the stream. The end of the stream is at some unknown point in the future. Events are ordered by the point in time at which each event occurred. When developing event driven solutions, you will typically see two types of event streams: Event streams whose events are defined and published into a stream as part of a solution. Event streams that connect to a real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems.","title":"Event streams"},{"location":"concepts/#command","text":"A command , is an instruction to do something . Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed.","title":"Command"},{"location":"concepts/#events-and-messages","text":"There is a long history of messaging in IT systems. You can easily see an event driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering: Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed. Events: Events are persisted as a replayable stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.)","title":"Events and Messages"},{"location":"concepts/#loose-coupling","text":"Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge about who is going to consume those events. Likewise, event consumers don't need to be aware of the event emitters. Because of this, event consuming modules and event producer modules can be implemented in different languages or use technologies that are different and appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly, result in a significant decrease in system complexity. Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must conform to a certain schema that must be known by both the consumer and the producer. One of the most important things that you can do to reduce coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention to the cohesiveness of those modules.","title":"Loose coupling"},{"location":"concepts/#cohesion","text":"Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion, a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.","title":"Cohesion"},{"location":"deployments/eventstreams/","text":"Install IBM Event Streams on ICP (Tested on September 2019 using ibm-eventstreams-prod helm chart 1.3.0 on ICP 3.2.0) You can use the ibm-eventstreams-dev or ibm-eventstreams-prod Helm chart from ICP catalog. The product installation instructions can be found in event stream documentation . Note If you need to upload the tar file for the event streams production (downloaded from IBM passport advantage or other support sites) use the following command: cloudctl catalog load - archive --archive eventstreams.pak.tar.gz As we do not want to rewrite the very good product documentation , we just want to highlight what we did for our own deployment. Our cluster has the following characteristics: Three masters also running ETCD cluster on 3 nodes Three management nodes Three proxy Six worker nodes For worker nodes we need good CPUs and hard disk space. We allocated 12 CPUs - 32 Gb RAM per worker nodes and around 300GB for each worker node. You need to decide if persistence should be enabled for ZooKeepers and Kafka brokers. Pre allocate one Persistence Volume per Kafka broker and one per ZooKeeper server. If you use dynamic persistence volume provisioning, ensure the expected volumes are present at installation time. The yaml file for PV creation is in the refarch-eda/deployments/eventstreams folder. The command: kubectl apply - f ibm - es - pv . yaml - n eventstreams creates 7 volumes: 3 for zookeeper, and 3 for kafka and 1 for schema registry. The configuration is for development purpose, and uses local host path, so if the VM has an issue data will be lost. kubectl get pv - n eventstreams We also created an empty config map so we can update the kafka server.properites in the future. kubectl create configmap greencompute - events - km - n eventstreams Configuration Parameters The following parameters were changed from default settings: Parameter Description Value Kafka.autoCreateTopicsEnable Enable auto-creation of topics true persistence.enabled enable persistent storage for the Kafka brokers true persistence.useDynamicProvisioning dynamically create persistent volume claims false zookeeper.persistence.enabled use persistent storage for the ZooKeeper nodes true zookeeper.persistence.useDynamicProvisioning dynamically create persistent volume claims for the ZooKeeper nodes false proxy.externalAccessEnabled allow external access to Kafka from outside the Kubernetes cluster true The matching server.properties file is under the deployments/eventstreams folder. See parameters description in the product documentation You can get the details of the release with: helm list 'green-events-streams' --tls or access helm detail via ICP console: Here is the helm release details: The figure above shows the following elements: ConfigMaps for UI, Kafka proxy The five deployments for each major components: UI, REST, proxy and access controller. Next is the job list which shows what was run during installation. The panel lists also the current network policies: A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. As soon as there are policies defined, pods will reject connections not allowed by any policies. The pods running in the platform. (One pod was a job) As we can see there are 3 kafka brokers, 3 zookeepers, 2 proxies, 2 access controllers. You can see the pods running on a node using the command: kubectl get pods --all-namespaces --field-selector=spec.nodeName=172.16.50.219 The figure below is for roles, rolebinding and secret as part of the Role Based Access Control settings. The figure below shows the services for zookeeper, Kafka and Event Stream REST api and user interface: The services expose capabilities to external world via nodePort type: The IBM Event Streams admin console is visible at the port 31253 on the k8s proxy IP address: 172.16.50.227 The REST api port 30121 stream proxy port bootstrap: 31348, broker 0: 32489... You get access to the Event Streams admin console by using the IP address of the master / proxy node and the port number of the service, which you can get using the kubectl get service command like: kubectl get svc - n streaming \"green-events-streams-ibm-es-ui-svc\" - o 'jsonpath={.spec.ports[?(@.name==\"admin-ui-https\")].nodePort}' kubectl cluster - info | grep \"catalog\" | awk 'match($0, /([0-9]{1,3}\\.){3}[0-9]{1,3}/) { print substr( $0, RSTART, RLENGTH )}' Here is the admin console home page: To connect an application or tool to this cluster, you will need the address of a bootstrap server, a certificate and an API key. The page to access this information, is on the top right corner: Connect to this cluster : Download certificate and Java truststore files, and the generated API key. A key can apply to all groups or being specific to a group. In Java to leverage the api key the code needs to set the some properties: properties . put ( CommonClientConfigs . SECURITY_PROTOCOL_CONFIG , \"SASL_SSL\" ); properties . put ( SaslConfigs . SASL_MECHANISM , \"PLAIN\" ); properties . put ( SaslConfigs . SASL_JAAS_CONFIG , \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"token\\\" password=\\\"\" + env . get ( \"KAFKA_APIKEY\" ) + \"\\\";\" ); properties . put ( SslConfigs . SSL_PROTOCOL_CONFIG , \"TLSv1.2\" ); properties . put ( SslConfigs . SSL_ENABLED_PROTOCOLS_CONFIG , \"TLSv1.2\" ); properties . put ( SslConfigs . SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG , \"HTTPS\" ); See code example in ApplicationConfig.java . Some challenges during the installation As presented in the high availability discussion in this note , normally we need 6 worker nodes to avoid allocating zookeeper and kafka servers on the same kubernetes nodes. The community edition installation is permissive on that constraint, so both products could co-exist but in that case, ensure to have enough physical resources. We have seen some Kafka brokers that could not be scheduled because some nodes have taints (can't meet the specs for the stateful set) and the remaining worker nodes don't have enough memory. Helm release does not install If you do not see a helm release added to the helm release list, try to see if the installation pod is working. For example the following command, returns a pod name for the release creation job: kubctl get pods - n eventstreams NAME READY STATUS RESTARTS AGE eventstreams - ibm - es - release - cm - creater - job - gkgx2 0 / 1 ImagePullBackOff 0 1 m You caan then access the pod logs to assess what's going on: kubectl logs eventstreams - ibm - es - release - cm - creater - job - gkgx2 - n eventstreams One possible common issue is related to the pod trying and failing to pull image from local repository. To find the solution, you need to know the name of the helm repository: Getting started application Use the Event Stream Toolbox to download a getting started application we can use to test the deployment and as code base for future Kafka consumer / producer development. One example of the generated app is in this repository under gettingStarted/EDAIEWStarterApp folder, and a description on how to compile, package and run it: see the ./gettingStarted/EDAIEWStarterApp/README.md. The application runs in Liberty at the URL: http://localhost:9080/EDAIESStarterApp/ and delivers a simple user interface splitted into two panels: producer and consumer. The figure below illustrates the fact that the connetion to the broker was not working for a short period of time, so the producer has error, but because of the buffering capabilities, it was able to pace and then as soon as the connection was re-established the consumer started to get the messages. No messages were lost!. We have two solution implementations using Kafka and Event Streams the manufacturing asset analytics and the most recent KC container shipment solution . We recommend using the second implementation. Verifying ICP Events Streams installation Once connected to the cluster with kubectl, get the list of pods for the namespace you used to install Kafka or IBM Event Streams: $ kubectl get pods -n streaming NAME READY STATUS RESTARTS green-even-c353-ibm-es-elas-ad8d-0 1 /1 Running 0 3d green-even-c353-ibm-es-elas-ad8d-1 1 /1 Running 0 3d green-even-c353-ibm-es-kafka-sts-0 4 /4 Running 2 3d green-even-c353-ibm-es-kafka-sts-1 4 /4 Running 2 3d green-even-c353-ibm-es-kafka-sts-2 4 /4 Running 5 3d green-even-c353-ibm-es-zook-c4c0-0 1 /1 Running 0 3d green-even-c353-ibm-es-zook-c4c0-1 1 /1 Running 0 3d green-even-c353-ibm-es-zook-c4c0-2 1 /1 Running 0 3d green-events-streams-ibm-es-access-controller-deploy-7cbf8jjs9n 2 /2 Running 0 3d green-events-streams-ibm-es-access-controller-deploy-7cbf8st95z 2 /2 Running 0 3d green-events-streams-ibm-es-indexmgr-deploy-6ff759779-c8ddc 1 /1 Running 0 3d green-events-streams-ibm-es-proxy-deploy-777d6cf76c-bxjtq 1 /1 Running 0 3d green-events-streams-ibm-es-proxy-deploy-777d6cf76c-p8rkc 1 /1 Running 0 3d green-events-streams-ibm-es-rest-deploy-547cc6f9b-774xx 3 /3 Running 0 3d green-events-streams-ibm-es-ui-deploy-7f9b9c6c6f-kvvs2 3 /3 Running 0 3d Select the first pod: green-even-c353-ibm-es-kafka-sts-0 , then execute a bash shell so you can access the Kafka tools: $ kubectl exec green - even - c353 - ibm - es - kafka - sts - 0 - itn streaming -- bash bash - 3 . 4 # cd / opt / Kafka / bin Now you have access to the kafka tools. The most important thing is to get the hostname and port number of the zookeeper server. To do so use the kubectl command: $ kubectl describe pods green-even-c353-ibm-es-zook-c4c0-0 --namespace streaming In the long result get the client port ( ZK_CLIENT_PORT: 2181) information and IP address (IP: 192.168.76.235). Using this information, in the bash shell within the Kafka broker server we can do the following command to get the topics configured. $ ./Kafka-topics.sh --list -zookeeper 192 .168.76.235:2181 # We can also use the service name of zookeeper and let k8s DNS resolve the IP address $ ./Kafka-topics.sh --list -zookeeper green-even-c353-ibm-es-zook-c4c0-0.streaming.svc.cluster.local:2181 Using the Event Stream CLI If not done already, you can install the Event Stream CLI on top of IBM cloud CLI by first downloading it from the Event Stream console and then running this command: $ cloudctl plugin install ./es-plugin Here is a simple summary of the possible cloudctl es commands: # Connect to the cluster cloudctl es init # create a topic - default is 3 replicas cloudctl es topic-create streams-plaintext-input cloudctl es topic-create streams-wordcount-output --replication-factor 1 --partitions 1 # list topics cloudctl es topics # delete topic cloudctl es topic-delete streams-plaintext-input Further Readings IBM Event Streams main page IBM Event Streams Product Documentation","title":"Event Streams Cloud Pak Integration deployment"},{"location":"deployments/eventstreams/#install-ibm-event-streams-on-icp","text":"(Tested on September 2019 using ibm-eventstreams-prod helm chart 1.3.0 on ICP 3.2.0) You can use the ibm-eventstreams-dev or ibm-eventstreams-prod Helm chart from ICP catalog. The product installation instructions can be found in event stream documentation . Note If you need to upload the tar file for the event streams production (downloaded from IBM passport advantage or other support sites) use the following command: cloudctl catalog load - archive --archive eventstreams.pak.tar.gz As we do not want to rewrite the very good product documentation , we just want to highlight what we did for our own deployment. Our cluster has the following characteristics: Three masters also running ETCD cluster on 3 nodes Three management nodes Three proxy Six worker nodes For worker nodes we need good CPUs and hard disk space. We allocated 12 CPUs - 32 Gb RAM per worker nodes and around 300GB for each worker node. You need to decide if persistence should be enabled for ZooKeepers and Kafka brokers. Pre allocate one Persistence Volume per Kafka broker and one per ZooKeeper server. If you use dynamic persistence volume provisioning, ensure the expected volumes are present at installation time. The yaml file for PV creation is in the refarch-eda/deployments/eventstreams folder. The command: kubectl apply - f ibm - es - pv . yaml - n eventstreams creates 7 volumes: 3 for zookeeper, and 3 for kafka and 1 for schema registry. The configuration is for development purpose, and uses local host path, so if the VM has an issue data will be lost. kubectl get pv - n eventstreams We also created an empty config map so we can update the kafka server.properites in the future. kubectl create configmap greencompute - events - km - n eventstreams","title":"Install IBM Event Streams on ICP"},{"location":"deployments/eventstreams/#configuration-parameters","text":"The following parameters were changed from default settings: Parameter Description Value Kafka.autoCreateTopicsEnable Enable auto-creation of topics true persistence.enabled enable persistent storage for the Kafka brokers true persistence.useDynamicProvisioning dynamically create persistent volume claims false zookeeper.persistence.enabled use persistent storage for the ZooKeeper nodes true zookeeper.persistence.useDynamicProvisioning dynamically create persistent volume claims for the ZooKeeper nodes false proxy.externalAccessEnabled allow external access to Kafka from outside the Kubernetes cluster true The matching server.properties file is under the deployments/eventstreams folder. See parameters description in the product documentation You can get the details of the release with: helm list 'green-events-streams' --tls or access helm detail via ICP console: Here is the helm release details: The figure above shows the following elements: ConfigMaps for UI, Kafka proxy The five deployments for each major components: UI, REST, proxy and access controller. Next is the job list which shows what was run during installation. The panel lists also the current network policies: A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. As soon as there are policies defined, pods will reject connections not allowed by any policies. The pods running in the platform. (One pod was a job) As we can see there are 3 kafka brokers, 3 zookeepers, 2 proxies, 2 access controllers. You can see the pods running on a node using the command: kubectl get pods --all-namespaces --field-selector=spec.nodeName=172.16.50.219 The figure below is for roles, rolebinding and secret as part of the Role Based Access Control settings. The figure below shows the services for zookeeper, Kafka and Event Stream REST api and user interface: The services expose capabilities to external world via nodePort type: The IBM Event Streams admin console is visible at the port 31253 on the k8s proxy IP address: 172.16.50.227 The REST api port 30121 stream proxy port bootstrap: 31348, broker 0: 32489... You get access to the Event Streams admin console by using the IP address of the master / proxy node and the port number of the service, which you can get using the kubectl get service command like: kubectl get svc - n streaming \"green-events-streams-ibm-es-ui-svc\" - o 'jsonpath={.spec.ports[?(@.name==\"admin-ui-https\")].nodePort}' kubectl cluster - info | grep \"catalog\" | awk 'match($0, /([0-9]{1,3}\\.){3}[0-9]{1,3}/) { print substr( $0, RSTART, RLENGTH )}' Here is the admin console home page: To connect an application or tool to this cluster, you will need the address of a bootstrap server, a certificate and an API key. The page to access this information, is on the top right corner: Connect to this cluster : Download certificate and Java truststore files, and the generated API key. A key can apply to all groups or being specific to a group. In Java to leverage the api key the code needs to set the some properties: properties . put ( CommonClientConfigs . SECURITY_PROTOCOL_CONFIG , \"SASL_SSL\" ); properties . put ( SaslConfigs . SASL_MECHANISM , \"PLAIN\" ); properties . put ( SaslConfigs . SASL_JAAS_CONFIG , \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"token\\\" password=\\\"\" + env . get ( \"KAFKA_APIKEY\" ) + \"\\\";\" ); properties . put ( SslConfigs . SSL_PROTOCOL_CONFIG , \"TLSv1.2\" ); properties . put ( SslConfigs . SSL_ENABLED_PROTOCOLS_CONFIG , \"TLSv1.2\" ); properties . put ( SslConfigs . SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG , \"HTTPS\" ); See code example in ApplicationConfig.java .","title":"Configuration Parameters"},{"location":"deployments/eventstreams/#some-challenges-during-the-installation","text":"As presented in the high availability discussion in this note , normally we need 6 worker nodes to avoid allocating zookeeper and kafka servers on the same kubernetes nodes. The community edition installation is permissive on that constraint, so both products could co-exist but in that case, ensure to have enough physical resources. We have seen some Kafka brokers that could not be scheduled because some nodes have taints (can't meet the specs for the stateful set) and the remaining worker nodes don't have enough memory.","title":"Some challenges during the installation"},{"location":"deployments/eventstreams/#helm-release-does-not-install","text":"If you do not see a helm release added to the helm release list, try to see if the installation pod is working. For example the following command, returns a pod name for the release creation job: kubctl get pods - n eventstreams NAME READY STATUS RESTARTS AGE eventstreams - ibm - es - release - cm - creater - job - gkgx2 0 / 1 ImagePullBackOff 0 1 m You caan then access the pod logs to assess what's going on: kubectl logs eventstreams - ibm - es - release - cm - creater - job - gkgx2 - n eventstreams One possible common issue is related to the pod trying and failing to pull image from local repository. To find the solution, you need to know the name of the helm repository:","title":"Helm release does not install"},{"location":"deployments/eventstreams/#getting-started-application","text":"Use the Event Stream Toolbox to download a getting started application we can use to test the deployment and as code base for future Kafka consumer / producer development. One example of the generated app is in this repository under gettingStarted/EDAIEWStarterApp folder, and a description on how to compile, package and run it: see the ./gettingStarted/EDAIEWStarterApp/README.md. The application runs in Liberty at the URL: http://localhost:9080/EDAIESStarterApp/ and delivers a simple user interface splitted into two panels: producer and consumer. The figure below illustrates the fact that the connetion to the broker was not working for a short period of time, so the producer has error, but because of the buffering capabilities, it was able to pace and then as soon as the connection was re-established the consumer started to get the messages. No messages were lost!. We have two solution implementations using Kafka and Event Streams the manufacturing asset analytics and the most recent KC container shipment solution . We recommend using the second implementation.","title":"Getting started application"},{"location":"deployments/eventstreams/#verifying-icp-events-streams-installation","text":"Once connected to the cluster with kubectl, get the list of pods for the namespace you used to install Kafka or IBM Event Streams: $ kubectl get pods -n streaming NAME READY STATUS RESTARTS green-even-c353-ibm-es-elas-ad8d-0 1 /1 Running 0 3d green-even-c353-ibm-es-elas-ad8d-1 1 /1 Running 0 3d green-even-c353-ibm-es-kafka-sts-0 4 /4 Running 2 3d green-even-c353-ibm-es-kafka-sts-1 4 /4 Running 2 3d green-even-c353-ibm-es-kafka-sts-2 4 /4 Running 5 3d green-even-c353-ibm-es-zook-c4c0-0 1 /1 Running 0 3d green-even-c353-ibm-es-zook-c4c0-1 1 /1 Running 0 3d green-even-c353-ibm-es-zook-c4c0-2 1 /1 Running 0 3d green-events-streams-ibm-es-access-controller-deploy-7cbf8jjs9n 2 /2 Running 0 3d green-events-streams-ibm-es-access-controller-deploy-7cbf8st95z 2 /2 Running 0 3d green-events-streams-ibm-es-indexmgr-deploy-6ff759779-c8ddc 1 /1 Running 0 3d green-events-streams-ibm-es-proxy-deploy-777d6cf76c-bxjtq 1 /1 Running 0 3d green-events-streams-ibm-es-proxy-deploy-777d6cf76c-p8rkc 1 /1 Running 0 3d green-events-streams-ibm-es-rest-deploy-547cc6f9b-774xx 3 /3 Running 0 3d green-events-streams-ibm-es-ui-deploy-7f9b9c6c6f-kvvs2 3 /3 Running 0 3d Select the first pod: green-even-c353-ibm-es-kafka-sts-0 , then execute a bash shell so you can access the Kafka tools: $ kubectl exec green - even - c353 - ibm - es - kafka - sts - 0 - itn streaming -- bash bash - 3 . 4 # cd / opt / Kafka / bin Now you have access to the kafka tools. The most important thing is to get the hostname and port number of the zookeeper server. To do so use the kubectl command: $ kubectl describe pods green-even-c353-ibm-es-zook-c4c0-0 --namespace streaming In the long result get the client port ( ZK_CLIENT_PORT: 2181) information and IP address (IP: 192.168.76.235). Using this information, in the bash shell within the Kafka broker server we can do the following command to get the topics configured. $ ./Kafka-topics.sh --list -zookeeper 192 .168.76.235:2181 # We can also use the service name of zookeeper and let k8s DNS resolve the IP address $ ./Kafka-topics.sh --list -zookeeper green-even-c353-ibm-es-zook-c4c0-0.streaming.svc.cluster.local:2181","title":"Verifying ICP Events Streams installation"},{"location":"deployments/eventstreams/#using-the-event-stream-cli","text":"If not done already, you can install the Event Stream CLI on top of IBM cloud CLI by first downloading it from the Event Stream console and then running this command: $ cloudctl plugin install ./es-plugin Here is a simple summary of the possible cloudctl es commands: # Connect to the cluster cloudctl es init # create a topic - default is 3 replicas cloudctl es topic-create streams-plaintext-input cloudctl es topic-create streams-wordcount-output --replication-factor 1 --partitions 1 # list topics cloudctl es topics # delete topic cloudctl es topic-delete streams-plaintext-input","title":"Using the Event Stream CLI"},{"location":"deployments/eventstreams/#further-readings","text":"IBM Event Streams main page IBM Event Streams Product Documentation","title":"Further Readings"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/","text":"Install and configure Ceph for IBM Cloud Private Ceph is open source software designed to provide highly scalable object, block and file-based storage under a unified system. Ceph provides a POSIX-compliant network file system (CephFS) that aims for high performance, large data storage, and maximum compatibility with legacy applications. Rook is an open source orchestrator for distributed storage systems running in cloud native environments. Rook turns storage software into self-managing, self-scaling, and self-healing storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties. Note The Helm chart ibm-rook-rbd-cluster is used for setting up Ceph Cluster in IBM Cloud Private. Environment A typical IBM Cloud Private Environment includes Boot node, Master node, Management node, Proxy node and Worker nodes. When the Ceph RBD Cluster is used for providing storage for API Connect, any three worker nodes should be configured to have additional raw disks. The following set of systems can be used as reference for building development (non-HA) environment that runs IBM API Connect workload on IBM Cloud Private. Node type Number of nodes CPU Memory (GB) Disk (GB) Boot (FTP Server) 1 8 32 2048 Master 1 8 32 300 Management 1 8 32 300 Proxy 1 4 16 300 Worker 3 8 32 300+500(disk2) Total 7 52 208 3848+1500(disk2) The following set of systems can be used as reference for building production (HA) environment that runs IBM API Connect workload on IBM Cloud Private. Node type Number of nodes CPU Memory (GB) Disk (GB) Boot (FTP Server) 1 8 32 2048 Master 3 8 32 300 Management 2 8 32 300 Proxy 3 4 16 300 Worker 3 16 64 300+750(disk2) Total 12 108 432 5348+2250(disk2) Note Additional worker nodes will be required when there is a a need to run workloads other than IBM API Connect on IBM Cloud Private. Setup This document covers the setup of Ceph storage using Rook . The following tasks are performed for setting up the Ceph Cluster. Download the required setup files Logon to IBM Cloud Private Cluster Setup Ceph Cluster Verify Ceph cluster Troubleshooting Ceph setup 1. Download the required setup files Note The following files are required for installing ibm-rook-rbd-cluster chart and setting up Ceph cluster login.sh - Utility for logging onto IBM Cloud Private ibm-rook-rbd-cluster-0.8.3.tgz - IBM Chart for Rook RBD Cluster ceph-values.yaml - Sample values.yaml for installing Ceph Cluster rook-ceph-cluster-role-binding.yaml - ClusterRoleBinding for the service account rook-ceph-cluster rook-ceph-operator-values.yaml - Sample values.yaml for installing rook operator rook-cluster-role.yaml - ClusterRole for the resource rook-privileged rook-pod-security-policy.yaml - Define PodSecurityPolicy rook-privileged setup.sh - Utility for setting up Ceph Cluster status.sh - Utility for verifying Ceph Cluster cleanup.sh - Utility for cleaning up Ceph Cluster 2. Logon to IBM Cloud Private Cluster The script login.sh can be run to login to IBM Cloud Private Cluster. !! note The script should be updated to include the correct value for CLUSTER_NAME . Sample run of the login script is as follows: 3. Setup Ceph Cluster Step #1 Update the ceph-values.yaml to match your environment. The file ceph-values.yaml needs to be updated to list the IP address of the storage node within the IBM Cloud Private cluster. ... # # UPDATE VARIABLES TO MATCH THE ENVIRONMENT # nodes : - name : \"X.X.X.X\" devices : - name : \"DISK_NAME\" - name : \"Y.Y.Y.Y\" devices : - name : \"DISK_NAME\" - name : \"Z.Z.Z.Z\" devices : - name : \"DISK_NAME\" ... ... Step #2 Modify and run the setup script to install Rook Operator chart and the IBM Rook RBD Cluster chart The contents of the script setup.sh is as follows: # # UPDATE VARIABLES TO MATCH THE ENVIRONMENT # # Define the location of images IMAGE_DIR =/ DIRECTORY_HAVING_IMAGES ... Note The script should be updated to include the correct location for IMAGE_DIR that has the location where the chart ibm-rook-rbd-cluster-0.8.3.tgz is downloaded and unzipped. The output of Ceph install is listed below for reference: ceph_install.log 4. Verify Ceph cluster The script status.sh can be run to check if Ceph cluster is working as expected. The contents of the script status.sh is as follows: . / deployments / ceph / status . sh Expected output is listed below. 5. Troubleshooting Ceph setup 5.1 Steps for reseting an used disk It is possible that sometimes OSD pods does't start up even though the OSD prepare jobs have completed successfully. It could happen when the device you have specified does not have a raw disk and the device name you have listed was used for other storage like GlusterFS cluster. In such case the following commands can be run to collect the Logical Volume group ID and Physical volume and remove it fully so that the raw disk is made available for the Ceph cluster. pvs pvdisplay vgremove LOGIOCAL_VOLUME_GROUP_ID - y pvremove PHYSICAL_VOLUME The output of the aforesaid commands is listed below. [ root@rsun-rhel-glusterfs03 ~ ] # pvs PV VG Fmt Attr PSize PFree / dev / sda2 rhel lvm2 a -- 39.00g 0 / dev / sdb vg_687894352b254c630b291bf094a8d43d lvm2 a -- 499.87g 499.87g / dev / sdc rhel lvm2 a -- 500.00g 0 [ root@rsun-rhel-glusterfs03 ~ ] # pvdisplay --- Physical volume --- PV Name / dev / sdb VG Name vg_687894352b254c630b291bf094a8d43d PV Size 500.00 GiB / not usable 132.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 127967 Free PE 127967 Allocated PE 0 PV UUID v6xOuh - M2ot - oXfl - IWyf - TnYL - nX3a - kzqizN --- Physical volume --- PV Name / dev / sda2 VG Name rhel PV Size 39.00 GiB / not usable 3.00 MiB Allocatable yes ( but full ) PE Size 4.00 MiB Total PE 9983 Free PE 0 Allocated PE 9983 PV UUID tNjUif - RlBT - kdDn - PWwE - LHlq - 3 w9O - 65 Hlph --- Physical volume --- PV Name / dev / sdc VG Name rhel PV Size 500.00 GiB / not usable 4.00 MiB Allocatable yes ( but full ) PE Size 4.00 MiB Total PE 127999 Free PE 0 Allocated PE 127999 PV UUID 7 CXpz5 - 95 hb - 0 WAC - 3 Efe - XrY1 - s6E6 - dqLasC [ root@rsun-rhel-glusterfs03 ~ ] # vgremove vg_687894352b254c630b291bf094a8d43d - y Volume group \"vg_687894352b254c630b291bf094a8d43d\" successfully removed [ root@rsun-rhel-glusterfs03 ~ ] # pvremove / dev / sdb Labels on physical volume \"/dev/sdb\" successfully wiped . 5.2 Steps for uninstalling the rook-ceph setup Step #1 The script cleanup.sh can be run to remove the Ceph setup completely. . / deployments / ceph / cleanup . sh Step #2 Remove the contents of the temporary directory used by rook: /var/lib/rook The following command should run on all the worker nodes: rm - fr / var / lib / rook Ceph Cluster Management The following links has additional details on how to diagnose, troubleshoot, monitor and report Ceph cluster storage: https://github.com/rook/rook/tree/master/Documentation https://github.com/rook/rook/blob/master/Documentation/common-issues.md#troubleshooting-techniques https://sysdig.com/blog/monitor-ceph-top-5-metrics-watch/ https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know https://sabaini.at/pages/ceph-cheatsheet.html The Ceph Monitor pod can be attached using the following command: kubectl - n rook - ceph exec - it $ ( kubectl - n rook - ceph get pod - l \" app=rook-ceph-mon \" - o jsonpath = ' {.items[0].metadata.name} ' ) bash After being attached to the Ceph Monitor pod, the following commands can be run which provides status and statistics of the Ceph Cluster . ceph health ceph status ceph df ceph osd stat ceph osd tree ceph osd df ceph osd df tree ceph osd perf ceph osd pool stats ceph osd status ceph osd utilization ceph auth list ceph quorum_status ceph mon_status ceph mon dump ceph pg dump ceph pg stat The following link has details on how to add and remove Ceph storage: https://github.com/rook/rook/blob/master/design/cluster-update.md The following link can be used as reference for backing up and restoring the images stored in the Ceph Pool. https://nicksabine.com/post/ceph-backup/ Related commands are: rbd ls - p replicapool rbd export rbd import The aforesaid commands can be run after being attached to the Ceph Monitor pod.","title":"Ceph deployment on ICP"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#install-and-configure-ceph-for-ibm-cloud-private","text":"Ceph is open source software designed to provide highly scalable object, block and file-based storage under a unified system. Ceph provides a POSIX-compliant network file system (CephFS) that aims for high performance, large data storage, and maximum compatibility with legacy applications. Rook is an open source orchestrator for distributed storage systems running in cloud native environments. Rook turns storage software into self-managing, self-scaling, and self-healing storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties. Note The Helm chart ibm-rook-rbd-cluster is used for setting up Ceph Cluster in IBM Cloud Private.","title":"Install and configure Ceph for IBM Cloud Private"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#environment","text":"A typical IBM Cloud Private Environment includes Boot node, Master node, Management node, Proxy node and Worker nodes. When the Ceph RBD Cluster is used for providing storage for API Connect, any three worker nodes should be configured to have additional raw disks. The following set of systems can be used as reference for building development (non-HA) environment that runs IBM API Connect workload on IBM Cloud Private. Node type Number of nodes CPU Memory (GB) Disk (GB) Boot (FTP Server) 1 8 32 2048 Master 1 8 32 300 Management 1 8 32 300 Proxy 1 4 16 300 Worker 3 8 32 300+500(disk2) Total 7 52 208 3848+1500(disk2) The following set of systems can be used as reference for building production (HA) environment that runs IBM API Connect workload on IBM Cloud Private. Node type Number of nodes CPU Memory (GB) Disk (GB) Boot (FTP Server) 1 8 32 2048 Master 3 8 32 300 Management 2 8 32 300 Proxy 3 4 16 300 Worker 3 16 64 300+750(disk2) Total 12 108 432 5348+2250(disk2) Note Additional worker nodes will be required when there is a a need to run workloads other than IBM API Connect on IBM Cloud Private.","title":"Environment"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#setup","text":"This document covers the setup of Ceph storage using Rook . The following tasks are performed for setting up the Ceph Cluster. Download the required setup files Logon to IBM Cloud Private Cluster Setup Ceph Cluster Verify Ceph cluster Troubleshooting Ceph setup","title":"Setup"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#1-download-the-required-setup-files","text":"Note The following files are required for installing ibm-rook-rbd-cluster chart and setting up Ceph cluster login.sh - Utility for logging onto IBM Cloud Private ibm-rook-rbd-cluster-0.8.3.tgz - IBM Chart for Rook RBD Cluster ceph-values.yaml - Sample values.yaml for installing Ceph Cluster rook-ceph-cluster-role-binding.yaml - ClusterRoleBinding for the service account rook-ceph-cluster rook-ceph-operator-values.yaml - Sample values.yaml for installing rook operator rook-cluster-role.yaml - ClusterRole for the resource rook-privileged rook-pod-security-policy.yaml - Define PodSecurityPolicy rook-privileged setup.sh - Utility for setting up Ceph Cluster status.sh - Utility for verifying Ceph Cluster cleanup.sh - Utility for cleaning up Ceph Cluster","title":"1. Download the required setup files"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#2-logon-to-ibm-cloud-private-cluster","text":"The script login.sh can be run to login to IBM Cloud Private Cluster. !! note The script should be updated to include the correct value for CLUSTER_NAME . Sample run of the login script is as follows:","title":"2. Logon to IBM Cloud Private Cluster"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#3-setup-ceph-cluster","text":"Step #1 Update the ceph-values.yaml to match your environment. The file ceph-values.yaml needs to be updated to list the IP address of the storage node within the IBM Cloud Private cluster. ... # # UPDATE VARIABLES TO MATCH THE ENVIRONMENT # nodes : - name : \"X.X.X.X\" devices : - name : \"DISK_NAME\" - name : \"Y.Y.Y.Y\" devices : - name : \"DISK_NAME\" - name : \"Z.Z.Z.Z\" devices : - name : \"DISK_NAME\" ... ... Step #2 Modify and run the setup script to install Rook Operator chart and the IBM Rook RBD Cluster chart The contents of the script setup.sh is as follows: # # UPDATE VARIABLES TO MATCH THE ENVIRONMENT # # Define the location of images IMAGE_DIR =/ DIRECTORY_HAVING_IMAGES ... Note The script should be updated to include the correct location for IMAGE_DIR that has the location where the chart ibm-rook-rbd-cluster-0.8.3.tgz is downloaded and unzipped. The output of Ceph install is listed below for reference: ceph_install.log","title":"3. Setup Ceph Cluster"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#4-verify-ceph-cluster","text":"The script status.sh can be run to check if Ceph cluster is working as expected. The contents of the script status.sh is as follows: . / deployments / ceph / status . sh Expected output is listed below.","title":"4. Verify Ceph cluster"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#5-troubleshooting-ceph-setup","text":"","title":"5. Troubleshooting Ceph setup"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#51-steps-for-reseting-an-used-disk","text":"It is possible that sometimes OSD pods does't start up even though the OSD prepare jobs have completed successfully. It could happen when the device you have specified does not have a raw disk and the device name you have listed was used for other storage like GlusterFS cluster. In such case the following commands can be run to collect the Logical Volume group ID and Physical volume and remove it fully so that the raw disk is made available for the Ceph cluster. pvs pvdisplay vgremove LOGIOCAL_VOLUME_GROUP_ID - y pvremove PHYSICAL_VOLUME The output of the aforesaid commands is listed below. [ root@rsun-rhel-glusterfs03 ~ ] # pvs PV VG Fmt Attr PSize PFree / dev / sda2 rhel lvm2 a -- 39.00g 0 / dev / sdb vg_687894352b254c630b291bf094a8d43d lvm2 a -- 499.87g 499.87g / dev / sdc rhel lvm2 a -- 500.00g 0 [ root@rsun-rhel-glusterfs03 ~ ] # pvdisplay --- Physical volume --- PV Name / dev / sdb VG Name vg_687894352b254c630b291bf094a8d43d PV Size 500.00 GiB / not usable 132.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 127967 Free PE 127967 Allocated PE 0 PV UUID v6xOuh - M2ot - oXfl - IWyf - TnYL - nX3a - kzqizN --- Physical volume --- PV Name / dev / sda2 VG Name rhel PV Size 39.00 GiB / not usable 3.00 MiB Allocatable yes ( but full ) PE Size 4.00 MiB Total PE 9983 Free PE 0 Allocated PE 9983 PV UUID tNjUif - RlBT - kdDn - PWwE - LHlq - 3 w9O - 65 Hlph --- Physical volume --- PV Name / dev / sdc VG Name rhel PV Size 500.00 GiB / not usable 4.00 MiB Allocatable yes ( but full ) PE Size 4.00 MiB Total PE 127999 Free PE 0 Allocated PE 127999 PV UUID 7 CXpz5 - 95 hb - 0 WAC - 3 Efe - XrY1 - s6E6 - dqLasC [ root@rsun-rhel-glusterfs03 ~ ] # vgremove vg_687894352b254c630b291bf094a8d43d - y Volume group \"vg_687894352b254c630b291bf094a8d43d\" successfully removed [ root@rsun-rhel-glusterfs03 ~ ] # pvremove / dev / sdb Labels on physical volume \"/dev/sdb\" successfully wiped .","title":"5.1 Steps for reseting an used disk"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#52-steps-for-uninstalling-the-rook-ceph-setup","text":"Step #1 The script cleanup.sh can be run to remove the Ceph setup completely. . / deployments / ceph / cleanup . sh Step #2 Remove the contents of the temporary directory used by rook: /var/lib/rook The following command should run on all the worker nodes: rm - fr / var / lib / rook","title":"5.2 Steps for uninstalling the rook-ceph setup"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#ceph-cluster-management","text":"The following links has additional details on how to diagnose, troubleshoot, monitor and report Ceph cluster storage: https://github.com/rook/rook/tree/master/Documentation https://github.com/rook/rook/blob/master/Documentation/common-issues.md#troubleshooting-techniques https://sysdig.com/blog/monitor-ceph-top-5-metrics-watch/ https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know https://sabaini.at/pages/ceph-cheatsheet.html The Ceph Monitor pod can be attached using the following command: kubectl - n rook - ceph exec - it $ ( kubectl - n rook - ceph get pod - l \" app=rook-ceph-mon \" - o jsonpath = ' {.items[0].metadata.name} ' ) bash After being attached to the Ceph Monitor pod, the following commands can be run which provides status and statistics of the Ceph Cluster . ceph health ceph status ceph df ceph osd stat ceph osd tree ceph osd df ceph osd df tree ceph osd perf ceph osd pool stats ceph osd status ceph osd utilization ceph auth list ceph quorum_status ceph mon_status ceph mon dump ceph pg dump ceph pg stat The following link has details on how to add and remove Ceph storage: https://github.com/rook/rook/blob/master/design/cluster-update.md The following link can be used as reference for backing up and restoring the images stored in the Ceph Pool. https://nicksabine.com/post/ceph-backup/ Related commands are: rbd ls - p replicapool rbd export rbd import The aforesaid commands can be run after being attached to the Ceph Monitor pod.","title":"Ceph Cluster Management"},{"location":"deployments/eventstreams/es-ibm-cloud/","text":"Event Stream Service on IBM Cloud The installation instructions are here: ibm.github.io/event-streams . Things to consider before installation Software and hardware pre-requisites Can start small with the 3 brokers cluster. See configuration note Be sure the servers running the nodes and then event streams pods have enough resource available, using free -m linux command for memory and df for disk Define the persistence strategy to support persisting logs from the Kafka topic and zookeepers metadata. Do you need to encrypt traffic to the cluster with your own certificates? Do you want to use TLS for pod to pod communication? For geo-replication defines the number of workers dedicated to do replication. Provision the service To provision your service, go to the IBM Cloud Catalog and search for Event Streams . It is in the Integration category. Create the service and specify a name, a region and a resource group, add a tag if you want to, then select the enterprise plan. Once the service is provisioned you should reach the welcome page: Add service credentials In the service credentials create new credentials to get the Kafka brokers list, the admim URL and the api_key needed to authenticate the consumers or producers. The api key and the broker urls will be used to set kubernetes secrets needed by the producer and consumer services. Add topic with advanced properties Launch the dashboard: and then select the 'create a topic' tile to add the topics needed for the solution. Enter a name and select Advanced switch: In the advanced configuration, specify the number of partition and event retention time: for topic with not a lot of events, topic can keep data for a long period of time. As events are persisted in log, we need to specify the cleanup policy, delete or comprest , and the size limit for retention, which control the maximum size a partition (which consists of log segments) can grow to before brokers discard old log segments to free up space if topic is set to use the \"delete\" retention policy. Log segment size correspond to the topic configuration, segment.bytes , which controls the segment file size for the log. Retention and cleaning are always done one file at a time, so a larger segment size means fewer files but less granular control over retention. segment.ms cotrols the time period after which Kafka will force the log to roll even if the segment file isn't full to ensure that retention can delete or compact old data. segment.index.bytes controls the size of the index that maps offsets to file positions. You generally should not need to change this setting. The admin REST api offers way to configure more parameters.","title":"Event Streams IBM CLOUD deployment"},{"location":"deployments/eventstreams/es-ibm-cloud/#event-stream-service-on-ibm-cloud","text":"The installation instructions are here: ibm.github.io/event-streams .","title":"Event Stream Service on IBM Cloud"},{"location":"deployments/eventstreams/es-ibm-cloud/#things-to-consider-before-installation","text":"Software and hardware pre-requisites Can start small with the 3 brokers cluster. See configuration note Be sure the servers running the nodes and then event streams pods have enough resource available, using free -m linux command for memory and df for disk Define the persistence strategy to support persisting logs from the Kafka topic and zookeepers metadata. Do you need to encrypt traffic to the cluster with your own certificates? Do you want to use TLS for pod to pod communication? For geo-replication defines the number of workers dedicated to do replication.","title":"Things to consider before installation"},{"location":"deployments/eventstreams/es-ibm-cloud/#provision-the-service","text":"To provision your service, go to the IBM Cloud Catalog and search for Event Streams . It is in the Integration category. Create the service and specify a name, a region and a resource group, add a tag if you want to, then select the enterprise plan. Once the service is provisioned you should reach the welcome page:","title":"Provision the service"},{"location":"deployments/eventstreams/es-ibm-cloud/#add-service-credentials","text":"In the service credentials create new credentials to get the Kafka brokers list, the admim URL and the api_key needed to authenticate the consumers or producers. The api key and the broker urls will be used to set kubernetes secrets needed by the producer and consumer services.","title":"Add service credentials"},{"location":"deployments/eventstreams/es-ibm-cloud/#add-topic-with-advanced-properties","text":"Launch the dashboard: and then select the 'create a topic' tile to add the topics needed for the solution. Enter a name and select Advanced switch: In the advanced configuration, specify the number of partition and event retention time: for topic with not a lot of events, topic can keep data for a long period of time. As events are persisted in log, we need to specify the cleanup policy, delete or comprest , and the size limit for retention, which control the maximum size a partition (which consists of log segments) can grow to before brokers discard old log segments to free up space if topic is set to use the \"delete\" retention policy. Log segment size correspond to the topic configuration, segment.bytes , which controls the segment file size for the log. Retention and cleaning are always done one file at a time, so a larger segment size means fewer files but less granular control over retention. segment.ms cotrols the time period after which Kafka will force the log to roll even if the segment file isn't full to ensure that retention can delete or compact old data. segment.index.bytes controls the size of the index that maps offsets to file positions. You generally should not need to change this setting. The admin REST api offers way to configure more parameters.","title":"Add topic with advanced properties"},{"location":"deployments/kafka/","text":"Kafka Deployment A frequently asked question related to deployment is whether Kafka should be deployed on its own servers or within kubernetes. While a lot of kakfa based product providers have selected to deploy to kubernetes via operators, like Strimzi, or helm chart, it may make sense to separate the Kafka broker servers on their own VM or even bare-metal. We want to highlight the following criterias to assess before going to non kubernetes deployment: Fast persistence storage capacity and technology used to support persistence volumes. You need high level throughput storage. Skill set around kubernetes deployment and operations Maintenance of operating system, software version for standalone servers Advantages of deploying Kafka on Kubernetes: Simplified and fast deployment of the solution. Event better with Helm or Operators. Consistent management plane with the consumer & producer applications. Kafka version upgrades are provided through new container images, reducing configuration drift over updating VM\u2019s in place. Kubernetes can manage automatic rebooting brokers in the event of failure, which simplifies management. The brokers are constantly monitored for liveness. Consumer application can use cluster internal hostnames, potentially reducing the number of network hops to/from producer/consumer applications. Can leverage logging and monitoring tools that are part of the Kubernetes platform. Easy to scale by adding new broker Automatic version rolling upgrade without impacting Kafka brokers availability. Cloud Platform agnostic. Disadvantages of deploying Kafka on Kubernetes: Kafka (and ZooKeeper) must have very fast highly-available persistent storage available. This limits the storage options for Kafka/ZooKeeper data to those storage options supported by Kubernetes. Teams used to managing Kafka on VM\u2019s or bare metal will have to learn the complexities of Kubernetes to effectively manage Kafka on Kubernetes. In our implementation solution are proposing different deployment approaches: Using IBM Event Streams on kubernetes or Openshift (See separate note ) Using Kafka on development environment, mostly developer workstation using docker and docker-compose. (See this note for reference ) Using IBM Event Streams as service on IBM Cloud platform. (See this note ) We are defining two types of manifests, one set for development environment and one for production. The manifests and scripts are under each deployment folders. Verifying Kafka is connected to zookeeper The goal is to connect to the kafka running container and use the scripts inside kafka bin folder: # connect to the running container: $ kubectl exec -ti gc-kafka-0 /bin/bash -n greencompute # next is the prompt inside the container: kafka@gc-kafka-0:/$ cd /opt/kafka/bin # for example create a topic for testing kafka@gc-kafka-0:/$./kafka-topics.sh --create --zookeeper gc-client-zookeeper-svc.greencompute.svc.cluster.local:2181 --replication-factor 1 --partitions 1 --topic text-topic This previous command create a text-topic and to verify the configured existing topics use the command (inside the container): kafka@gc-kafka-0:/$./kafka-topics.sh --list --zookeeper gc-client-zookeeper-svc.greencompute.svc.cluster.local:2181 The URL of the zookeeper matches the hostname defined when deploying zookeeper service (see installing zookeeper note ): kubectl describe svc gc-client-zookeeper-svc Verifying pub/sub works with text messages Two scripts exist in the scripts folder in this repository. Those scripts are using kafkacat tool from Confluent. You need to add the following in your hostname resolution configuration (DNS or /etc/hosts), matching you IP address of your laptop. 192 .168.1.89 gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local Start the consumer in a terminal window ./scripts/kafka/consumetext.sh And start the producer in a second terminal: ./scripts/kafka/producetext.sh You should see the text: try to send some text to the text-topic Let see... % Reached end of topic text-topic [ 0 ] at offset 3 Run Kafka in Docker On Linux If you run on a linux operating system, you can use the Spotify Kafka image from dockerhub as it includes Zookeeper and Kafka in a single image. It is started in background (-d), named \" Kafka \" and mounting scripts folder to /scripts: docker run -d -p 2181 :2181 -p 9092 :9092 -v ` pwd ` :/scripts --env ADVERTISED_HOST = ` docker-machine ip \\` docker-machine active \\` ` --name kafka --env ADVERTISED_PORT = 9092 spotify/kafka Then remote connect to the docker container to open a bash shell: docker exec -ti kafka/bin/bash Create a topic: it uses zookeeper as a backend to persist partition within the topic. In this deployment zookeeper and Kafka are running on the localhost inside the container. So port 2181 is the client port for zookeeper. cd /opt/kafka/bin ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic mytopic ./kafka-topics.sh --list --zookeeper localhost:2181 We have done shell scripts for you to do those command and test your local Kafka . The scripts are under ../scripts/kafka createtopic.sh listtopic.sh sendText.sh Send a multiple lines message on mytopic topic- open this one in one terminal. consumeMessage.sh Connect to the topic to get messages. and this second in another terminal. Deployment considerations One major requirement to address which impacts kubernetes Kafka Services configuration and Kafka Broker server configuration is to assess remote access need: do we need to have applications not deployed on Kubernetes that will publish and/or consume message to/from topics defined in the Kafka Brokers running in pods. Normally the answer should be yes as all deployments are hybrid cloud per nature. As the current client API is doing its own load balancing between brokers we will not be able to use ingress or dynamic node port allocation. Let explain by starting to review Java code to access brokers. The properties needed to access is BOOTSTRAP_SERVERS: public static String BOOTSTRAP_SERVERS = \"172.16.40.133:32224,172.16.40.137:32224,172.16.40.135:32224\" ; Properties properties = new Properties (); properties . put ( ProducerConfig . BOOTSTRAP_SERVERS_CONFIG , BOOTSTRAP_SERVERS ); kafkaProducer = new KafkaProducer <>( properties ); .... To connect to broker their addresses and port numbers need to be specified. This information should come from external properties file (the code above is for illustration). The problem is that once deployed in Kubernetes, Kafka broker runs as pod so have dynamic port numbers if we expose a service using NodePort, the IP address may change overtime while pod are scheduled to Node. The list of brokers need to be in the format: : , : , : . A host list, without port number will not work, forbidden the use of virtual host name defined with Ingress manifest and managed by Kubernetes ingress proxy. An external load balancer will not work too. Here is an example of return message when the broker list is not set right: Connection to node -1 could not be established. Broker may not be available . There are two options to support remote connection: implement a proxy, deployed inside the Kubernetes cluster, with 3 or 5 hostnames and port to expose the brokers, or use static NodePort. As of now for development we used NodePort: apiVersion : v1 kind : Service metadata : labels : app : gc-kafka name : gc-kafka-svc spec : type : NodePort ports : - name : kafka-port port : 32224 nodePort : 32224 targetPort : 32224 selector : app : gc-kafka So we use a port number for internal and external communication. In statefulset we use a google created tool to start the kafka server and set parameters to override the default the conf/server.properties . command: - \"exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id= ${ HOSTNAME ##*- } \\ --override listeners=PLAINTEXT://:32224 \\ When consumer or producer connect to a broker in the list there are some messages exchanged, like getting the cluster ID and the endpoint to be used which corresponds to a virtual DNS name of the exposed service: gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local : INFO org.apache.kafka.clients.Metadata - Cluster ID: 4qlnD1e-S8ONpOkIOGE8mg INFO o.a.k.c.c.i.AbstractCoordinator - [ Consumer clientId = consumer-1, groupId = b6e69280-aa7f-47d2-95f5-f69a8f86b967 ] Discovered group coordinator gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local:32224 ( id: 2147483647 rack: null ) So the code may not have this entry defined in the DNS. I used /etc/hosts to map it to K8s Proxy IP address. Also the port number return is the one specified in the server configuration, it has to be one Kubernetes and Calico set in the accepted range and exposed on each host of the cluster. With that connection can be established. Verifying deployment We can use the tools delivered with Kafka by using the very helpful kubectl exec command. Validate the list of topics from the developer's workstation using the command: $ kubectl exec -ti gc-Kafka-0 -- bash -c \"kafka-topics.sh --list --zookeeper gc-srv-zookeeper-svc.greencompute.svc.cluster.local:2181 \" or Kafka-topics.sh --describe --topic text-topic --zookeeper gc-srv-zookeeper-svc.greencompute.svc.cluster.local:2181 start the consumer from the developer's workstation kubectl get pods | grep gc-Kafka kubectl exec gc-Kafka-0 -- bash -c \"Kafka-console-consumer.sh --bootstrap-server localhost:9093 --topic test-topic --from-beginning\" the script deployment/Kafka/consumetext.sh executes those commands. As we run in the Kafka broker the host is localhost and the port number is the headless service one. start a text producer Using the same approach we can use broker tool: $ kubectl exec gc-Kafka-0 -- bash -c \"/opt/Kafka/bin/Kafka-console-producer.sh --broker-list localhost:9093 --topic test-topic << EOB this is a message for you and this one too but this one... I m not sure EOB\" Next steps... do pub/sub message using remote IP and port from remote server. The code is in this project . Troubleshooting For ICP troubleshooting see this centralized note Assess the list of Topics # remote connect to the Kafka pod and open a bash: kubectl exec -ti Kafka-786975b994-9m8n2 bash bash-4.4# ./Kafka-topics.sh --zookeeper 192 .168.1.89:30181 --list Purge a topic with bad message: delete and recreate it ./Kafka-topics.sh --zookeeper 192 .168.1.89:30181 --delete --topic test-topic ./Kafka-topics.sh --zookeeper 192 .168.1.89:30181 --create --replication-factor 1 --partitions 1 --topic test-topic Timeout while sending message to topic The error message may look like: Error when sending message to topic test-topic with key: null, value: 12 bytes with error: ( org.apache.Kafka.clients.producer.internals.ErrorLoggingCallback ) org.apache.Kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms. This can be linked to a lot of different issues, but it is a communication problem. Assess the following: port number exposed match the broker's one. host name known by the server running the producer or consumer code.","title":"Kafka deployment"},{"location":"deployments/kafka/#kafka-deployment","text":"A frequently asked question related to deployment is whether Kafka should be deployed on its own servers or within kubernetes. While a lot of kakfa based product providers have selected to deploy to kubernetes via operators, like Strimzi, or helm chart, it may make sense to separate the Kafka broker servers on their own VM or even bare-metal. We want to highlight the following criterias to assess before going to non kubernetes deployment: Fast persistence storage capacity and technology used to support persistence volumes. You need high level throughput storage. Skill set around kubernetes deployment and operations Maintenance of operating system, software version for standalone servers Advantages of deploying Kafka on Kubernetes: Simplified and fast deployment of the solution. Event better with Helm or Operators. Consistent management plane with the consumer & producer applications. Kafka version upgrades are provided through new container images, reducing configuration drift over updating VM\u2019s in place. Kubernetes can manage automatic rebooting brokers in the event of failure, which simplifies management. The brokers are constantly monitored for liveness. Consumer application can use cluster internal hostnames, potentially reducing the number of network hops to/from producer/consumer applications. Can leverage logging and monitoring tools that are part of the Kubernetes platform. Easy to scale by adding new broker Automatic version rolling upgrade without impacting Kafka brokers availability. Cloud Platform agnostic. Disadvantages of deploying Kafka on Kubernetes: Kafka (and ZooKeeper) must have very fast highly-available persistent storage available. This limits the storage options for Kafka/ZooKeeper data to those storage options supported by Kubernetes. Teams used to managing Kafka on VM\u2019s or bare metal will have to learn the complexities of Kubernetes to effectively manage Kafka on Kubernetes. In our implementation solution are proposing different deployment approaches: Using IBM Event Streams on kubernetes or Openshift (See separate note ) Using Kafka on development environment, mostly developer workstation using docker and docker-compose. (See this note for reference ) Using IBM Event Streams as service on IBM Cloud platform. (See this note ) We are defining two types of manifests, one set for development environment and one for production. The manifests and scripts are under each deployment folders.","title":"Kafka Deployment"},{"location":"deployments/kafka/#verifying-kafka-is-connected-to-zookeeper","text":"The goal is to connect to the kafka running container and use the scripts inside kafka bin folder: # connect to the running container: $ kubectl exec -ti gc-kafka-0 /bin/bash -n greencompute # next is the prompt inside the container: kafka@gc-kafka-0:/$ cd /opt/kafka/bin # for example create a topic for testing kafka@gc-kafka-0:/$./kafka-topics.sh --create --zookeeper gc-client-zookeeper-svc.greencompute.svc.cluster.local:2181 --replication-factor 1 --partitions 1 --topic text-topic This previous command create a text-topic and to verify the configured existing topics use the command (inside the container): kafka@gc-kafka-0:/$./kafka-topics.sh --list --zookeeper gc-client-zookeeper-svc.greencompute.svc.cluster.local:2181 The URL of the zookeeper matches the hostname defined when deploying zookeeper service (see installing zookeeper note ): kubectl describe svc gc-client-zookeeper-svc","title":"Verifying Kafka is connected to zookeeper"},{"location":"deployments/kafka/#verifying-pubsub-works-with-text-messages","text":"Two scripts exist in the scripts folder in this repository. Those scripts are using kafkacat tool from Confluent. You need to add the following in your hostname resolution configuration (DNS or /etc/hosts), matching you IP address of your laptop. 192 .168.1.89 gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local Start the consumer in a terminal window ./scripts/kafka/consumetext.sh And start the producer in a second terminal: ./scripts/kafka/producetext.sh You should see the text: try to send some text to the text-topic Let see... % Reached end of topic text-topic [ 0 ] at offset 3","title":"Verifying pub/sub works with text messages"},{"location":"deployments/kafka/#run-kafka-in-docker-on-linux","text":"If you run on a linux operating system, you can use the Spotify Kafka image from dockerhub as it includes Zookeeper and Kafka in a single image. It is started in background (-d), named \" Kafka \" and mounting scripts folder to /scripts: docker run -d -p 2181 :2181 -p 9092 :9092 -v ` pwd ` :/scripts --env ADVERTISED_HOST = ` docker-machine ip \\` docker-machine active \\` ` --name kafka --env ADVERTISED_PORT = 9092 spotify/kafka Then remote connect to the docker container to open a bash shell: docker exec -ti kafka/bin/bash Create a topic: it uses zookeeper as a backend to persist partition within the topic. In this deployment zookeeper and Kafka are running on the localhost inside the container. So port 2181 is the client port for zookeeper. cd /opt/kafka/bin ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic mytopic ./kafka-topics.sh --list --zookeeper localhost:2181 We have done shell scripts for you to do those command and test your local Kafka . The scripts are under ../scripts/kafka createtopic.sh listtopic.sh sendText.sh Send a multiple lines message on mytopic topic- open this one in one terminal. consumeMessage.sh Connect to the topic to get messages. and this second in another terminal.","title":"Run Kafka in Docker On Linux"},{"location":"deployments/kafka/#deployment-considerations","text":"One major requirement to address which impacts kubernetes Kafka Services configuration and Kafka Broker server configuration is to assess remote access need: do we need to have applications not deployed on Kubernetes that will publish and/or consume message to/from topics defined in the Kafka Brokers running in pods. Normally the answer should be yes as all deployments are hybrid cloud per nature. As the current client API is doing its own load balancing between brokers we will not be able to use ingress or dynamic node port allocation. Let explain by starting to review Java code to access brokers. The properties needed to access is BOOTSTRAP_SERVERS: public static String BOOTSTRAP_SERVERS = \"172.16.40.133:32224,172.16.40.137:32224,172.16.40.135:32224\" ; Properties properties = new Properties (); properties . put ( ProducerConfig . BOOTSTRAP_SERVERS_CONFIG , BOOTSTRAP_SERVERS ); kafkaProducer = new KafkaProducer <>( properties ); .... To connect to broker their addresses and port numbers need to be specified. This information should come from external properties file (the code above is for illustration). The problem is that once deployed in Kubernetes, Kafka broker runs as pod so have dynamic port numbers if we expose a service using NodePort, the IP address may change overtime while pod are scheduled to Node. The list of brokers need to be in the format: : , : , : . A host list, without port number will not work, forbidden the use of virtual host name defined with Ingress manifest and managed by Kubernetes ingress proxy. An external load balancer will not work too. Here is an example of return message when the broker list is not set right: Connection to node -1 could not be established. Broker may not be available . There are two options to support remote connection: implement a proxy, deployed inside the Kubernetes cluster, with 3 or 5 hostnames and port to expose the brokers, or use static NodePort. As of now for development we used NodePort: apiVersion : v1 kind : Service metadata : labels : app : gc-kafka name : gc-kafka-svc spec : type : NodePort ports : - name : kafka-port port : 32224 nodePort : 32224 targetPort : 32224 selector : app : gc-kafka So we use a port number for internal and external communication. In statefulset we use a google created tool to start the kafka server and set parameters to override the default the conf/server.properties . command: - \"exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id= ${ HOSTNAME ##*- } \\ --override listeners=PLAINTEXT://:32224 \\ When consumer or producer connect to a broker in the list there are some messages exchanged, like getting the cluster ID and the endpoint to be used which corresponds to a virtual DNS name of the exposed service: gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local : INFO org.apache.kafka.clients.Metadata - Cluster ID: 4qlnD1e-S8ONpOkIOGE8mg INFO o.a.k.c.c.i.AbstractCoordinator - [ Consumer clientId = consumer-1, groupId = b6e69280-aa7f-47d2-95f5-f69a8f86b967 ] Discovered group coordinator gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local:32224 ( id: 2147483647 rack: null ) So the code may not have this entry defined in the DNS. I used /etc/hosts to map it to K8s Proxy IP address. Also the port number return is the one specified in the server configuration, it has to be one Kubernetes and Calico set in the accepted range and exposed on each host of the cluster. With that connection can be established.","title":"Deployment considerations"},{"location":"deployments/kafka/#verifying-deployment","text":"We can use the tools delivered with Kafka by using the very helpful kubectl exec command. Validate the list of topics from the developer's workstation using the command: $ kubectl exec -ti gc-Kafka-0 -- bash -c \"kafka-topics.sh --list --zookeeper gc-srv-zookeeper-svc.greencompute.svc.cluster.local:2181 \" or Kafka-topics.sh --describe --topic text-topic --zookeeper gc-srv-zookeeper-svc.greencompute.svc.cluster.local:2181 start the consumer from the developer's workstation kubectl get pods | grep gc-Kafka kubectl exec gc-Kafka-0 -- bash -c \"Kafka-console-consumer.sh --bootstrap-server localhost:9093 --topic test-topic --from-beginning\" the script deployment/Kafka/consumetext.sh executes those commands. As we run in the Kafka broker the host is localhost and the port number is the headless service one. start a text producer Using the same approach we can use broker tool: $ kubectl exec gc-Kafka-0 -- bash -c \"/opt/Kafka/bin/Kafka-console-producer.sh --broker-list localhost:9093 --topic test-topic << EOB this is a message for you and this one too but this one... I m not sure EOB\" Next steps... do pub/sub message using remote IP and port from remote server. The code is in this project .","title":"Verifying deployment"},{"location":"deployments/kafka/#troubleshooting","text":"For ICP troubleshooting see this centralized note","title":"Troubleshooting"},{"location":"deployments/kafka/#assess-the-list-of-topics","text":"# remote connect to the Kafka pod and open a bash: kubectl exec -ti Kafka-786975b994-9m8n2 bash bash-4.4# ./Kafka-topics.sh --zookeeper 192 .168.1.89:30181 --list Purge a topic with bad message: delete and recreate it ./Kafka-topics.sh --zookeeper 192 .168.1.89:30181 --delete --topic test-topic ./Kafka-topics.sh --zookeeper 192 .168.1.89:30181 --create --replication-factor 1 --partitions 1 --topic test-topic","title":"Assess the list of Topics"},{"location":"deployments/kafka/#timeout-while-sending-message-to-topic","text":"The error message may look like: Error when sending message to topic test-topic with key: null, value: 12 bytes with error: ( org.apache.Kafka.clients.producer.internals.ErrorLoggingCallback ) org.apache.Kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms. This can be linked to a lot of different issues, but it is a communication problem. Assess the following: port number exposed match the broker's one. host name known by the server running the producer or consumer code.","title":"Timeout while sending message to topic"},{"location":"deployments/postgresql/","text":"Deploying Postgresql to ICP Update 05/10/2019 - ICP 3.2.1 Pre-requisites Access to an ICP cluster with an up to date catalog Once logged to the admin consoler (something like: https://172.16.254.80:8443) go to the Command Line Tools menu and download the IBM Cloud Private CLI. Rename the downloaded file to cloudctl and move it to a folder in your $PATH (e.g. /usr/local/bin/cloudctl) Download the kubeclt CLI that match ICP version. Rename and move the tool to /usr/local/bin/ Download the kubeclt CLI that match ICP version. Rename and move the tool to /usr/local/bin/ Get psql to access the postgresql. Steps Login to the cluster: cloudctl login - a https : // 172 . 16 . 254 . 80 : 8443 - u admin - p < passwordyoushouldknow > --skip-ssl-validation When selecting the postgresql tile in the database category of the catalog (https://172.16.254.80:8443/catalog/) the Overview gives some steps to follow, but those are from the product documentation and they may need some update. Below are the specifics we did: For the namespace we use greencompute , so the secret was something like: $ kubectl create secret generic postgresql-pwd-secret --from-literal = 'postgresql-password=<>' --namespace greencompute secret \"postgresql-pwd-secret\" created Create a persistence volume. You can use HostPath for development purpose, or if you have a NFS or ceph cluster available adapt the CRD file apiVersion : v1, kind : PersistentVolume, metadata : name : posgresql-pv, spec : capacity : storage : 10Gi hostPath : path : /bitnami/postgresql, type : \"\" accessModes : ReadWriteOnce persistentVolumeReclaimPolicy : Retain For NFS use the following changes: spec : nfs : server : path : / bitnami / postgresql As we deploy postgres in a namespace scope, we need to specify an image policy to authorize access to docker.io repository: apiVersion : securityenforcement.admission.cloud.ibm.com/v1beta1 kind : ImagePolicy namespace : greencompute metadata : name : postgresql-image-policy spec : repositories : - name : docker.io/* policy : va : enabled : false save the file as securitypolicies.yml and then run: $ kubectl apply -f securitypolicies.yml -n greencompute $ kubectl describe ImagePolicy postgresql-image-policy -n greencompute Use helm to install the release. Here is an example $ export PSWD = $( k get secret postgresql-pwd-secret -n greencompute -o jsonpath = \"{.data.postgresql-password}\" | base64 --decode ; echo ) $ helm install stable/postgresql --name postgresql --namespace greencompute --set postgresqlPassword = $PSWD ,postgresqlDatabase = postgres --tls Access to the database with psql running locally on your computer In one terminal start a port forwarding using: kubectl port-forward postgresql-postgresql-0 5432:5432 &>> /dev/null & . Now we can connect our local psql CLI to the remote server via a command like: $ psql \"dbname=postgres host=127.0.0.1 user=postgres port=5432 password= $PSWD \" postgres = # \\d containers id | character varying ( 255 ) | | not null | brand | character varying ( 255 ) | | | capacity | integer | | not null | created_at | timestamp without time zone | | not null | current_city | character varying ( 255 ) | | | latitude | double precision | | not null | longitude | double precision | | not null | status | integer | | | type | character varying ( 255 ) | | | updated_at | timestamp without time zone | | not null | For more information about the psql tool see this note. Troubleshooting admission webhook \"trust.hooks.securityenforcement.admission.cloud.ibm.com\" denied the request: Deny \"docker.io/bitnami/postgresql:10.7.0\", no matching repositories in ClusterImagePolicy and no ImagePolicies in the \"greencompute\" namespace Be sure to use a ImagePolicy and not a cluster policy when using namespace deployment. Error: release postgresql failed: Internal error occurred: admission webhook \"trust.hooks.securityenforcement.admission.cloud.ibm.com\" denied the request: Deny \"docker.io/bitnami/postgresql:10.7.0\", no matching repositories in the ImagePolicies Be sure to authorize docker.io/* in the ImagePolicy. More Readings ICP 2.1 Postgresql install recipe: older recipeusing the configuration user interface in the ICP console. postgresql helm chart explanation and configuration : a must read. Installing postgresql via Helm Reefer container management microservice using Springboot, kafka and postgresql","title":"Postgresql"},{"location":"deployments/postgresql/#deploying-postgresql-to-icp","text":"Update 05/10/2019 - ICP 3.2.1","title":"Deploying Postgresql to ICP"},{"location":"deployments/postgresql/#pre-requisites","text":"Access to an ICP cluster with an up to date catalog Once logged to the admin consoler (something like: https://172.16.254.80:8443) go to the Command Line Tools menu and download the IBM Cloud Private CLI. Rename the downloaded file to cloudctl and move it to a folder in your $PATH (e.g. /usr/local/bin/cloudctl) Download the kubeclt CLI that match ICP version. Rename and move the tool to /usr/local/bin/ Download the kubeclt CLI that match ICP version. Rename and move the tool to /usr/local/bin/ Get psql to access the postgresql.","title":"Pre-requisites"},{"location":"deployments/postgresql/#steps","text":"Login to the cluster: cloudctl login - a https : // 172 . 16 . 254 . 80 : 8443 - u admin - p < passwordyoushouldknow > --skip-ssl-validation When selecting the postgresql tile in the database category of the catalog (https://172.16.254.80:8443/catalog/) the Overview gives some steps to follow, but those are from the product documentation and they may need some update. Below are the specifics we did: For the namespace we use greencompute , so the secret was something like: $ kubectl create secret generic postgresql-pwd-secret --from-literal = 'postgresql-password=<>' --namespace greencompute secret \"postgresql-pwd-secret\" created Create a persistence volume. You can use HostPath for development purpose, or if you have a NFS or ceph cluster available adapt the CRD file apiVersion : v1, kind : PersistentVolume, metadata : name : posgresql-pv, spec : capacity : storage : 10Gi hostPath : path : /bitnami/postgresql, type : \"\" accessModes : ReadWriteOnce persistentVolumeReclaimPolicy : Retain For NFS use the following changes: spec : nfs : server : path : / bitnami / postgresql As we deploy postgres in a namespace scope, we need to specify an image policy to authorize access to docker.io repository: apiVersion : securityenforcement.admission.cloud.ibm.com/v1beta1 kind : ImagePolicy namespace : greencompute metadata : name : postgresql-image-policy spec : repositories : - name : docker.io/* policy : va : enabled : false save the file as securitypolicies.yml and then run: $ kubectl apply -f securitypolicies.yml -n greencompute $ kubectl describe ImagePolicy postgresql-image-policy -n greencompute Use helm to install the release. Here is an example $ export PSWD = $( k get secret postgresql-pwd-secret -n greencompute -o jsonpath = \"{.data.postgresql-password}\" | base64 --decode ; echo ) $ helm install stable/postgresql --name postgresql --namespace greencompute --set postgresqlPassword = $PSWD ,postgresqlDatabase = postgres --tls Access to the database with psql running locally on your computer In one terminal start a port forwarding using: kubectl port-forward postgresql-postgresql-0 5432:5432 &>> /dev/null & . Now we can connect our local psql CLI to the remote server via a command like: $ psql \"dbname=postgres host=127.0.0.1 user=postgres port=5432 password= $PSWD \" postgres = # \\d containers id | character varying ( 255 ) | | not null | brand | character varying ( 255 ) | | | capacity | integer | | not null | created_at | timestamp without time zone | | not null | current_city | character varying ( 255 ) | | | latitude | double precision | | not null | longitude | double precision | | not null | status | integer | | | type | character varying ( 255 ) | | | updated_at | timestamp without time zone | | not null | For more information about the psql tool see this note.","title":"Steps"},{"location":"deployments/postgresql/#troubleshooting","text":"admission webhook \"trust.hooks.securityenforcement.admission.cloud.ibm.com\" denied the request: Deny \"docker.io/bitnami/postgresql:10.7.0\", no matching repositories in ClusterImagePolicy and no ImagePolicies in the \"greencompute\" namespace Be sure to use a ImagePolicy and not a cluster policy when using namespace deployment. Error: release postgresql failed: Internal error occurred: admission webhook \"trust.hooks.securityenforcement.admission.cloud.ibm.com\" denied the request: Deny \"docker.io/bitnami/postgresql:10.7.0\", no matching repositories in the ImagePolicies Be sure to authorize docker.io/* in the ImagePolicy.","title":"Troubleshooting"},{"location":"deployments/postgresql/#more-readings","text":"ICP 2.1 Postgresql install recipe: older recipeusing the configuration user interface in the ICP console. postgresql helm chart explanation and configuration : a must read. Installing postgresql via Helm Reefer container management microservice using Springboot, kafka and postgresql","title":"More Readings"},{"location":"deployments/strimzi/deploy/","text":"Strimzi Kafka deployment on Openshift or Kubernetes Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka or Kafka Connect cluster configuration. Create a namespace or openshift project kubectl create namespace kafka-strimzi oc create project kafka-strimzi Download the strimzi artefacts For the last release github . Then modify the Role binding yaml files with the namespace set in previous step. sed -i '' 's/namespace: .*/namespace: kafka-strimzi/' install/cluster-operator/*RoleBinding*.yaml Define Custom Resource Definition for kafka oc apply -f install/cluster-operator/ -n jb-kafka-strimzi This should create the following resources: Names Resource Command strimzi-cluster-operator Service account oc get sa strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Role oc get clusterrole strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definition oc get customresourcedefinition Deploy Kafka cluster Change the name of the cluster in one the yaml in the examples/kafka folder. Using non presistence: oc apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi oc get kafka # NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS # my-cluster 3 3 # Or kubectl apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi When looking at the pods running we can see the three kafka and zookeeper nodes, but also an entity operator pod. Using persistence: oc apply -f examples/kafka/kafka-persistent.yaml -n jb-kafka-strimzi Topic Operator The role of the Topic Operator is to keep a set of KafkaTopic OpenShift or Kubernetes resources describing Kafka topics in-sync with corresponding Kafka topics. Deploy the operator oc apply -f install/topic-operator/ -n jb-kafka-strimzi This will add the following: Names Resource Command strimzi-topic-operator Service account oc get sa strimzi-topic-operator Role binding oc get rolebinding kafkatopics Custom Resource Definition oc get customresourcedefinition Create a topic Edit a yaml file like the following: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : test labels : strimzi.io/cluster : my-cluster spec : partitions : 1 replicas : 3 config : retention.ms : 7200000 segment.bytes : 1073741824 oc apply -f test.yaml -n jb-kafka-strimzi oc get kafkatopics This creates a topic test in your kafka cluster. Test with producer and consumer pods Verify Docker hub strimzi account to get the lastest image tag. # Start a consumer on test topic oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic test --from-beginning # Start a text producer oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic test # enter text","title":"Kafka deployment with Strimzi"},{"location":"deployments/strimzi/deploy/#strimzi-kafka-deployment-on-openshift-or-kubernetes","text":"Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka or Kafka Connect cluster configuration.","title":"Strimzi Kafka deployment on Openshift or Kubernetes"},{"location":"deployments/strimzi/deploy/#create-a-namespace-or-openshift-project","text":"kubectl create namespace kafka-strimzi oc create project kafka-strimzi","title":"Create a namespace or openshift project"},{"location":"deployments/strimzi/deploy/#download-the-strimzi-artefacts","text":"For the last release github . Then modify the Role binding yaml files with the namespace set in previous step. sed -i '' 's/namespace: .*/namespace: kafka-strimzi/' install/cluster-operator/*RoleBinding*.yaml","title":"Download the strimzi artefacts"},{"location":"deployments/strimzi/deploy/#define-custom-resource-definition-for-kafka","text":"oc apply -f install/cluster-operator/ -n jb-kafka-strimzi This should create the following resources: Names Resource Command strimzi-cluster-operator Service account oc get sa strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Role oc get clusterrole strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definition oc get customresourcedefinition","title":"Define Custom Resource Definition for kafka"},{"location":"deployments/strimzi/deploy/#deploy-kafka-cluster","text":"Change the name of the cluster in one the yaml in the examples/kafka folder. Using non presistence: oc apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi oc get kafka # NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS # my-cluster 3 3 # Or kubectl apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi When looking at the pods running we can see the three kafka and zookeeper nodes, but also an entity operator pod. Using persistence: oc apply -f examples/kafka/kafka-persistent.yaml -n jb-kafka-strimzi","title":"Deploy Kafka cluster"},{"location":"deployments/strimzi/deploy/#topic-operator","text":"The role of the Topic Operator is to keep a set of KafkaTopic OpenShift or Kubernetes resources describing Kafka topics in-sync with corresponding Kafka topics.","title":"Topic Operator"},{"location":"deployments/strimzi/deploy/#deploy-the-operator","text":"oc apply -f install/topic-operator/ -n jb-kafka-strimzi This will add the following: Names Resource Command strimzi-topic-operator Service account oc get sa strimzi-topic-operator Role binding oc get rolebinding kafkatopics Custom Resource Definition oc get customresourcedefinition","title":"Deploy the operator"},{"location":"deployments/strimzi/deploy/#create-a-topic","text":"Edit a yaml file like the following: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : test labels : strimzi.io/cluster : my-cluster spec : partitions : 1 replicas : 3 config : retention.ms : 7200000 segment.bytes : 1073741824 oc apply -f test.yaml -n jb-kafka-strimzi oc get kafkatopics This creates a topic test in your kafka cluster.","title":"Create a topic"},{"location":"deployments/strimzi/deploy/#test-with-producer-and-consumer-pods","text":"Verify Docker hub strimzi account to get the lastest image tag. # Start a consumer on test topic oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic test --from-beginning # Start a text producer oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic test # enter text","title":"Test with producer and consumer pods"},{"location":"deployments/zookeeper/","text":"Zookeeper Deployment Development deployment uses one zookeeper server. For production the replicas is set to 5 to tolerate one planned and one unplanned failure. The service defines 3 ports: one for the inter-server communication, one for client access and one for leader-election. Persistence volumes are needed to provide durable storage. Better to use network storage like NFS or glusterfs. The zookeeper manifests are defined in this project under the deployments/zookeeper/dev folder. We are using our own docker images and the Dockerfile to build this image is in deployments/zookeeper . The image is already pushed to the Docker Hub under ibmcase account. We are providing a script to install zookeeper as a kubernetes environment. First be sure to be connected to your kubernetes cluster then run the following command: $ pwd > refarch-eda/deployments/zookeeper $ ./deployZoopeeker.sh $ kubectl get pods -n greencompute NAME READY STATUS RESTARTS AGE gc-zookeeper-57dc5679bb-bh29q 1 /1 Running 0 1m It creates volume, services and deployment or statefulset. If you want to deploy it in more resilient deployment we provide other manifests under the prod folder. To install: $ ./deployZoopeeker.sh prod Once installed you do not need to reinstall it. We are also delivering a script to remove zookeeper when you are done using it. (./removeZookeeper.sh) When running in production it is better to use separate zookeeper ensemble for each Kafka cluster. Each server should have at least 2 GiB of heap with at least 4 GiB of reserved memory","title":"Zookeeper deployment"},{"location":"deployments/zookeeper/#zookeeper-deployment","text":"Development deployment uses one zookeeper server. For production the replicas is set to 5 to tolerate one planned and one unplanned failure. The service defines 3 ports: one for the inter-server communication, one for client access and one for leader-election. Persistence volumes are needed to provide durable storage. Better to use network storage like NFS or glusterfs. The zookeeper manifests are defined in this project under the deployments/zookeeper/dev folder. We are using our own docker images and the Dockerfile to build this image is in deployments/zookeeper . The image is already pushed to the Docker Hub under ibmcase account. We are providing a script to install zookeeper as a kubernetes environment. First be sure to be connected to your kubernetes cluster then run the following command: $ pwd > refarch-eda/deployments/zookeeper $ ./deployZoopeeker.sh $ kubectl get pods -n greencompute NAME READY STATUS RESTARTS AGE gc-zookeeper-57dc5679bb-bh29q 1 /1 Running 0 1m It creates volume, services and deployment or statefulset. If you want to deploy it in more resilient deployment we provide other manifests under the prod folder. To install: $ ./deployZoopeeker.sh prod Once installed you do not need to reinstall it. We are also delivering a script to remove zookeeper when you are done using it. (./removeZookeeper.sh) When running in production it is better to use separate zookeeper ensemble for each Kafka cluster. Each server should have at least 2 GiB of heap with at least 4 GiB of reserved memory","title":"Zookeeper Deployment"},{"location":"design-patterns/ED-patterns/","text":"Understanding event driven microservice patterns Abstract In this article, we are detailing some of the most import event-driven patterns to be used during your microservice implementation and when adopting kafka as an event backbone. Adopting messaging (Pub/Sub) as a microservice communication backbone involves using at least the following patterns: Decompose by subdomain , event driven microservices are still microservices, so we need to find them, and the domain-driven subdomains is a good approach to identify and classify business function and therefore microservices. With the event storming method, aggregates help to find those subdomain of responsibility. Database per service to enforce each service persists data privately and is accessible only via its API. Services are loosely coupled limiting impact to other service when database schema changes. The database technology is selected from business requirements. The implementation of transactions that span multiple services is complex and enforce using the Saga pattern. Queries that goes over multiple entities is a challenge and CQRS represents an interesting solution. Strangler pattern is used to incrementally migrate an existing, monolytic application by replacing a set of features to a microservice but keep both running in parallel. Applying a domain driven design approach, you may strangle the application using bounded context. But then aS soon as this pattern is applied, you need to assess the co-existence between existing bounded contexts and the new microservices. One of the challenges will be to define where the write and read operations occurs, and how data should be replicated between the contexts. This is where event driven architecture helps. Event sourcing persists the state of a business entity such an Order as a sequence of state-changing events. Command Query Responsibility Segregation helps to separate queries from commands and help to address queries with cross-microservice boundary. Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice interested in other business entities, subscribe to those events and it can update its own states and business entities when receiving such events. Business entity keys needs to be unique, immutable. Event reprocessing with dead letter : event driven microservice may have to call external service via synchronous call, we need to process failure to get response from those service, using event backbone. Transactional outbox : A service command typically needs to update the database and send messages/events. The approach is to use a outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone. Strangler pattern Problem How to migrate a monolytics application to microservice without doing a big bang, redeveloping the application from white page. Replacing and rewritting an existing application can be a huge investment. Rewritting a subset of business functions while running current application in parallel may be relevant and reduce risk and velocity of changes. Solution The approach is to use a \"strangler\" interface to dispatch request to new or old features. Existing features to migrate are selected by trying to isolate sub components. One of main challenge is to isolate data store and how the new microservices and the legacy application are accessing the shared data. Continuous data replication can be a solution to propagate write model to read model. Write model will most likely stays on the monolitic application, change data capture can be used, with event backbone to propagate change to read model. The facade needs to be scalable and not a single point of failure. It needs to support new APIs (RESTful) and old API (most likely SOAP). Event reprocessing with dead letter pattern With event driven microservice, it is not just about pub/sub. There are use cases where the microservice needs to call existing service via an HTTP or RPC call. The call may fail. So what should be the processing to be done to retry and gracefully fail by leveraging the power of topics and the concept of dead letter. This pattern is influenced by the adoption of Kafka as event backbone and the offset management offered by Kafka. Once a message is read from a Kafka topic by a consumer the offset can be automatically committed so the consumer can poll the next batch of events, or in most the case manually committed, to support business logic to process those events. The figure below demonstrates the problem to address: the reefer management microservice get order event to process by allocating a reefer container to the order. The call to update the container inventory fails. At first glance, the approach is to commit the offset only when the three internal operations are succesful: write to reefer database (2), update the container inventory using legacy application service (3), and produce new event to orders (4) and containers (5) topics. Step (4) and (5) will not be done as no response from (3) happened. In fact a better approach is to commit the read offset on the orders topic, and then starts the processing: the goal is to do not impact the input throughput. In case of step (2) or (3) fails the order data is published to an order-retries topic (4) with some added metadata like number of retries and timestamp. A new order retry service or function consumes the order retry events (5) and do a new call to the remote service using a delay according to the number of retry already done: this is to pace the calls to a service that has issue for longer time. If the call (6) fails this function creates a new event in the order-retries topic with a retry counter increased by one. If the number of retry reaches a certain threshold then the order is sent to order-dead-letter topic (7) for human to work on. A CLI could read from this dead letter topic to deal with the data, or retry automatically once we know the backend service works. Using this approach we delay the call to the remote service without putting too much preassure on it. For more detail we recommend this article from Uber engineering: Building Reliable Reprocessing and Dead Letter Queues with Apache Kafka .","title":"Event-driven patterns"},{"location":"design-patterns/ED-patterns/#understanding-event-driven-microservice-patterns","text":"Abstract In this article, we are detailing some of the most import event-driven patterns to be used during your microservice implementation and when adopting kafka as an event backbone. Adopting messaging (Pub/Sub) as a microservice communication backbone involves using at least the following patterns: Decompose by subdomain , event driven microservices are still microservices, so we need to find them, and the domain-driven subdomains is a good approach to identify and classify business function and therefore microservices. With the event storming method, aggregates help to find those subdomain of responsibility. Database per service to enforce each service persists data privately and is accessible only via its API. Services are loosely coupled limiting impact to other service when database schema changes. The database technology is selected from business requirements. The implementation of transactions that span multiple services is complex and enforce using the Saga pattern. Queries that goes over multiple entities is a challenge and CQRS represents an interesting solution. Strangler pattern is used to incrementally migrate an existing, monolytic application by replacing a set of features to a microservice but keep both running in parallel. Applying a domain driven design approach, you may strangle the application using bounded context. But then aS soon as this pattern is applied, you need to assess the co-existence between existing bounded contexts and the new microservices. One of the challenges will be to define where the write and read operations occurs, and how data should be replicated between the contexts. This is where event driven architecture helps. Event sourcing persists the state of a business entity such an Order as a sequence of state-changing events. Command Query Responsibility Segregation helps to separate queries from commands and help to address queries with cross-microservice boundary. Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice interested in other business entities, subscribe to those events and it can update its own states and business entities when receiving such events. Business entity keys needs to be unique, immutable. Event reprocessing with dead letter : event driven microservice may have to call external service via synchronous call, we need to process failure to get response from those service, using event backbone. Transactional outbox : A service command typically needs to update the database and send messages/events. The approach is to use a outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone.","title":"Understanding event driven microservice patterns"},{"location":"design-patterns/ED-patterns/#strangler-pattern","text":"","title":"Strangler pattern"},{"location":"design-patterns/ED-patterns/#problem","text":"How to migrate a monolytics application to microservice without doing a big bang, redeveloping the application from white page. Replacing and rewritting an existing application can be a huge investment. Rewritting a subset of business functions while running current application in parallel may be relevant and reduce risk and velocity of changes.","title":"Problem"},{"location":"design-patterns/ED-patterns/#solution","text":"The approach is to use a \"strangler\" interface to dispatch request to new or old features. Existing features to migrate are selected by trying to isolate sub components. One of main challenge is to isolate data store and how the new microservices and the legacy application are accessing the shared data. Continuous data replication can be a solution to propagate write model to read model. Write model will most likely stays on the monolitic application, change data capture can be used, with event backbone to propagate change to read model. The facade needs to be scalable and not a single point of failure. It needs to support new APIs (RESTful) and old API (most likely SOAP).","title":"Solution"},{"location":"design-patterns/ED-patterns/#event-reprocessing-with-dead-letter-pattern","text":"With event driven microservice, it is not just about pub/sub. There are use cases where the microservice needs to call existing service via an HTTP or RPC call. The call may fail. So what should be the processing to be done to retry and gracefully fail by leveraging the power of topics and the concept of dead letter. This pattern is influenced by the adoption of Kafka as event backbone and the offset management offered by Kafka. Once a message is read from a Kafka topic by a consumer the offset can be automatically committed so the consumer can poll the next batch of events, or in most the case manually committed, to support business logic to process those events. The figure below demonstrates the problem to address: the reefer management microservice get order event to process by allocating a reefer container to the order. The call to update the container inventory fails. At first glance, the approach is to commit the offset only when the three internal operations are succesful: write to reefer database (2), update the container inventory using legacy application service (3), and produce new event to orders (4) and containers (5) topics. Step (4) and (5) will not be done as no response from (3) happened. In fact a better approach is to commit the read offset on the orders topic, and then starts the processing: the goal is to do not impact the input throughput. In case of step (2) or (3) fails the order data is published to an order-retries topic (4) with some added metadata like number of retries and timestamp. A new order retry service or function consumes the order retry events (5) and do a new call to the remote service using a delay according to the number of retry already done: this is to pace the calls to a service that has issue for longer time. If the call (6) fails this function creates a new event in the order-retries topic with a retry counter increased by one. If the number of retry reaches a certain threshold then the order is sent to order-dead-letter topic (7) for human to work on. A CLI could read from this dead letter topic to deal with the data, or retry automatically once we know the backend service works. Using this approach we delay the call to the remote service without putting too much preassure on it. For more detail we recommend this article from Uber engineering: Building Reliable Reprocessing and Dead Letter Queues with Apache Kafka .","title":"Event reprocessing with dead letter pattern"},{"location":"design-patterns/cqrs/","text":"Command Query Responsibility Segregation (CQRS) pattern Problems and Constraints A domain model encapsulates domain data with the behavior for maintaining the correctness of that data as it is modified, structuring the data based on how it is stored in the database and to facilitate managing the data. Multiple clients can independently update the data concurrently. Different clients may not use the data the way the domain model structures it, and may not agree with each other on how it should be structured. When a domain model becomes overburdened managing complex aggregate objects, concurrent updates, and numerous cross-cutting views, how can it be refactored to separate different aspects of how the data is used? An application accesses data both to read and to modify it. The primitive data tasks are often expressed as create, read, update, and delete (CRUD); using them is known as CRUDing the data. Application code often does not make much distinction between the tasks; individual operations may mix reading the data with changing the data as needed. This simple approach works well when all clients of the data can use the same structure and contention is low. A single domain model can manage the data, make it accessible as domain objects, and ensure updates maintain its consistency. However, this approach becomes inadaquate when different clients want different views across multiple sets of data, when the data is too widely used, and/or when multiple clients updating the data my unknowlingly conflict with each other. For example, in a microservices architecture, each microservice should store and manage its own data, but a user interface may need to display data from several microservices. A query that gathers bits of data from multiple sources can be inefficient (time and bandwidth consumed accessing multiple data sources, CPU consumed transforming data, memory consumed by intermediate objects) and must be repeated each time the data is accessed. Another example is an enterprise database of record managing data required by multiple applications. It can become overloaded with too many clients needing too many connections to run too many threads performing too many transactions--such that the database becomes a performance bottleneck and can even crash. Another example is maintaining consistency of the data while clients concurrently make independent updates to the data. While each update may be consistent, they may conflict with each other. Database locking ensures that the updates don't change the same data concurrently, but doesn't ensure multiple independent changes result in a consistent data model. When data usage is more complex than a single domain model can facilitate, a more sophisticated approach is needed. Solution and Pattern Refactor a domain model to separate operations for querying data and operations for updating data so that they may be handled independently. The CQRS pattern strictly segregates operations that read data from operations that update data. An operation can read data (the R in CRUD) or can write data (the CUD in CRUD), but not both. This separation can make using data much more manageable in several respects. The read operations and the write operations are simpler to implement because their functionality is more finely focused. The operations can be developed independently, potentially by separate teams. The operations can be optimized independently and can evolve independently, following changing user requirements more easily. These optimized operations can scale better, perform better, and security can be applied more precisely. The full CQRS pattern uses separate read and write databases. In doing so, the pattern segregates not just the APIs for accessing data or the models for managing data, but even segregates the database itself into two, a read/write database that is effectively write-only and one or more read-only databases. The adoption of the pattern can be applied in phases, incrementally from existing code. To illustrate this, we will use four stages that could be used incrementally or a developer can go directly from stage 0 to 3 without considering the others: Stage 0: Typical application data access Stage 1: Separate read and write APIs Stage 2: Separate read and write models Stage 3: Separate read and write databases Typical application data access Before even beginning to apply the pattern, let\u2019s consider the typical app design for accessing data. This diagram shows an app with a domain model for accessing data persisted in a database of record, i.e. a single source of truth for that data. The domain model has an API that at a minimum enables clients to perform CRUD tasks on domain objects within the model. The domain model is an object representation of the database documents or records. It is comprised of domain objects that represent individual documents or records and the business logic for managing and using them. Domain-Driven Design (DDD) models these domain objects as entities \u2014\u201cobjects that have a distinct identity that runs through time and different representations\u201d\u2014and aggregates \u2014\u201ca cluster of domain objects that can be treated as a single unit\u201d; the aggregate root maintains the integrity of the aggregate as a whole. Ideally, the domain model\u2019s API should be more domain-specific than simply CRUDing of data. Instead, it should expose higher-level operations that represent business functionality like findCustomer() , placeOrder() , transferFunds() , and so on. These operations read and update data as needed, sometimes doing both in a single operation. They are correct as long as they fit the way the business works. Separate read and write APIs The first and most visible step in applying the CQRS pattern is splitting the CRUD API into separate read and write APIs. This diagram shows the same domain model as before, but its single CRUD API is split into retrieve and modify APIs. The two APIs share the existing domain model but split the behavior: Read : The retrieve API is used to read the existing state of the objects in the domain model without changing that state. The API treats the domain state as read only. Write : The modify API is used to change the objects in the domain model. Changes are made using CUD tasks: create new domain objects, update the state in existing ones, and delete ones that are no longer needed. The operations in this API do not return result values, they return success (ack or void) or failure (nak or throw an exception). The create operation might return the primary of key of the entity, which can be generated either by the domain model or in the data source. This separation of APIs is an application of the Command Query Separation (CQS) pattern, which says to clearly separate methods that change state from those that don\u2019t. To do so, each of an object\u2019s methods can be in one of two categories (but not both): Query : Returns a result. Does not change the system\u2019s state nor cause any side effects that change the state. Command (a.k.a. modifiers or mutators): Changes the state of a system. Does not return a value, just an indication of success or failure. With this approach, the domain model works the same and provides access to the data the same as before. What has changed is the API for using the domain model. Whereas a higher-level operation might previously have both changed the state of the application and returned a part of that state, now each such operation is redesigned to only do one or the other. When the domain model splits its API into read and write operations, clients using the API must likewise split their functionality into querying and updating functionality. Most new web based applications are based in the single page application, with components and services that use and encapsulate remote API. So this separation of backend API fits well with modern web applications. This stage depends on the domain model being able to implement both the retrieve and modify APIs. A single domain model requires the retrieve and modify behavior to have similar, corresponding implementations. For them to evolve independently, the two APIs will need to be implemented with separate read and write models. Separate read and write models The second step in applying the CQRS pattern is to split the domain model into separate read and write models. This doesn\u2019t just change the API for accessing domain functionality, it also changes the design of how that functionality is structured and implemented. This diagram shows that the domain model becomes the basis for a write model that handles changes to the domain objects, along with a separate read model used to access the state of the app. Naturally, the read model implements the retrieve API and the write model implements the modify API. Now, the application consists not only of separate APIs for querying and updating the domain objects, there\u2019s also separate business functionality for doing so. Both the read business functionality and the write business functionality share the same database. The write model is implemented by specializing the domain model to focus solely on maintaining the valid structure of domain objects when changing their state and by applying any business rules. Meanwhile, responsibility for returning domain objects is shifted to a separate read model. The read model defines data transfer objects (DTOs) designed specifically for the model to return just the data the client wants in a structure the client finds convenient. The read model knows how to gather the data used to populate the DTOs. DTOs encapsulate little if any domain functionality, they just bundle data into a convenient package that can easily be transmitted using a single method call, especially between processes. The read model should be able to implement the retrieve API by implementing the necessary queries and executing them. If the retrieve API is already built to return domain objects as results, the read model can continue to do so, or better yet, implements DTO types that are compatible with the domain objects and returns those. Likewise, the modify API was already implemented using the domain model, so the write model should preserve that. The write model may enhance the implementation to more explicitly implement a command interface or use command objects. This phase assumes that the read and write models can both be implemented using the same database of record the domain model has been using. To the extent this is true, the implementations of reading and writing can evolve independently and be optimized independently. This independence may become increasingly limited since they are both bound to the same database with a single schema or data model. To enable the read and write models to evolve independently, they may each need their own database. Separate read and write databases The third step in applying the CQRS pattern\u2014-which implements the complete CQRS pattern solution\u2014-is splitting the database of record into separate read and write databases. This diagram shows the write model and read model, each supported by its own database. The overall solution consists of two main parts: the write solution that supports updating the data and the read solution that supports querying the data. The two parts are connected by the event bus. The write model has its own read/write database and the read model has its own read-only database. The read/write database still serves as the database of record (the single source of truth for the data) but is mostly used write-only: mostly written to and rarely read. Reading is offloaded onto a separate read database that contains the same data but is used read-only. The query database is effectively a cache of the database of record, with all of the inherit benefits and complexity of the Caching pattern. The query database contains a copy of the data in the database of record, with the copy structured and staged for easier access by the clients using the retrieve API. As a copy, overhead is needed to keep the copy synchronized with changes in the original. Latency in this synchronization process creates eventual consistency, during which the data copy is stale. The separate databases enable the separate read and write models and their respective retrieve and modify APIs to truly evolve independently. Not only can the read model or write model\u2019s implementation change without changing the other, but how each stores its data can be changed independently. This solution offers the following advantages: Scaling : The query load is moved from the write database to the read database. If the database of record is a scalability bottleneck and a lot of the load on it is caused by queries, unloading those query responsibilities can significantly improve the scalability of the combined data access. Performance : The schemas of the two databases can be different, enabling them to be designed and optimized independently for better performance. The write database can be optimized for data consistency and correctness, with capabilities such as stored procedures that fit the write model and assist with data updates. The read database can store the data in units that better fit the read model and are better optimized for querying, with larger rows requiring fewer joins. Notice that the design for this stage is significantly more complex than the design for the previous stage. Separate databases with copies of the same data may make data modeling and using data easier, but they require significant overhead to synchronize the data and keep the copies consistent. CQRS employs a couple of design features that support keeping the databases synchronized: Command Bus for queuing commands (optional): A more subtle and optional design decision is to queue the commands produced by the modify API, shown in the diagram as the command bus. This can significantly increase the throughput of multiple apps updating the database, as well as serialize updates to help avoid--or at least detect--merge conflicts. With the bus, a client making an update does not block synchronously while the change is written to the database. Rather, the request to change the database is captured as a command ( Design Patterns ) and put on a message queue, after which the client can proceed with other work. Asynchronously in the background, the write model processes the commands at the maximum sustainable rate that the database can handle, without the database ever becoming overloaded. If the database becomes temporarily unavailable, the commands queue and will be processed when the database becomes available once more. Event Bus for publishing update events (required): Whenever the write database is updated, a change notification is published as an event on the event bus. Interested parties can subscribe to the event bus to be notified when the database is updated. One such party is an event processor for the query database, which receives update events and processes them by updating the query database accordingly. In this way, every time the write database is updated, a corresponding update is made to the read database to keep it in sync. The connection between the command bus and the event bus is facilitated by an application of the Event Sourcing pattern , which keeps a change log that is suitable for publishing. Event sourcing maintains not only the current state of the data but also the history of how that current state was reached. For each command on the command bus, the write model performs these tasks to process the command: Logs the change Updates the database with the change Creates an update event describing the change and publishes it to the event bus The changes that are logged can be the commands from the command bus or the update events published to the event bus Considerations Keep these decisions in mind while applying this pattern: Client impact : Applying CQRS not only changes how data is stored and accessed, but also changes the APIs that clients use to access data. This means that each client must be redesigned to use the new APIs. Riskiness : A lot of the complexity of the pattern solution involves duplicating the data in two databases and keeping them synchronized. Risk comes from querying data that is stale or downright wrong because of problems with the synchronization. Eventual consistency : Clients querying data must expect that updates will have latency. In a microservices architecture, eventual data consistency is a given and acceptable in many of cases. Command queuing : Using a command bus as part of the write solution to queue the commands is optional but powerful. In addition to the benefits of queuing, the command objects can easily be stored in the change log and easily be converted into notification events. (In the next section, we illustrate a way to use event bus to queue commands as well.) Change log : The log of changes to the database of record can be either the list of commands from the command bus or the list of event notifications published on the event bus. The Event Sourcing pattern assumes it\u2019s a log of events, but that pattern doesn\u2019t include the command bus. An event list may be easier to scan as a history, whereas a command list is easier to replay. Create keys : Strick interpretation of the Command Query Separation (CQS) pattern says that command operations do not have return types. A possible exception is commands that create data: An operation that creates a new record or document typically returns the key for accessing the new data, which is convenient for the client. However, if the create operation is invoked asynchronously by a command on a command bus, the write model will need to perform a callback on the client to return the key. Messaging queues and topics : While messaging is used to implement both the command bus and event bus, the two busses use messaging differently. The command bus guarantees exactly once delivery. The event bus broadcasts each event to all interested event processors. Query database persistence : The database of record is always persistent. The query database is a cache that can be a persistent cache or an in-memory cache. If the cache is in-memory and is lost, it must be rebuilt completely from the database of record. Security : Controls on reading data and updating data can be applied separately using the two parts of the solution. Combining event sourcing and CQRS The CQRS application pattern is frequently associated with event sourcing: when doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence. As introduced in previous section, creations and updates are done as state notification events (change of state), and are persisted in the event log/store. The following figure, presents two separate microservices, one supporting the write model, and multiple other supporting the queries: The query part is separate processes that consume change log events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database, or distributed cache... They do not need to be in the same language. With CQRS and ES the projections are retroactives. New query equals implementing new projection and read the events from the beginning of time or the recent committed state and snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins. The event backbone, use a pub/sub model, and Kafka is a good candidate as an implementation technology. With this structure, the Read model microservice will most likely consume events from multiple topics to build the data projection based on joining those data streams. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines. We can note that, we can separate the API definition and management in a API gateway. The shipment order microservice is implementing this pattern. Some implementation items to consider: Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the write model and one for the read model. On write model, strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable. Scalability : Separating read and write as two different microservices allows for high availability. Caching at the read level can be used to increase performance response time, and can be deployed as multiple standalone instances (Pods in Kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries. Availability : The write model sacrafices consistency for availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still is able to read them as they are served by different databases and services. With CQRS, the write model can evolve over time without impacting the read model, as long as the event model doesn't change. The read model requires additional tables, but they are often simpler and easier to understand. CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex. Some challenges to always consider: How to support event structure version management? How much data to keep in the event store (history)? How to adopt data duplication which results to eventual data consistency?. The CQRS pattern was introduced by Greg Young , and described in Martin Fowler's work on microservices. As you can see in previous figure, as soon as we see two arrows from the same component, we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We detail this in next section. The consistency challenge As introduced in the previous section, there is a potential problem of data inconsistency: once a command saves changes into the database, the consumers do not see the new or updated data until event notification completes processing. With traditional Java service, using JPA and JMS, the save and send operations can be part of the same XA transaction and both succeed or fail. With event sourcing pattern, the source of trust is the event source, which acts as a version control system, as shown in the diagram below. The steps for syncronizing changes to the data are: 1. The write model creates the event and publishes it 1. The consumer receives the event and extracts its payload 1. The consumer updates its local datasource with the payload data 1. If the consumer fails to process the update, it can persist the event to an error log 1. Each error in the log can be replayed 1. A command line interface replays an event via an admin API, which searches in the topic using this order id to replay the save operation This implementation causes a problem for the createOrder(order): string operation: The Order Service is supposed to return the new order complete with the order id that is a unique key, a key most likely created by the database. If updating the database fails, there is no new order yet and so no database key to use as the order ID. To avoid this problem, if the underlying technology supports assigning the new order's key, the service can generate the order ID and use that as the order's key in the database. It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note. CQRS and Change Data Capture There are other ways to support this dual operations level: When using Kafka, Kafka Connect has the capability to subscribe to databases via JDBC, allowing to poll tables for updates and then produce events to Kafka. There is an open-source change data capture solution based on extracting change events from database transaction logs, Debezium that helps to respond to insert, update and delete operations on databases and generate events accordingly. It supports databases like MySQL, Postgres, MongoDB and others. Write the order to the database and in the same transaction write to an event table ( \"outbox pattern\" ). Then use a polling to get the events to send to Kafka from this event table and delete the row in the table once the event is sent. Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour . The CQRS implementation using CDC will look like in the following diagram: What is important to note is that the event needs to be flexible on the data payload. We are presenting a event model in the reference implementation. On the view side, updates to the view part need to be idempotent. Delay in the view There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale. When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application like our kc- user interface , the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation: @POST public Response create ( OrderCreate dto ) { Order order = new Order ( UUID . randomUUID (). toString (), dto . getProductID (),...); // ... return Response . ok (). entity ( order ). build () } Schema change What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures. Code reference The following project includes two sub modules, each deployable as a microservice to illustrate the command and query part: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms Compendium https://www.codeproject.com/Articles/555855/Introduction-to-CQRS http://udidahan.com/2009/12/09/clarified-cqrs https://martinfowler.com/bliki/CQRS.html https://microservices.io/patterns/data/cqrs.html https://community.risingstack.com/when-to-use-cqrs https://dzone.com/articles/concepts-of-cqrs https://martinfowler.com/bliki/CommandQuerySeparation.html https://www.martinfowler.com/eaaCatalog/domainModel.html https://dddcommunity.org/learning-ddd/what_is_ddd/ https://martinfowler.com/bliki/EvansClassification.html https://martinfowler.com/bliki/DDD_Aggregate.html https://martinfowler.com/eaaCatalog/dataTransferObject.html https://en.wikipedia.org/wiki/Command_pattern https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusable-Object-Oriented-Software/PGM14333.html","title":"CQRS"},{"location":"design-patterns/cqrs/#command-query-responsibility-segregation-cqrs-pattern","text":"","title":"Command Query Responsibility Segregation (CQRS) pattern"},{"location":"design-patterns/cqrs/#problems-and-constraints","text":"A domain model encapsulates domain data with the behavior for maintaining the correctness of that data as it is modified, structuring the data based on how it is stored in the database and to facilitate managing the data. Multiple clients can independently update the data concurrently. Different clients may not use the data the way the domain model structures it, and may not agree with each other on how it should be structured. When a domain model becomes overburdened managing complex aggregate objects, concurrent updates, and numerous cross-cutting views, how can it be refactored to separate different aspects of how the data is used? An application accesses data both to read and to modify it. The primitive data tasks are often expressed as create, read, update, and delete (CRUD); using them is known as CRUDing the data. Application code often does not make much distinction between the tasks; individual operations may mix reading the data with changing the data as needed. This simple approach works well when all clients of the data can use the same structure and contention is low. A single domain model can manage the data, make it accessible as domain objects, and ensure updates maintain its consistency. However, this approach becomes inadaquate when different clients want different views across multiple sets of data, when the data is too widely used, and/or when multiple clients updating the data my unknowlingly conflict with each other. For example, in a microservices architecture, each microservice should store and manage its own data, but a user interface may need to display data from several microservices. A query that gathers bits of data from multiple sources can be inefficient (time and bandwidth consumed accessing multiple data sources, CPU consumed transforming data, memory consumed by intermediate objects) and must be repeated each time the data is accessed. Another example is an enterprise database of record managing data required by multiple applications. It can become overloaded with too many clients needing too many connections to run too many threads performing too many transactions--such that the database becomes a performance bottleneck and can even crash. Another example is maintaining consistency of the data while clients concurrently make independent updates to the data. While each update may be consistent, they may conflict with each other. Database locking ensures that the updates don't change the same data concurrently, but doesn't ensure multiple independent changes result in a consistent data model. When data usage is more complex than a single domain model can facilitate, a more sophisticated approach is needed.","title":"Problems and Constraints"},{"location":"design-patterns/cqrs/#solution-and-pattern","text":"Refactor a domain model to separate operations for querying data and operations for updating data so that they may be handled independently. The CQRS pattern strictly segregates operations that read data from operations that update data. An operation can read data (the R in CRUD) or can write data (the CUD in CRUD), but not both. This separation can make using data much more manageable in several respects. The read operations and the write operations are simpler to implement because their functionality is more finely focused. The operations can be developed independently, potentially by separate teams. The operations can be optimized independently and can evolve independently, following changing user requirements more easily. These optimized operations can scale better, perform better, and security can be applied more precisely. The full CQRS pattern uses separate read and write databases. In doing so, the pattern segregates not just the APIs for accessing data or the models for managing data, but even segregates the database itself into two, a read/write database that is effectively write-only and one or more read-only databases. The adoption of the pattern can be applied in phases, incrementally from existing code. To illustrate this, we will use four stages that could be used incrementally or a developer can go directly from stage 0 to 3 without considering the others: Stage 0: Typical application data access Stage 1: Separate read and write APIs Stage 2: Separate read and write models Stage 3: Separate read and write databases","title":"Solution and Pattern"},{"location":"design-patterns/cqrs/#typical-application-data-access","text":"Before even beginning to apply the pattern, let\u2019s consider the typical app design for accessing data. This diagram shows an app with a domain model for accessing data persisted in a database of record, i.e. a single source of truth for that data. The domain model has an API that at a minimum enables clients to perform CRUD tasks on domain objects within the model. The domain model is an object representation of the database documents or records. It is comprised of domain objects that represent individual documents or records and the business logic for managing and using them. Domain-Driven Design (DDD) models these domain objects as entities \u2014\u201cobjects that have a distinct identity that runs through time and different representations\u201d\u2014and aggregates \u2014\u201ca cluster of domain objects that can be treated as a single unit\u201d; the aggregate root maintains the integrity of the aggregate as a whole. Ideally, the domain model\u2019s API should be more domain-specific than simply CRUDing of data. Instead, it should expose higher-level operations that represent business functionality like findCustomer() , placeOrder() , transferFunds() , and so on. These operations read and update data as needed, sometimes doing both in a single operation. They are correct as long as they fit the way the business works.","title":"Typical application data access"},{"location":"design-patterns/cqrs/#separate-read-and-write-apis","text":"The first and most visible step in applying the CQRS pattern is splitting the CRUD API into separate read and write APIs. This diagram shows the same domain model as before, but its single CRUD API is split into retrieve and modify APIs. The two APIs share the existing domain model but split the behavior: Read : The retrieve API is used to read the existing state of the objects in the domain model without changing that state. The API treats the domain state as read only. Write : The modify API is used to change the objects in the domain model. Changes are made using CUD tasks: create new domain objects, update the state in existing ones, and delete ones that are no longer needed. The operations in this API do not return result values, they return success (ack or void) or failure (nak or throw an exception). The create operation might return the primary of key of the entity, which can be generated either by the domain model or in the data source. This separation of APIs is an application of the Command Query Separation (CQS) pattern, which says to clearly separate methods that change state from those that don\u2019t. To do so, each of an object\u2019s methods can be in one of two categories (but not both): Query : Returns a result. Does not change the system\u2019s state nor cause any side effects that change the state. Command (a.k.a. modifiers or mutators): Changes the state of a system. Does not return a value, just an indication of success or failure. With this approach, the domain model works the same and provides access to the data the same as before. What has changed is the API for using the domain model. Whereas a higher-level operation might previously have both changed the state of the application and returned a part of that state, now each such operation is redesigned to only do one or the other. When the domain model splits its API into read and write operations, clients using the API must likewise split their functionality into querying and updating functionality. Most new web based applications are based in the single page application, with components and services that use and encapsulate remote API. So this separation of backend API fits well with modern web applications. This stage depends on the domain model being able to implement both the retrieve and modify APIs. A single domain model requires the retrieve and modify behavior to have similar, corresponding implementations. For them to evolve independently, the two APIs will need to be implemented with separate read and write models.","title":"Separate read and write APIs"},{"location":"design-patterns/cqrs/#separate-read-and-write-models","text":"The second step in applying the CQRS pattern is to split the domain model into separate read and write models. This doesn\u2019t just change the API for accessing domain functionality, it also changes the design of how that functionality is structured and implemented. This diagram shows that the domain model becomes the basis for a write model that handles changes to the domain objects, along with a separate read model used to access the state of the app. Naturally, the read model implements the retrieve API and the write model implements the modify API. Now, the application consists not only of separate APIs for querying and updating the domain objects, there\u2019s also separate business functionality for doing so. Both the read business functionality and the write business functionality share the same database. The write model is implemented by specializing the domain model to focus solely on maintaining the valid structure of domain objects when changing their state and by applying any business rules. Meanwhile, responsibility for returning domain objects is shifted to a separate read model. The read model defines data transfer objects (DTOs) designed specifically for the model to return just the data the client wants in a structure the client finds convenient. The read model knows how to gather the data used to populate the DTOs. DTOs encapsulate little if any domain functionality, they just bundle data into a convenient package that can easily be transmitted using a single method call, especially between processes. The read model should be able to implement the retrieve API by implementing the necessary queries and executing them. If the retrieve API is already built to return domain objects as results, the read model can continue to do so, or better yet, implements DTO types that are compatible with the domain objects and returns those. Likewise, the modify API was already implemented using the domain model, so the write model should preserve that. The write model may enhance the implementation to more explicitly implement a command interface or use command objects. This phase assumes that the read and write models can both be implemented using the same database of record the domain model has been using. To the extent this is true, the implementations of reading and writing can evolve independently and be optimized independently. This independence may become increasingly limited since they are both bound to the same database with a single schema or data model. To enable the read and write models to evolve independently, they may each need their own database.","title":"Separate read and write models"},{"location":"design-patterns/cqrs/#separate-read-and-write-databases","text":"The third step in applying the CQRS pattern\u2014-which implements the complete CQRS pattern solution\u2014-is splitting the database of record into separate read and write databases. This diagram shows the write model and read model, each supported by its own database. The overall solution consists of two main parts: the write solution that supports updating the data and the read solution that supports querying the data. The two parts are connected by the event bus. The write model has its own read/write database and the read model has its own read-only database. The read/write database still serves as the database of record (the single source of truth for the data) but is mostly used write-only: mostly written to and rarely read. Reading is offloaded onto a separate read database that contains the same data but is used read-only. The query database is effectively a cache of the database of record, with all of the inherit benefits and complexity of the Caching pattern. The query database contains a copy of the data in the database of record, with the copy structured and staged for easier access by the clients using the retrieve API. As a copy, overhead is needed to keep the copy synchronized with changes in the original. Latency in this synchronization process creates eventual consistency, during which the data copy is stale. The separate databases enable the separate read and write models and their respective retrieve and modify APIs to truly evolve independently. Not only can the read model or write model\u2019s implementation change without changing the other, but how each stores its data can be changed independently. This solution offers the following advantages: Scaling : The query load is moved from the write database to the read database. If the database of record is a scalability bottleneck and a lot of the load on it is caused by queries, unloading those query responsibilities can significantly improve the scalability of the combined data access. Performance : The schemas of the two databases can be different, enabling them to be designed and optimized independently for better performance. The write database can be optimized for data consistency and correctness, with capabilities such as stored procedures that fit the write model and assist with data updates. The read database can store the data in units that better fit the read model and are better optimized for querying, with larger rows requiring fewer joins. Notice that the design for this stage is significantly more complex than the design for the previous stage. Separate databases with copies of the same data may make data modeling and using data easier, but they require significant overhead to synchronize the data and keep the copies consistent. CQRS employs a couple of design features that support keeping the databases synchronized: Command Bus for queuing commands (optional): A more subtle and optional design decision is to queue the commands produced by the modify API, shown in the diagram as the command bus. This can significantly increase the throughput of multiple apps updating the database, as well as serialize updates to help avoid--or at least detect--merge conflicts. With the bus, a client making an update does not block synchronously while the change is written to the database. Rather, the request to change the database is captured as a command ( Design Patterns ) and put on a message queue, after which the client can proceed with other work. Asynchronously in the background, the write model processes the commands at the maximum sustainable rate that the database can handle, without the database ever becoming overloaded. If the database becomes temporarily unavailable, the commands queue and will be processed when the database becomes available once more. Event Bus for publishing update events (required): Whenever the write database is updated, a change notification is published as an event on the event bus. Interested parties can subscribe to the event bus to be notified when the database is updated. One such party is an event processor for the query database, which receives update events and processes them by updating the query database accordingly. In this way, every time the write database is updated, a corresponding update is made to the read database to keep it in sync. The connection between the command bus and the event bus is facilitated by an application of the Event Sourcing pattern , which keeps a change log that is suitable for publishing. Event sourcing maintains not only the current state of the data but also the history of how that current state was reached. For each command on the command bus, the write model performs these tasks to process the command: Logs the change Updates the database with the change Creates an update event describing the change and publishes it to the event bus The changes that are logged can be the commands from the command bus or the update events published to the event bus","title":"Separate read and write databases"},{"location":"design-patterns/cqrs/#considerations","text":"Keep these decisions in mind while applying this pattern: Client impact : Applying CQRS not only changes how data is stored and accessed, but also changes the APIs that clients use to access data. This means that each client must be redesigned to use the new APIs. Riskiness : A lot of the complexity of the pattern solution involves duplicating the data in two databases and keeping them synchronized. Risk comes from querying data that is stale or downright wrong because of problems with the synchronization. Eventual consistency : Clients querying data must expect that updates will have latency. In a microservices architecture, eventual data consistency is a given and acceptable in many of cases. Command queuing : Using a command bus as part of the write solution to queue the commands is optional but powerful. In addition to the benefits of queuing, the command objects can easily be stored in the change log and easily be converted into notification events. (In the next section, we illustrate a way to use event bus to queue commands as well.) Change log : The log of changes to the database of record can be either the list of commands from the command bus or the list of event notifications published on the event bus. The Event Sourcing pattern assumes it\u2019s a log of events, but that pattern doesn\u2019t include the command bus. An event list may be easier to scan as a history, whereas a command list is easier to replay. Create keys : Strick interpretation of the Command Query Separation (CQS) pattern says that command operations do not have return types. A possible exception is commands that create data: An operation that creates a new record or document typically returns the key for accessing the new data, which is convenient for the client. However, if the create operation is invoked asynchronously by a command on a command bus, the write model will need to perform a callback on the client to return the key. Messaging queues and topics : While messaging is used to implement both the command bus and event bus, the two busses use messaging differently. The command bus guarantees exactly once delivery. The event bus broadcasts each event to all interested event processors. Query database persistence : The database of record is always persistent. The query database is a cache that can be a persistent cache or an in-memory cache. If the cache is in-memory and is lost, it must be rebuilt completely from the database of record. Security : Controls on reading data and updating data can be applied separately using the two parts of the solution.","title":"Considerations"},{"location":"design-patterns/cqrs/#combining-event-sourcing-and-cqrs","text":"The CQRS application pattern is frequently associated with event sourcing: when doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence. As introduced in previous section, creations and updates are done as state notification events (change of state), and are persisted in the event log/store. The following figure, presents two separate microservices, one supporting the write model, and multiple other supporting the queries: The query part is separate processes that consume change log events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database, or distributed cache... They do not need to be in the same language. With CQRS and ES the projections are retroactives. New query equals implementing new projection and read the events from the beginning of time or the recent committed state and snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins. The event backbone, use a pub/sub model, and Kafka is a good candidate as an implementation technology. With this structure, the Read model microservice will most likely consume events from multiple topics to build the data projection based on joining those data streams. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines. We can note that, we can separate the API definition and management in a API gateway. The shipment order microservice is implementing this pattern. Some implementation items to consider: Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the write model and one for the read model. On write model, strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable. Scalability : Separating read and write as two different microservices allows for high availability. Caching at the read level can be used to increase performance response time, and can be deployed as multiple standalone instances (Pods in Kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries. Availability : The write model sacrafices consistency for availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still is able to read them as they are served by different databases and services. With CQRS, the write model can evolve over time without impacting the read model, as long as the event model doesn't change. The read model requires additional tables, but they are often simpler and easier to understand. CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex. Some challenges to always consider: How to support event structure version management? How much data to keep in the event store (history)? How to adopt data duplication which results to eventual data consistency?. The CQRS pattern was introduced by Greg Young , and described in Martin Fowler's work on microservices. As you can see in previous figure, as soon as we see two arrows from the same component, we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We detail this in next section.","title":"Combining event sourcing and CQRS"},{"location":"design-patterns/cqrs/#the-consistency-challenge","text":"As introduced in the previous section, there is a potential problem of data inconsistency: once a command saves changes into the database, the consumers do not see the new or updated data until event notification completes processing. With traditional Java service, using JPA and JMS, the save and send operations can be part of the same XA transaction and both succeed or fail. With event sourcing pattern, the source of trust is the event source, which acts as a version control system, as shown in the diagram below. The steps for syncronizing changes to the data are: 1. The write model creates the event and publishes it 1. The consumer receives the event and extracts its payload 1. The consumer updates its local datasource with the payload data 1. If the consumer fails to process the update, it can persist the event to an error log 1. Each error in the log can be replayed 1. A command line interface replays an event via an admin API, which searches in the topic using this order id to replay the save operation This implementation causes a problem for the createOrder(order): string operation: The Order Service is supposed to return the new order complete with the order id that is a unique key, a key most likely created by the database. If updating the database fails, there is no new order yet and so no database key to use as the order ID. To avoid this problem, if the underlying technology supports assigning the new order's key, the service can generate the order ID and use that as the order's key in the database. It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note.","title":"The consistency challenge"},{"location":"design-patterns/cqrs/#cqrs-and-change-data-capture","text":"There are other ways to support this dual operations level: When using Kafka, Kafka Connect has the capability to subscribe to databases via JDBC, allowing to poll tables for updates and then produce events to Kafka. There is an open-source change data capture solution based on extracting change events from database transaction logs, Debezium that helps to respond to insert, update and delete operations on databases and generate events accordingly. It supports databases like MySQL, Postgres, MongoDB and others. Write the order to the database and in the same transaction write to an event table ( \"outbox pattern\" ). Then use a polling to get the events to send to Kafka from this event table and delete the row in the table once the event is sent. Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour . The CQRS implementation using CDC will look like in the following diagram: What is important to note is that the event needs to be flexible on the data payload. We are presenting a event model in the reference implementation. On the view side, updates to the view part need to be idempotent.","title":"CQRS and Change Data Capture"},{"location":"design-patterns/cqrs/#delay-in-the-view","text":"There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale. When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application like our kc- user interface , the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation: @POST public Response create ( OrderCreate dto ) { Order order = new Order ( UUID . randomUUID (). toString (), dto . getProductID (),...); // ... return Response . ok (). entity ( order ). build () }","title":"Delay in the view"},{"location":"design-patterns/cqrs/#schema-change","text":"What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures.","title":"Schema change"},{"location":"design-patterns/cqrs/#code-reference","text":"The following project includes two sub modules, each deployable as a microservice to illustrate the command and query part: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms","title":"Code reference"},{"location":"design-patterns/cqrs/#compendium","text":"https://www.codeproject.com/Articles/555855/Introduction-to-CQRS http://udidahan.com/2009/12/09/clarified-cqrs https://martinfowler.com/bliki/CQRS.html https://microservices.io/patterns/data/cqrs.html https://community.risingstack.com/when-to-use-cqrs https://dzone.com/articles/concepts-of-cqrs https://martinfowler.com/bliki/CommandQuerySeparation.html https://www.martinfowler.com/eaaCatalog/domainModel.html https://dddcommunity.org/learning-ddd/what_is_ddd/ https://martinfowler.com/bliki/EvansClassification.html https://martinfowler.com/bliki/DDD_Aggregate.html https://martinfowler.com/eaaCatalog/dataTransferObject.html https://en.wikipedia.org/wiki/Command_pattern https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusable-Object-Oriented-Software/PGM14333.html","title":"Compendium"},{"location":"design-patterns/data-replication/","text":"Data replication in context of EDA As an introduction to the scope of the data replication in the context of distributed system, we encourage to read our summary in this article . In this section we are going to address the data replication in the context of Kafka using IBM Geo Replication and Kafka Mirror Maker 2 and change data capture. Problem statement We suppose we need to replicate data in Kafka topics between different clusters running in different availability zones or data centers. There are multiple motivations for such replication. We can list at least the followings: Support disaster recovery, using different data centers to replicate microservice generated data in an active - passive mode. Move data closer to end user to improve performance and latency, this is more an active - active model, where the same microservices are deployed on the different data centers. The need to build on-premise data aggregation or data lake layer to perform batch analytics jobs from data gathered from different remote environments. Isolate secure data from on-premise cluster, with data encryption and data transformation to remove personal identifiable information, but still exchange such data between environments. We are proposing two environments: one running a local, on-premise cluster using Kafka 2.4 open source packaging, like Strimzi vanilla Kafka or Red Hat AMQ Streams, and IBM Event Streams on Cloud. one running Event streams on Openshift on-premise, and using Geo-replication to Event Streams on IBM cloud. Then we want to address two main scenarios: Active - passive , which addresses more a disaster recovery approach where consumers reconnect to a new cluster after the first one fails. Active - active deployments where participant clusters have producers and consumers and some topics are replicated so the messages are processed by different consumers But first we need to review the new replication capability introduced with Kafka 2.4: Mirror maker 2.0. Mirror Maker 2.0 Mirror maker 2.0 is the new feature as part of Kafka 2.4 to support data replication between clusters. It is based on Kafka Connect framework, and it supports data and metadata replication, like the topic configuration, the offset and the consumer checkpoints are synchronously replicated to the target cluster. Mirror maker uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. The specification is described in detail in this issue . To test the tool, we can use the Strimzi Kafka latest docker image deployed on Openshift cluster (We address Strimzi deployment in this note ). For mirror maker, the deployment descriptor are in the Strimzi project under the examples/kafka-mirror-maker-2 folder. To define the clusters and topic configuration we use a properties file. One simple example to replicate from IBM Cloud Event streams to Kafka on premise is in the folder Using the same kafka image we can start a mirror maker container with: docker run -ti bitnami/kafka:2 -v $( pwd ) /mirror-maker:/home --network docker_default bash Active - Passive Active - Active producers, deployed on IBM cloud within Openshift, send messages to Event streams on cloud, on the 'reeferTelemetries` topic. The explanation of the telemetry simulator producer deployment is done here.","title":"Data replication pattern"},{"location":"design-patterns/data-replication/#data-replication-in-context-of-eda","text":"As an introduction to the scope of the data replication in the context of distributed system, we encourage to read our summary in this article . In this section we are going to address the data replication in the context of Kafka using IBM Geo Replication and Kafka Mirror Maker 2 and change data capture.","title":"Data replication in context of EDA"},{"location":"design-patterns/data-replication/#problem-statement","text":"We suppose we need to replicate data in Kafka topics between different clusters running in different availability zones or data centers. There are multiple motivations for such replication. We can list at least the followings: Support disaster recovery, using different data centers to replicate microservice generated data in an active - passive mode. Move data closer to end user to improve performance and latency, this is more an active - active model, where the same microservices are deployed on the different data centers. The need to build on-premise data aggregation or data lake layer to perform batch analytics jobs from data gathered from different remote environments. Isolate secure data from on-premise cluster, with data encryption and data transformation to remove personal identifiable information, but still exchange such data between environments. We are proposing two environments: one running a local, on-premise cluster using Kafka 2.4 open source packaging, like Strimzi vanilla Kafka or Red Hat AMQ Streams, and IBM Event Streams on Cloud. one running Event streams on Openshift on-premise, and using Geo-replication to Event Streams on IBM cloud. Then we want to address two main scenarios: Active - passive , which addresses more a disaster recovery approach where consumers reconnect to a new cluster after the first one fails. Active - active deployments where participant clusters have producers and consumers and some topics are replicated so the messages are processed by different consumers But first we need to review the new replication capability introduced with Kafka 2.4: Mirror maker 2.0.","title":"Problem statement"},{"location":"design-patterns/data-replication/#mirror-maker-20","text":"Mirror maker 2.0 is the new feature as part of Kafka 2.4 to support data replication between clusters. It is based on Kafka Connect framework, and it supports data and metadata replication, like the topic configuration, the offset and the consumer checkpoints are synchronously replicated to the target cluster. Mirror maker uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. The specification is described in detail in this issue . To test the tool, we can use the Strimzi Kafka latest docker image deployed on Openshift cluster (We address Strimzi deployment in this note ). For mirror maker, the deployment descriptor are in the Strimzi project under the examples/kafka-mirror-maker-2 folder. To define the clusters and topic configuration we use a properties file. One simple example to replicate from IBM Cloud Event streams to Kafka on premise is in the folder Using the same kafka image we can start a mirror maker container with: docker run -ti bitnami/kafka:2 -v $( pwd ) /mirror-maker:/home --network docker_default bash","title":"Mirror Maker 2.0"},{"location":"design-patterns/data-replication/#active-passive","text":"","title":"Active - Passive"},{"location":"design-patterns/data-replication/#active-active","text":"producers, deployed on IBM cloud within Openshift, send messages to Event streams on cloud, on the 'reeferTelemetries` topic. The explanation of the telemetry simulator producer deployment is done here.","title":"Active - Active"},{"location":"design-patterns/event-sourcing/","text":"Event sourcing Problems and Constraints Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state . For that the application needs to keep history of business facts. Traditional domain oriented implementation builds a domain data model and map it to a RDBMS. As an example, in the simple Order model below, the database record will keep the last state of the order, the different addresses and the last ordered items in separate tables. If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table. Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data. For legal reason, most businesses do not remove data. As an example, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability. Solution and Pattern Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or \"facts\" ordered over time. When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions. The state-change event is immutable stored in an event log or event store in time order. The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business. You can see the \"removing an item\" in the order is a new event. With this capability, we can count how often a specific product is removed for the shopping cart. In some cases, the event sourcing pattern is implemented completely within the event backbone. Kafka topic and partitions are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. An event store needs to store only three pieces of information: The type of event or aggregate. The sequence number of the event. The data as a serialized entity. More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records. With a central event logs, producers append events to the log, and consumers read them from an offset (the last committed read). To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot or the origin of \"time\". Advantages The main goal is to be able to understand what happens to a business entity over time. But there are a set of interesting things that can be done: We can rebuild the data project within a microservice after it crashes, be reloading the event log. As events are ordered with time, we can apply complex event processing with temporal queries, time window operations, and looking at non-event. Be able to reverse the state and correct data with new events. Considerations When replaying the event, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. So the consumer of events need to be adapted to the query and business requirement. For example if the code needs to answer to the question: \"what happened to the order over time for order ID = 75?\" then there is no side effect, only a report can be created each time the consumer runs. Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots. Kafka is supporting the event sourcing pattern with the topic and partition . In our reference implementation we are validating event sourcing with Kafka in the Order microservices and specially this set of test cases. The event sourcing pattern is well described in this article on microservices.io . It is a very important pattern to support eventual data consistency between microservices and for data synchronization between system as the event store becomes the source of truth. See also this event sourcing article from Martin Fowler, where he is also using ship movement examples. Our implementation differs as we are using Kafka topic as event store and use different entities to support the container shipping process: the Orders, ShipLocations, Containers entities... Another common use case, where event sourcing helps, is when developers push a new code version that corrupts the data: being able to see what was done on the data, and being able to reload from a previous state helps fixing problems. Command sourcing Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, AddItem command is becoming AddItemValidated , then once persisted to a database it becomes an event as ItemAdded . So mixing command and event sourcing is a common practice. Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\". The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events. At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log. One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity. Code repository All the microservices implementing the Reefer management solution is using event sourcing, as we use kafka with long persistence. The order management service is using CQRS combined with event sourcing: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms and an integration test validate the pattern here . Compendium Martin Fowler - event sourcing pattern Microservice design pattern from Chris Richardson Greg Young video on event sourcing at the goto; conference","title":"Event Sourcing"},{"location":"design-patterns/event-sourcing/#event-sourcing","text":"","title":"Event sourcing"},{"location":"design-patterns/event-sourcing/#problems-and-constraints","text":"Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state . For that the application needs to keep history of business facts. Traditional domain oriented implementation builds a domain data model and map it to a RDBMS. As an example, in the simple Order model below, the database record will keep the last state of the order, the different addresses and the last ordered items in separate tables. If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table. Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data. For legal reason, most businesses do not remove data. As an example, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability.","title":"Problems and Constraints"},{"location":"design-patterns/event-sourcing/#solution-and-pattern","text":"Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or \"facts\" ordered over time. When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions. The state-change event is immutable stored in an event log or event store in time order. The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business. You can see the \"removing an item\" in the order is a new event. With this capability, we can count how often a specific product is removed for the shopping cart. In some cases, the event sourcing pattern is implemented completely within the event backbone. Kafka topic and partitions are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. An event store needs to store only three pieces of information: The type of event or aggregate. The sequence number of the event. The data as a serialized entity. More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records. With a central event logs, producers append events to the log, and consumers read them from an offset (the last committed read). To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot or the origin of \"time\".","title":"Solution and Pattern"},{"location":"design-patterns/event-sourcing/#advantages","text":"The main goal is to be able to understand what happens to a business entity over time. But there are a set of interesting things that can be done: We can rebuild the data project within a microservice after it crashes, be reloading the event log. As events are ordered with time, we can apply complex event processing with temporal queries, time window operations, and looking at non-event. Be able to reverse the state and correct data with new events.","title":"Advantages"},{"location":"design-patterns/event-sourcing/#considerations","text":"When replaying the event, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. So the consumer of events need to be adapted to the query and business requirement. For example if the code needs to answer to the question: \"what happened to the order over time for order ID = 75?\" then there is no side effect, only a report can be created each time the consumer runs. Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots. Kafka is supporting the event sourcing pattern with the topic and partition . In our reference implementation we are validating event sourcing with Kafka in the Order microservices and specially this set of test cases. The event sourcing pattern is well described in this article on microservices.io . It is a very important pattern to support eventual data consistency between microservices and for data synchronization between system as the event store becomes the source of truth. See also this event sourcing article from Martin Fowler, where he is also using ship movement examples. Our implementation differs as we are using Kafka topic as event store and use different entities to support the container shipping process: the Orders, ShipLocations, Containers entities... Another common use case, where event sourcing helps, is when developers push a new code version that corrupts the data: being able to see what was done on the data, and being able to reload from a previous state helps fixing problems.","title":"Considerations"},{"location":"design-patterns/event-sourcing/#command-sourcing","text":"Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, AddItem command is becoming AddItemValidated , then once persisted to a database it becomes an event as ItemAdded . So mixing command and event sourcing is a common practice. Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\". The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events. At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log. One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity.","title":"Command sourcing"},{"location":"design-patterns/event-sourcing/#code-repository","text":"All the microservices implementing the Reefer management solution is using event sourcing, as we use kafka with long persistence. The order management service is using CQRS combined with event sourcing: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms and an integration test validate the pattern here .","title":"Code repository"},{"location":"design-patterns/event-sourcing/#compendium","text":"Martin Fowler - event sourcing pattern Microservice design pattern from Chris Richardson Greg Young video on event sourcing at the goto; conference","title":"Compendium"},{"location":"design-patterns/saga/","text":"Saga pattern Problems and Constraints With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option. Solution and Pattern Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern help to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation , illustrates those concepts for an order transaction: When the order is created, the business process says, we need to allocate a \"voyage\", assign containers and update the list of containers to load on the ship. Those actions / commands are chained. The final state (in this schema, not in the reality, as the process has more steps) is the Order assigned state in the order microservice. With a unique application implementation, the integrity between order, voyage and container tables will be done via transactions. With distributed system we could not apply two phase commit transaction so the Saga pattern will help. SAGA pattern supports two types of implementation: Choreography and Orchestration. Services choreography With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not. The first service executes a transaction to its own data store and then publishes an event ( OrderCreated event (1)) as fact about its business entity update. It maintains the business entity status, (order.status) to the Pending state until the saga is completed. This event is listened by one or more services which execute local transactions and publish new events (VoyageAllocated (3), ReeferAssigned (4), PaymentProcessed (5)). The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not polled by any of the saga\u2019s participants. For example, the Order microservice gets all the events from the other service and changed the Order state to be Accepted . In case of failure, the source microservice is keeping state and timer to monitor for the expected completion events. When a message from any service is missing, the source service, needs to trigger a compensation process: Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. This will be a new event sent by the service responsible of the transaction integrity. In the order example, in the rare case where one of the service is not able to provide a positive response, no voyage found, or no Reefer container found, then the order needs to change to 'Uncompleted' status, and an event to the orders topic will claim the orderID is now uncompleted (OrderUncompleted event Step 1 above) . Any service that has something allocated for this orderId will 'unroll' their changes in their own data source (Steps 2,3,4 below). Also it is important to note, that if one of the service is taking time to answer this may not be a problem as the order is in pending state. If the business requirement stipulates to address an order within a small time period then the compensation process may start. Uncompleted orders can be reviewed by a business user for manual handling. Email can be automatically sent to the customer about issue related to his order. There are a lot of different ways to handle order issue at the business level. Services orchestration With orchestration, one service is responsible to drive each participant on what to do and when. It uses the different topics to control the saga by issuing event commands to the different service. It uses the event backbone as a queue processing to support the asynchronous invocations. In this case the event should be exactly once delivered and idempotent. Each participant produces response in their context and to the order topic. The orchestration layer needs to keep a state machine and acts once all the expected responses are received. If anything fails, the orchestrator is also responsible for coordinating the compensation process by sending rollback events with orderID and their respective impacted entity key (voyageID, reeferID, transactionID). Each participant will undo its previous operations. Orchestrator is a State Machine where each transformation corresponds to a command or message. See also this article from Chris Richardson on the Saga pattern. We have implemented the choreography saga pattern in the order management, voyage and refeer management microservices within the EDA reference implementation solution with a detailed explanation of the integration tests to validate the happy path and the exception path with compensation.","title":"Saga"},{"location":"design-patterns/saga/#saga-pattern","text":"","title":"Saga pattern"},{"location":"design-patterns/saga/#problems-and-constraints","text":"With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option.","title":"Problems and Constraints"},{"location":"design-patterns/saga/#solution-and-pattern","text":"Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern help to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation , illustrates those concepts for an order transaction: When the order is created, the business process says, we need to allocate a \"voyage\", assign containers and update the list of containers to load on the ship. Those actions / commands are chained. The final state (in this schema, not in the reality, as the process has more steps) is the Order assigned state in the order microservice. With a unique application implementation, the integrity between order, voyage and container tables will be done via transactions. With distributed system we could not apply two phase commit transaction so the Saga pattern will help. SAGA pattern supports two types of implementation: Choreography and Orchestration.","title":"Solution and Pattern"},{"location":"design-patterns/saga/#services-choreography","text":"With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not. The first service executes a transaction to its own data store and then publishes an event ( OrderCreated event (1)) as fact about its business entity update. It maintains the business entity status, (order.status) to the Pending state until the saga is completed. This event is listened by one or more services which execute local transactions and publish new events (VoyageAllocated (3), ReeferAssigned (4), PaymentProcessed (5)). The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not polled by any of the saga\u2019s participants. For example, the Order microservice gets all the events from the other service and changed the Order state to be Accepted . In case of failure, the source microservice is keeping state and timer to monitor for the expected completion events. When a message from any service is missing, the source service, needs to trigger a compensation process: Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. This will be a new event sent by the service responsible of the transaction integrity. In the order example, in the rare case where one of the service is not able to provide a positive response, no voyage found, or no Reefer container found, then the order needs to change to 'Uncompleted' status, and an event to the orders topic will claim the orderID is now uncompleted (OrderUncompleted event Step 1 above) . Any service that has something allocated for this orderId will 'unroll' their changes in their own data source (Steps 2,3,4 below). Also it is important to note, that if one of the service is taking time to answer this may not be a problem as the order is in pending state. If the business requirement stipulates to address an order within a small time period then the compensation process may start. Uncompleted orders can be reviewed by a business user for manual handling. Email can be automatically sent to the customer about issue related to his order. There are a lot of different ways to handle order issue at the business level.","title":"Services choreography"},{"location":"design-patterns/saga/#services-orchestration","text":"With orchestration, one service is responsible to drive each participant on what to do and when. It uses the different topics to control the saga by issuing event commands to the different service. It uses the event backbone as a queue processing to support the asynchronous invocations. In this case the event should be exactly once delivered and idempotent. Each participant produces response in their context and to the order topic. The orchestration layer needs to keep a state machine and acts once all the expected responses are received. If anything fails, the orchestrator is also responsible for coordinating the compensation process by sending rollback events with orderID and their respective impacted entity key (voyageID, reeferID, transactionID). Each participant will undo its previous operations. Orchestrator is a State Machine where each transformation corresponds to a command or message. See also this article from Chris Richardson on the Saga pattern. We have implemented the choreography saga pattern in the order management, voyage and refeer management microservices within the EDA reference implementation solution with a detailed explanation of the integration tests to validate the happy path and the exception path with compensation.","title":"Services orchestration"},{"location":"evt-action/","text":"Taking An Action with Cloud Functions IBM Cloud Functions is a \"Serverless\" compute offering. While one of the appeals of serverless computing is the provision of cost-effective compute time, it also provides a simplified event-driven programming model which is very valuable for event-driven solutions. valuable for event-driven solutions. With Cloud Functions, the process is as follows: Developers write functional logic called actions . Actions can be written in many supported languages including Java, Python, Node, Swift, Go, or other languages. Actions are triggered from events being published to Kafka topics (the event backbone). Cloud Functions brings up the required compute to run the action. Cloud Functions shuts down the server when the action is complete. Cloud Functions automatically scales for event volume and velocity. For event-driven systems, this simple event driven programming model is powerful. It abstracts the complications of event handling and load balancing to ensure that you have enough subscribing consumers ready to handle the velocity of events published through the system. Developers write the code which executes the required business logic. Supporting products IBM Cloud Functions is a commercial service offering version of the Apache Openwhisk project IBM Cloud Functions product offering https://www.ibm.com/cloud/functions Suggested reading Using Cloud functions with event trigger in Kafka https://github.com/IBM/ibm-cloud-functions-message-hub-trigger","title":"Event actions"},{"location":"evt-action/#taking-an-action-with-cloud-functions","text":"IBM Cloud Functions is a \"Serverless\" compute offering. While one of the appeals of serverless computing is the provision of cost-effective compute time, it also provides a simplified event-driven programming model which is very valuable for event-driven solutions. valuable for event-driven solutions. With Cloud Functions, the process is as follows: Developers write functional logic called actions . Actions can be written in many supported languages including Java, Python, Node, Swift, Go, or other languages. Actions are triggered from events being published to Kafka topics (the event backbone). Cloud Functions brings up the required compute to run the action. Cloud Functions shuts down the server when the action is complete. Cloud Functions automatically scales for event volume and velocity. For event-driven systems, this simple event driven programming model is powerful. It abstracts the complications of event handling and load balancing to ensure that you have enough subscribing consumers ready to handle the velocity of events published through the system. Developers write the code which executes the required business logic.","title":"Taking An Action with Cloud Functions"},{"location":"evt-action/#supporting-products","text":"IBM Cloud Functions is a commercial service offering version of the Apache Openwhisk project IBM Cloud Functions product offering https://www.ibm.com/cloud/functions","title":"Supporting products"},{"location":"evt-action/#suggested-reading","text":"Using Cloud functions with event trigger in Kafka https://github.com/IBM/ibm-cloud-functions-message-hub-trigger","title":"Suggested reading"},{"location":"evt-backbone/","text":"Event Backbone The event backbone is the communication layer in the event driven architecture. It provides the connection between event driven capabilities and in the Cloud Native , it becomes the Pub/Sub communication layer for event driven microservices. At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs . Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions: Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth). Enabling direct replay of events. Enabling Event Sourcing as a way of recording state changes in distributed systems. Enabling programmatic access to the continuous event stream . When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components. Defining the Event Backbone for the event driven reference architecture For the event driven architecture we defined the following characteristics to be essential for the event backbone Publish-subscribe event communication between event producers and consumers Facilitate many consumers with shared central \u201csource of truth\u201d. Capability to store events for a given period of time (event log). This is the shared source of the truth for events. Ability for consumers to subscribe to events. Provide replay of events from history for evolving application instances. Provide programmatic access to continuous stream of events, with minimum time lag. Must be highly scalable and resilient to cloud deployment levels. Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies lead us to selecting Kafka as the Open Source technology base for the event backbone. You can read more about Apache Kafka project here https://kafka.apache.org Event backbone considerations While choosing an event backbone for your event-driven app development, you need to consider below points, Persistence When source systems generate events, the consumers of those are interested in those events may not be online or available at the same time. So you need a way to store these messages for a configurable period of time until they are consumed and acted upon. Event backbone should be able to provide such event persistence. Observability At times, you need an overall view of how events are ingested by source systems and getting processed by consumers. It could be a management console where events can be observed. Event backbone should provide such observability. Fault tolerance Event backbone could be made of several components. If one of them becomes unavailable, there should not be any impact on the event processors dependent on the backbone. Event backbone needs to provide this resiliency. High availability Event backbone provides persistence of messages/events. If one of the components of the backbone becomes unavailable, there should not be any impact on the availability of these messages/events. Event backbone should be highly available. Performance Event backbone should provide means of accelerating the event processing operations (e.g. parallelising event processing) thereby providing enhanced performance. Delivery guarantees Event backbone should support guaranteed delivery both for producer and consumer. It should support below delivery guarantees: at least once at most once exactly once Security The data residing in the event backbone should be secured, at rest as well as in transit. Only authenticated and authorized users should be able to publish and consume messages from the backbone. Topic specific authorizations will also help blocking access by unauthorized consumers. Event backbone should provide these security measures. Stateful operations for events streams Sometimes, source systems generate a continuous flow of 'inter-related' events (e.g. IoT sensors sending data every second). In order to process such messages correctly, the event backbone needs to support for stateful operations like windowing, joins, aggregations. and any type of real time analytics. Ease of development Developing a consumer or a stream application should be straight-forward with the programmatic features that the event backbone provides. Ease of deployment The installation of event backbone should be an easy to follow process. Event routing options In EDA, event consumers may not be online at all times. So, it should be easier for consumers to subscribe to a topic when it comes online. On-failure hooks Event backbone can support pre-configured actions/behaviors for certain messages. E.g. if a consumer fails to process a message more than a certain number of times, that message can be sent to another topic for re-trying the processing action. Predetermined for unprocessed events (retries, dead-letter queues etc) Management plane Event backbone can provide a management plane from which infrastrucutre level configurations can be managed. Supporting products The IBM Event Streams offering provides a Kafka service for the Event Backbone. The service is available as a fully managed service within Public cloud and as a supported build for IBM Cloud Private. IBM Event Streams Public Cloud IBM Event Streams Private Cloud See also our own Kafka study article on how to support high availability and how to deploy to your local environment or to a kubernetes cluster like IBM Cloud Private. Active MQ Artemis and our study notes Messaging versus event streaming We recommend reading this article and this one , to get insight between messaging (focusing on operations / actions to be performed by a system or service) versus event (focusing on the state / facts of a system with no knowledge of the downstream processing. To summarize messaging (like MQ) are to support: Transient Data \u2013 data is only stored until a consumer has processed the message, or it expires Request / reply most of the time Targeted reliable delivery: targeted to the entity that will process the request or receive the response. Reliable with transaction support. For events: Stream history: consumers are interested by history and not just the most recent event Scalable Consumption: A single event is consumed by many consumers with limited impact as the number of consumers grow. Immutable Data. Decoupling of Producers and consumers Deployments In term of event backbone deployment environment we propose different approaches: IBM Cloud with the Event Streams service . Deployment discussions for the KC solution are in this note IBM Cloud Private Event Streams deployment . Zookeeper deployment and Kafka deployment for ICP. Running locally with docker compose. See this note for details.","title":"Event backbone"},{"location":"evt-backbone/#event-backbone","text":"The event backbone is the communication layer in the event driven architecture. It provides the connection between event driven capabilities and in the Cloud Native , it becomes the Pub/Sub communication layer for event driven microservices. At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs . Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions: Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth). Enabling direct replay of events. Enabling Event Sourcing as a way of recording state changes in distributed systems. Enabling programmatic access to the continuous event stream . When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components.","title":"Event Backbone"},{"location":"evt-backbone/#defining-the-event-backbone-for-the-event-driven-reference-architecture","text":"For the event driven architecture we defined the following characteristics to be essential for the event backbone Publish-subscribe event communication between event producers and consumers Facilitate many consumers with shared central \u201csource of truth\u201d. Capability to store events for a given period of time (event log). This is the shared source of the truth for events. Ability for consumers to subscribe to events. Provide replay of events from history for evolving application instances. Provide programmatic access to continuous stream of events, with minimum time lag. Must be highly scalable and resilient to cloud deployment levels. Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies lead us to selecting Kafka as the Open Source technology base for the event backbone. You can read more about Apache Kafka project here https://kafka.apache.org","title":"Defining the Event Backbone for the event driven reference architecture"},{"location":"evt-backbone/#event-backbone-considerations","text":"While choosing an event backbone for your event-driven app development, you need to consider below points,","title":"Event backbone considerations"},{"location":"evt-backbone/#persistence","text":"When source systems generate events, the consumers of those are interested in those events may not be online or available at the same time. So you need a way to store these messages for a configurable period of time until they are consumed and acted upon. Event backbone should be able to provide such event persistence.","title":"Persistence"},{"location":"evt-backbone/#observability","text":"At times, you need an overall view of how events are ingested by source systems and getting processed by consumers. It could be a management console where events can be observed. Event backbone should provide such observability.","title":"Observability"},{"location":"evt-backbone/#fault-tolerance","text":"Event backbone could be made of several components. If one of them becomes unavailable, there should not be any impact on the event processors dependent on the backbone. Event backbone needs to provide this resiliency.","title":"Fault tolerance"},{"location":"evt-backbone/#high-availability","text":"Event backbone provides persistence of messages/events. If one of the components of the backbone becomes unavailable, there should not be any impact on the availability of these messages/events. Event backbone should be highly available.","title":"High availability"},{"location":"evt-backbone/#performance","text":"Event backbone should provide means of accelerating the event processing operations (e.g. parallelising event processing) thereby providing enhanced performance.","title":"Performance"},{"location":"evt-backbone/#delivery-guarantees","text":"Event backbone should support guaranteed delivery both for producer and consumer. It should support below delivery guarantees: at least once at most once exactly once","title":"Delivery guarantees"},{"location":"evt-backbone/#security","text":"The data residing in the event backbone should be secured, at rest as well as in transit. Only authenticated and authorized users should be able to publish and consume messages from the backbone. Topic specific authorizations will also help blocking access by unauthorized consumers. Event backbone should provide these security measures.","title":"Security"},{"location":"evt-backbone/#stateful-operations-for-events-streams","text":"Sometimes, source systems generate a continuous flow of 'inter-related' events (e.g. IoT sensors sending data every second). In order to process such messages correctly, the event backbone needs to support for stateful operations like windowing, joins, aggregations. and any type of real time analytics.","title":"Stateful operations for events streams"},{"location":"evt-backbone/#ease-of-development","text":"Developing a consumer or a stream application should be straight-forward with the programmatic features that the event backbone provides.","title":"Ease of development"},{"location":"evt-backbone/#ease-of-deployment","text":"The installation of event backbone should be an easy to follow process.","title":"Ease of deployment"},{"location":"evt-backbone/#event-routing-options","text":"In EDA, event consumers may not be online at all times. So, it should be easier for consumers to subscribe to a topic when it comes online.","title":"Event routing options"},{"location":"evt-backbone/#on-failure-hooks","text":"Event backbone can support pre-configured actions/behaviors for certain messages. E.g. if a consumer fails to process a message more than a certain number of times, that message can be sent to another topic for re-trying the processing action. Predetermined for unprocessed events (retries, dead-letter queues etc)","title":"On-failure hooks"},{"location":"evt-backbone/#management-plane","text":"Event backbone can provide a management plane from which infrastrucutre level configurations can be managed.","title":"Management plane"},{"location":"evt-backbone/#supporting-products","text":"The IBM Event Streams offering provides a Kafka service for the Event Backbone. The service is available as a fully managed service within Public cloud and as a supported build for IBM Cloud Private. IBM Event Streams Public Cloud IBM Event Streams Private Cloud See also our own Kafka study article on how to support high availability and how to deploy to your local environment or to a kubernetes cluster like IBM Cloud Private. Active MQ Artemis and our study notes","title":"Supporting products"},{"location":"evt-backbone/#messaging-versus-event-streaming","text":"We recommend reading this article and this one , to get insight between messaging (focusing on operations / actions to be performed by a system or service) versus event (focusing on the state / facts of a system with no knowledge of the downstream processing. To summarize messaging (like MQ) are to support: Transient Data \u2013 data is only stored until a consumer has processed the message, or it expires Request / reply most of the time Targeted reliable delivery: targeted to the entity that will process the request or receive the response. Reliable with transaction support. For events: Stream history: consumers are interested by history and not just the most recent event Scalable Consumption: A single event is consumed by many consumers with limited impact as the number of consumers grow. Immutable Data. Decoupling of Producers and consumers","title":"Messaging versus event streaming"},{"location":"evt-backbone/#deployments","text":"In term of event backbone deployment environment we propose different approaches: IBM Cloud with the Event Streams service . Deployment discussions for the KC solution are in this note IBM Cloud Private Event Streams deployment . Zookeeper deployment and Kafka deployment for ICP. Running locally with docker compose. See this note for details.","title":"Deployments"},{"location":"evt-microservices/","text":"Event-driven cloud native apps On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling. Event backbone - Pub/Sub communication and data sharing for microservices With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ... Read more on patterns Supporting products and suggested reading Event backbone IBM Cloud Functions/Openwhisk programming model Using Cloud functions with event trigger in Kafka IBM Cloud Functions product offering Getting Started with Cloud Functions Event driven apps with containers While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack. Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture. As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics. The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event. Supporting products and suggested reading IBM Cloud Private - Kubernetes base container platform IBM Cloud Kubernetes Service Deploy a microservices application on Kubernetes IBM Cloud Kubernetes Service: Manage apps in containers and clusters on cloud","title":"Microservices"},{"location":"evt-microservices/#event-driven-cloud-native-apps","text":"On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling.","title":"Event-driven cloud native apps"},{"location":"evt-microservices/#event-backbone-pubsub-communication-and-data-sharing-for-microservices","text":"With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ... Read more on patterns","title":"Event backbone - Pub/Sub communication and data sharing for microservices"},{"location":"evt-microservices/#supporting-products-and-suggested-reading","text":"Event backbone IBM Cloud Functions/Openwhisk programming model Using Cloud functions with event trigger in Kafka IBM Cloud Functions product offering Getting Started with Cloud Functions","title":"Supporting products and suggested reading"},{"location":"evt-microservices/#event-driven-apps-with-containers","text":"While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack. Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture. As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics. The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event.","title":"Event driven apps with containers"},{"location":"evt-microservices/#supporting-products-and-suggested-reading_1","text":"IBM Cloud Private - Kubernetes base container platform IBM Cloud Kubernetes Service Deploy a microservices application on Kubernetes IBM Cloud Kubernetes Service: Manage apps in containers and clusters on cloud","title":"Supporting products and suggested reading"},{"location":"evt-src/","text":"Event Sources When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing. These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business. Here is a list of common event sources: IOT devices or sensors showing device status changes Click Stream data from web or mobile applications Mobile applications (HTTP to Back-end for Front-end service and then to topic) Geospacial data Weather alerts Social media feeds Real-time voice feeds Other messaging backbone Data change event streams from databases (change data capture) IOT devices and sensors With IOT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IOT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level. One example of this is to relate a device number or identifier in the event to something that the business recognizes. Clickstream data Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures. Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience. Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done. There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript. Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics. Microservices as event producers and consumers The event-driven reference architecture provides support for event-driven microservices, this is microservices are connected and communicate via the pub/sub communication protocol within the Event Backbone. With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events. Event standards and schemas Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard. Our recommendation is to follow CloudEvents where we have the ability to define the event structure and so pass \"CloudEvents\" through the event backbone. Supporting products IBM Mobile IBM MQ IBM Internet of Things platform IBM Streaming Analytics Kafka Producer API for Java Weather Company Data Voice Agent with Watson Code references The following code repositories can be used for event sourcing inspiration: ship movements/ container metrics event producer as a microservice Container stream analytics Pump Simulator to send New Pump/ Asset event or Metric events to emulate intelligent IoT Electrical Pump. Simple text message producer As well as the starting application generated from IBM Event Streams. See such app in the folder gettingStarted and explanation in the starter App","title":"Event sources"},{"location":"evt-src/#event-sources","text":"When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing. These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business. Here is a list of common event sources: IOT devices or sensors showing device status changes Click Stream data from web or mobile applications Mobile applications (HTTP to Back-end for Front-end service and then to topic) Geospacial data Weather alerts Social media feeds Real-time voice feeds Other messaging backbone Data change event streams from databases (change data capture)","title":"Event Sources"},{"location":"evt-src/#iot-devices-and-sensors","text":"With IOT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IOT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level. One example of this is to relate a device number or identifier in the event to something that the business recognizes.","title":"IOT devices and sensors"},{"location":"evt-src/#clickstream-data","text":"Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures. Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience. Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done. There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript. Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics.","title":"Clickstream data"},{"location":"evt-src/#microservices-as-event-producers-and-consumers","text":"The event-driven reference architecture provides support for event-driven microservices, this is microservices are connected and communicate via the pub/sub communication protocol within the Event Backbone. With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events.","title":"Microservices as event producers and consumers"},{"location":"evt-src/#event-standards-and-schemas","text":"Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard. Our recommendation is to follow CloudEvents where we have the ability to define the event structure and so pass \"CloudEvents\" through the event backbone.","title":"Event standards and schemas"},{"location":"evt-src/#supporting-products","text":"IBM Mobile IBM MQ IBM Internet of Things platform IBM Streaming Analytics Kafka Producer API for Java Weather Company Data Voice Agent with Watson","title":"Supporting products"},{"location":"evt-src/#code-references","text":"The following code repositories can be used for event sourcing inspiration: ship movements/ container metrics event producer as a microservice Container stream analytics Pump Simulator to send New Pump/ Asset event or Metric events to emulate intelligent IoT Electrical Pump. Simple text message producer As well as the starting application generated from IBM Event Streams. See such app in the folder gettingStarted and explanation in the starter App","title":"Code references"},{"location":"evt-state/","text":"Event managed state While the prime focus for an event-driven architecture is processing events, in certain cases you need to persist events for post processing and queries by other applications. The event backbone has a built-in event log that can be used to store and reply to events that are published to the backbone. However, considering the full scope of event-driven solutions, other use cases and types of store can be supported: Event stores optimized for analytics. Event sourcing as a pattern for recording state changes and updates across distributed systems. Command Query Response Separation (CQRS) as an optimization that separates updates and reads across different stores. Event sourcing When the state of a system changes, an application issues a notification event of the state change. We are detailing this pattern here >> Command Query Responsibility Segregation (CQRS) The event log leads to more work to support business query as it requires converting the events into the application state suitable to the query. We are detailing this pattern here >> See the following order management project for a detail explanation and implementation of the CQRS and event sourcing patterns. Event sourcing, CQRS and microservices With the adoption of microservices you have explicitly separated state, so that a microservice is bounded with its own state. Further, with the use of event sourcing, you create a history log that is not easy to query. The challenge now comes when you need to implement a query that requires a joining of data from multiple services. There are multiple choices to address service orchestration: API composition or the CQRS pattern. For API composition the query is supported by an operation which integrate with all other microservices and may do some data transformation to combine the results. With this pattern you need to assess for aggregation requirements as they may dramatically impact performance. You may need to assess where to put this API composition component. It can be an API gateway or part of a BFF or even its own microservices. The other answer is to implement a CQRS pattern where state changes are published as events by multiple related business objects. Each change is persisted in the event log or event store, and a higher level operation subscribes to each event and persists the data in a queryable data store. Fearther readings Read more on this pattern at https://microservices.io/patterns/data/cqrs.html and our reference implementation Supporting Products IBM Event Streams Public Cloud IBM Event Streams Private Cloud IBM Db2 Event store","title":"Event managed states"},{"location":"evt-state/#event-managed-state","text":"While the prime focus for an event-driven architecture is processing events, in certain cases you need to persist events for post processing and queries by other applications. The event backbone has a built-in event log that can be used to store and reply to events that are published to the backbone. However, considering the full scope of event-driven solutions, other use cases and types of store can be supported: Event stores optimized for analytics. Event sourcing as a pattern for recording state changes and updates across distributed systems. Command Query Response Separation (CQRS) as an optimization that separates updates and reads across different stores.","title":"Event managed state"},{"location":"evt-state/#event-sourcing","text":"When the state of a system changes, an application issues a notification event of the state change. We are detailing this pattern here >>","title":"Event sourcing"},{"location":"evt-state/#command-query-responsibility-segregation-cqrs","text":"The event log leads to more work to support business query as it requires converting the events into the application state suitable to the query. We are detailing this pattern here >> See the following order management project for a detail explanation and implementation of the CQRS and event sourcing patterns.","title":"Command Query Responsibility Segregation (CQRS)"},{"location":"evt-state/#event-sourcing-cqrs-and-microservices","text":"With the adoption of microservices you have explicitly separated state, so that a microservice is bounded with its own state. Further, with the use of event sourcing, you create a history log that is not easy to query. The challenge now comes when you need to implement a query that requires a joining of data from multiple services. There are multiple choices to address service orchestration: API composition or the CQRS pattern. For API composition the query is supported by an operation which integrate with all other microservices and may do some data transformation to combine the results. With this pattern you need to assess for aggregation requirements as they may dramatically impact performance. You may need to assess where to put this API composition component. It can be an API gateway or part of a BFF or even its own microservices. The other answer is to implement a CQRS pattern where state changes are published as events by multiple related business objects. Each change is persisted in the event log or event store, and a higher level operation subscribes to each event and persists the data in a queryable data store.","title":"Event sourcing, CQRS and microservices"},{"location":"evt-state/#fearther-readings","text":"Read more on this pattern at https://microservices.io/patterns/data/cqrs.html and our reference implementation","title":"Fearther readings"},{"location":"evt-state/#supporting-products","text":"IBM Event Streams Public Cloud IBM Event Streams Private Cloud IBM Db2 Event store","title":"Supporting Products"},{"location":"kafka/FAQ/","text":"Frequently asked questions Kafka concepts? See this introduction How to support exactly once delivery? See the section in the producer implementation considerations note . Also it is important to note that the Kafka Stream API supports exactly once semantics with the config: processing.guarantee=exactly_once . Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once. Why does kafka use zookeeper? Kafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registrering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains kafka cluster integrity, select broker leader... Retention time for topic what does it mean? The message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: retention.ms and retention.bytes . Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to cleanup.policy parameter. See the kafka documentation on topic configuration parameters . Here is a command to create a topic with specific retention properties: bin / kafka - configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config retention.ms=55000 --add-config retention.byte=100000 But there is also the offsets.retention.minutes property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers run not continuously. So consumers need to commit their offset. If the consumer settings define: auto.offset.reset=earliest , the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to latest ). When using latest , if the consumers are offline for more than the offsets retention time window, they will lose events. What are the topic characteristics I need to define during requirements? This is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy. Number of brokers in the cluster fire or forget or persist data for which amount of time Need for HA, set replicas to number of broker or at least the value of 3 Type of data to transport Schema management to control change to the payload definition volume per day Accept snapshot Need to do ge replication to other kafka cluster Network filesystem used on the target kubernetes cluster and current storage class Differences between Akka and Kafka? Akka is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM. Kafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture. vert.x is another open source implementation of such internal messaging mechanism but supporting more language: Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin. Event streams resource requirements See the detailed tables in the product documentation. Other FAQs IBM Event streams on Cloud FAQ FAQ from Confluent","title":"Kafka FAQ"},{"location":"kafka/FAQ/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"kafka/FAQ/#kafka-concepts","text":"See this introduction","title":"Kafka concepts?"},{"location":"kafka/FAQ/#how-to-support-exactly-once-delivery","text":"See the section in the producer implementation considerations note . Also it is important to note that the Kafka Stream API supports exactly once semantics with the config: processing.guarantee=exactly_once . Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once.","title":"How to support exactly once delivery?"},{"location":"kafka/FAQ/#why-does-kafka-use-zookeeper","text":"Kafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registrering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains kafka cluster integrity, select broker leader...","title":"Why does kafka use zookeeper?"},{"location":"kafka/FAQ/#retention-time-for-topic-what-does-it-mean","text":"The message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: retention.ms and retention.bytes . Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to cleanup.policy parameter. See the kafka documentation on topic configuration parameters . Here is a command to create a topic with specific retention properties: bin / kafka - configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config retention.ms=55000 --add-config retention.byte=100000 But there is also the offsets.retention.minutes property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers run not continuously. So consumers need to commit their offset. If the consumer settings define: auto.offset.reset=earliest , the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to latest ). When using latest , if the consumers are offline for more than the offsets retention time window, they will lose events.","title":"Retention time for topic what does it mean?"},{"location":"kafka/FAQ/#what-are-the-topic-characteristics-i-need-to-define-during-requirements","text":"This is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy. Number of brokers in the cluster fire or forget or persist data for which amount of time Need for HA, set replicas to number of broker or at least the value of 3 Type of data to transport Schema management to control change to the payload definition volume per day Accept snapshot Need to do ge replication to other kafka cluster Network filesystem used on the target kubernetes cluster and current storage class","title":"What are the topic characteristics I need to define during requirements?"},{"location":"kafka/FAQ/#differences-between-akka-and-kafka","text":"Akka is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM. Kafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture. vert.x is another open source implementation of such internal messaging mechanism but supporting more language: Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin.","title":"Differences between Akka and Kafka?"},{"location":"kafka/FAQ/#event-streams-resource-requirements","text":"See the detailed tables in the product documentation.","title":"Event streams resource requirements"},{"location":"kafka/FAQ/#other-faqs","text":"IBM Event streams on Cloud FAQ FAQ from Confluent","title":"Other FAQs"},{"location":"kafka/arch/","text":"IBM Event Streams / Kafka Architecture Considerations If you need to understand the Kafka key concepts review this summary article . In this section we are covering some architecture concerns like high availability. High Availability As a distributed cluster, kafka brokers ensure high availability to process new events. Topic has replication factor to support not loosing data in case of broker failure. You need at least 3 brokers to ensure availability and a replication factor set to 3 for each topic, so no data should be lost. In production it is recommended to use 5 brokers cluster to ensure the quorum is always set. The brokers need to run on separate physical machines. Usually replicas is done in-sync, and the configuration settings specify the number of replicas in-sync needed: for example, a replicas 3 can have a minimum in-sync of 2, to tolerate 1 out of sync replica (1 broker outage). Partition enables data locality, elasticity, scalability, high performance, parallelism, and fault tolerance. Each partitition is replicated at least 3 times and allocated in different brokers. One replicas is the leader . In the case of broker failure (broker 1 in figure below), one of the existing partition in the remaining running brokers will take the leader role (e.g. red partition in broker 3): Replication and partition leadership The keys in the data record determine the partitioning of data in Kafka . The records with the same key will be in the same partition. As kafka is keeping its cluster states in Apache Zookeeper , you also need to have at least a three node cluster for zookeeper. Writes to Zookeeper are only be performed on changes to the membership of consumer groups or on changes to the Kafka cluster itself. Assuming you are using the most recent kafka version (after 0.9), it is possible to have a unique zookeeper cluster for multiple kafka clusters. But the latency between Kafka and zookeeper needs to be under few milliseconds (< 15ms) anyway. Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated on different racks and blades. Consumers and producers are using a list of bootstrap server names (also named advertiser.listeners) to contact the cluster. The list is used for cluster discovery, it does not need to keep the full set of server names or ip addresses. A Kafka cluster has exactly one broker that acts as the controller. Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low latency dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property to assign kafka broker using rack awareness. (See this configuration from the product documentation). Always assess the latency requirements and consumers needs. Throughtput is linked to the number of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes. Running in parallel, also means the order of event arrivals will be lost. Most of the time, consumers are processing events from a unique partition and Kafka record to partition assignment will guarantee that records with the same key hashcode will be in the same partition. So orders are preserved within a partition. But if consumer needs to read from multiple paritions then if ordered records is needed, the consumer needs to rebuild the order with some complex logic. For high availability assess any potential single point of failure, such as server, rack, network, power supply... The figure below illustrates a kubernetes deployment, where zookeeper and kafka brokers are allocated to 3 worker nodes, with some event driven microservices deployed in separate worker nodes. Those microservices are consumers and producers of events from one to many topics. We recommend reading this event stream article for planning your kafka on kubernetes installation. The advantages of deploying Kafka on kubernetes cluster is to facilitate the management of stateful sets, by scheduling both the persistence volume and broker pods in a clean rolling rehydration. Services add a logical name to access brokers for any deployed workload within the cluster. The virtual network also enables transparent TLS communication between components. For the consumers code update, the recreation of the consumer instance within the consumer group will trigger the partition rebalancing. This includes all the state of the aggregated data calculations that were persisted on disk. Until this process is finished real-time events are not processed. It is possible to limit this impact by setting the group.initial.rebalance.delay.ms to delay the rebalancing process one one instance of the consumer dies. Nevertheless the rebalancing will still occur when the updated consumer will rejoin the consumer group. When consumers are stream processing using Kafka streams, it is important to note that during the rollover the downstream processing will see a lag in event arrival: the time for the consumer to reread from the last committed offset. So if end to end timing is becoming important, we need to setup a standby consumer cluster (cluster B). This consumer group has different name, but does the same processing logic, and is consuming the same events from the same topic as the active consumer group cluster (cluster A). The difference is that they do not send events to the downstream topic until they are set up active. So to process the release cluster B is set active while cluster A is set inactive. The downstream will not be that much impacted. Finally to be exhaustif, the control of the segment size for the change log topic, may be considered to avoid having the stream processing doing a lot of computation to reload its state when it restarts. To add new broker, you can deploy the runtime to a new server / rack / blade, and give it a unique ID. Broker will process new topic, but it is possible to use tool to migrate some existing topic/ partitions to the new server. The tool is used to reassign partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers. High Availability in the context of Kubernetes deployment The combination of kafka with kubernetes seems to be a sound approach, but it is not that easy to achieve. Kubernetes workloads prefer to be stateless, Kafka is a stateful platform and manages its own brokers, and replications across known servers. It knows the underlying infrastructure. In kubernetes, nodes and pods may change dynamically. For any Kubernetes deployment real high availability is constrained by the application / workload deployed on it. The Kubernetes platform supports high availability by having at least the following configuration: At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm. Three proxy nodes. At least three worker nodes, but with zookeeper and Kafka clusters, we may need to define six nodes as we do not want to have zookeeper nodes with Kafka cluster broker on the same host. Externalize the management stack to three manager nodes Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence. Use etcd cluster: See recommendations from this article . The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster. It leverages etcd for storing information, so it is important that etcd is high available too and connected to low latency network below 10ms. Traditionally disaster recovery and high availability were always consider separated subjects. Now active/active deployment where workloads are deployed in different data centers, is becoming a common request. For Kafka context, the Confluent website presents an interesting article for Kafka production deployment . One of their recommendation is to avoid cluster that spans multiple data centers and specially long distance ones, because of the latency, and the chatty interface between zookeeper and kafka brokers. But the semantic of the event processing may authorize some adaptations. For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper ensemble running at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by. The diagram above illustrates a simple deployment where zookeeper servers and kakfka brokers are running in pods, in different worker nodes. It is a viable solution to start deploying solution on top of kafka. When you have bigger cluster, it may be interesting to separate Zookeeper from Kafka nodes to limit the risk of failover, as zookeeper keeps state of the Kafka cluster. You will limit to have both the zookeeper leader and one kafka broker dying at the same time. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by zookeeper. It uses the labels on pods with a rule like: Kafka pod should not run on same node as zookeeper pods. Here is an example of such spec: apiVersion : v1 kind : Pod metadata : name : with-pod-affinity spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : labelSelector : matchExpressions : - key : name operator : In values : - gc-zookeeper topologyKey : kubernetes.io/hostname We recommend reading the \"running zookeeper in k8s tutorial\" for understanding such configuration. Provision a fast storage class for persistence volume. Kafka uses the log.dirs property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support log.dirs. Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only. In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster. We are addressing offset management in one of our consumer projects here . For configuring ICP for HA on VmWare read this note . For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means: producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers. communications between zookeepers and cluster nodes are redundant and safe for data losses consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer. Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation. Kafka configuration is an art and you need to tune the parameters by use case: Partition replication for at least 3 replicas. Recall that in case of node failure, coordination of partition re-assignments is provided with ZooKeeper. End to end latency needs to be measured from producer (when a message is sent) to consumer when it is read. A consumer is able to get a message when the broker finishes replicating to all in-synch replicas. Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold. Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. Assess the retention hours to control when old messages in topic can be deleted Control the maximum message size the server can receive. Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two cpu per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot. Kubernetes Operator It is important to note that the deployment and management of stateful application in Kubernetes should, now, use the proposed Operator Framework introduced by Red Hat and Google. One important contribution is the Strimzi Kafka operator that simplifies the deployment of Kafka within k8s by adding a set of operators to deploy and manage Kafka clusters, topics, users and more. Performance considerations Performance will vary depending of the current kafka broker nodes load: in kubernetes deployment, with small production topology, nodes may shared with other pods. It is recommended to control the environment with dedicated nodes for Kafka to achieve higher throughput. Performance will always depend on numerous factors including message throughput, message size, hardware, configuration settings, ... Performance may be linked to different focuses: Resilience: ensuring replication and not loosing data Throughput: ensuring message processing performance Payload size: support larger message Resilience When defining a topic, we need to specify the replicas factor to match the be at least 3 and then set the minimum number of in-sync replicas that specifies how may replicas must acknowledge a write to satisfy a producer that requests acknowledgments from all replicas. ( min.insync.replicas ). The replication of message data between brokers can consume a lot of network bandwidth so isolating replication traffic from application traffic can benefit performance. To achieve this, all replication traffic is configured to flow on a dedicated internal network. Throughput To achieve higher throughput the messages are not replicated across brokers and the acknowledgement can be set to only one broker. Expose resilienc to failures. The number of producers and consumers are aligned, and the number of partition match the number of consumers. All consumers are in the same consumer group. Measurement has to be done from the producer code. With 12 producers on a 3 brokers cluster and small payload (128 bytes), with 24 consumers the measured throughput is around 2.3 M messages / second. Payload size From measurement tests done using Kafka producer performance tool, there is a 1/log(s) curve, where below 10k bytes the performances are correct and then slowly degrade from 3000 msg /s (10k bytes msg) to 65 msg/s (515kb msg). To do performance test the event-streams-sample-producer github provides producer tool in Java, using a group of threads to run in multi cores machine. This project can be dockerized, and deployed in k8s. It uses the kafka tool named: ProducerPerformance.java in the jar: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-tools </artifactId> </dependency> Parameter considerations There are a lot of factors and parameters that needs to be tuned to improve performance at the brokers threading level ( num.replica.fetchers, num.io.threads, num.network.threads, log.cleaner.threads ) and the pod resources constraints. See configuration documentation . Multi regions for disaster recovery With the current implementation it is recommended to have one cluster per data center / availability zone. Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the data replicated in both data center, you need to assess what kind of data can be aggregated, and if Kafka mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved. The above diagram is using Kafka MirrorMaker with a master to slave deployment. Within the data center 2, the brokers are here to manage the topics and events. When there is no consumer running, nothing happen. Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact, we could have consumers within the DC2 processing topics to manage a readonly model, keeping in memory their projection view, as presented in the CQRS pattern . The second solution is to use one mirror maker in each site, for each topic. This is an active - actice topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic. When you want to deploy solution that spreads over multiple regions to support global streaming, you need to address the following challenges: How do you make data available to applications across multiple data centers? How to serve data closer to the geography? How to be compliant on regulations, like GDPR? How to address no duplication of records? Kafka 2.4 introduces the capability for a consumer to read messages from the closest replica using some rack-id and specific algorithm. This capability will help to extend the cluster to multiple data center and avoid having consumers going over WAN communication. MQ integration IBM has created a pair of connectors, available as source code or as part of IBM Event Streams product. The Source Connector responsible to support the integration from MQ queue to Kafka topic is available in the github repository named ibm-messaging/kafka-connect-mq-source while the sink connector, from Kafka topic to MQ queue is at ibm-messaging/kafka-connect-mq-sink The following figure illustrates the high level components. It is important to note that the Kafka connectors is a cluster deployment for local high availability and scalability. We are proposing an MQ to Kafka implementation sample in the container inventory repository where we mockup the integration of a legacy DB managing shipment container inventory, it runs as a java appm jms producer and consumer on MQ queues. This solution is integrated in the global EDA reference solution implementation and specially the Reefer container management microservice.","title":"Architecture considerations"},{"location":"kafka/arch/#ibm-event-streams-kafka-architecture-considerations","text":"If you need to understand the Kafka key concepts review this summary article . In this section we are covering some architecture concerns like high availability.","title":"IBM Event Streams / Kafka Architecture Considerations"},{"location":"kafka/arch/#high-availability","text":"As a distributed cluster, kafka brokers ensure high availability to process new events. Topic has replication factor to support not loosing data in case of broker failure. You need at least 3 brokers to ensure availability and a replication factor set to 3 for each topic, so no data should be lost. In production it is recommended to use 5 brokers cluster to ensure the quorum is always set. The brokers need to run on separate physical machines. Usually replicas is done in-sync, and the configuration settings specify the number of replicas in-sync needed: for example, a replicas 3 can have a minimum in-sync of 2, to tolerate 1 out of sync replica (1 broker outage). Partition enables data locality, elasticity, scalability, high performance, parallelism, and fault tolerance. Each partitition is replicated at least 3 times and allocated in different brokers. One replicas is the leader . In the case of broker failure (broker 1 in figure below), one of the existing partition in the remaining running brokers will take the leader role (e.g. red partition in broker 3): Replication and partition leadership The keys in the data record determine the partitioning of data in Kafka . The records with the same key will be in the same partition. As kafka is keeping its cluster states in Apache Zookeeper , you also need to have at least a three node cluster for zookeeper. Writes to Zookeeper are only be performed on changes to the membership of consumer groups or on changes to the Kafka cluster itself. Assuming you are using the most recent kafka version (after 0.9), it is possible to have a unique zookeeper cluster for multiple kafka clusters. But the latency between Kafka and zookeeper needs to be under few milliseconds (< 15ms) anyway. Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated on different racks and blades. Consumers and producers are using a list of bootstrap server names (also named advertiser.listeners) to contact the cluster. The list is used for cluster discovery, it does not need to keep the full set of server names or ip addresses. A Kafka cluster has exactly one broker that acts as the controller. Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low latency dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property to assign kafka broker using rack awareness. (See this configuration from the product documentation). Always assess the latency requirements and consumers needs. Throughtput is linked to the number of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes. Running in parallel, also means the order of event arrivals will be lost. Most of the time, consumers are processing events from a unique partition and Kafka record to partition assignment will guarantee that records with the same key hashcode will be in the same partition. So orders are preserved within a partition. But if consumer needs to read from multiple paritions then if ordered records is needed, the consumer needs to rebuild the order with some complex logic. For high availability assess any potential single point of failure, such as server, rack, network, power supply... The figure below illustrates a kubernetes deployment, where zookeeper and kafka brokers are allocated to 3 worker nodes, with some event driven microservices deployed in separate worker nodes. Those microservices are consumers and producers of events from one to many topics. We recommend reading this event stream article for planning your kafka on kubernetes installation. The advantages of deploying Kafka on kubernetes cluster is to facilitate the management of stateful sets, by scheduling both the persistence volume and broker pods in a clean rolling rehydration. Services add a logical name to access brokers for any deployed workload within the cluster. The virtual network also enables transparent TLS communication between components. For the consumers code update, the recreation of the consumer instance within the consumer group will trigger the partition rebalancing. This includes all the state of the aggregated data calculations that were persisted on disk. Until this process is finished real-time events are not processed. It is possible to limit this impact by setting the group.initial.rebalance.delay.ms to delay the rebalancing process one one instance of the consumer dies. Nevertheless the rebalancing will still occur when the updated consumer will rejoin the consumer group. When consumers are stream processing using Kafka streams, it is important to note that during the rollover the downstream processing will see a lag in event arrival: the time for the consumer to reread from the last committed offset. So if end to end timing is becoming important, we need to setup a standby consumer cluster (cluster B). This consumer group has different name, but does the same processing logic, and is consuming the same events from the same topic as the active consumer group cluster (cluster A). The difference is that they do not send events to the downstream topic until they are set up active. So to process the release cluster B is set active while cluster A is set inactive. The downstream will not be that much impacted. Finally to be exhaustif, the control of the segment size for the change log topic, may be considered to avoid having the stream processing doing a lot of computation to reload its state when it restarts. To add new broker, you can deploy the runtime to a new server / rack / blade, and give it a unique ID. Broker will process new topic, but it is possible to use tool to migrate some existing topic/ partitions to the new server. The tool is used to reassign partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers.","title":"High Availability"},{"location":"kafka/arch/#high-availability-in-the-context-of-kubernetes-deployment","text":"The combination of kafka with kubernetes seems to be a sound approach, but it is not that easy to achieve. Kubernetes workloads prefer to be stateless, Kafka is a stateful platform and manages its own brokers, and replications across known servers. It knows the underlying infrastructure. In kubernetes, nodes and pods may change dynamically. For any Kubernetes deployment real high availability is constrained by the application / workload deployed on it. The Kubernetes platform supports high availability by having at least the following configuration: At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm. Three proxy nodes. At least three worker nodes, but with zookeeper and Kafka clusters, we may need to define six nodes as we do not want to have zookeeper nodes with Kafka cluster broker on the same host. Externalize the management stack to three manager nodes Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence. Use etcd cluster: See recommendations from this article . The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster. It leverages etcd for storing information, so it is important that etcd is high available too and connected to low latency network below 10ms. Traditionally disaster recovery and high availability were always consider separated subjects. Now active/active deployment where workloads are deployed in different data centers, is becoming a common request. For Kafka context, the Confluent website presents an interesting article for Kafka production deployment . One of their recommendation is to avoid cluster that spans multiple data centers and specially long distance ones, because of the latency, and the chatty interface between zookeeper and kafka brokers. But the semantic of the event processing may authorize some adaptations. For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper ensemble running at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by. The diagram above illustrates a simple deployment where zookeeper servers and kakfka brokers are running in pods, in different worker nodes. It is a viable solution to start deploying solution on top of kafka. When you have bigger cluster, it may be interesting to separate Zookeeper from Kafka nodes to limit the risk of failover, as zookeeper keeps state of the Kafka cluster. You will limit to have both the zookeeper leader and one kafka broker dying at the same time. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by zookeeper. It uses the labels on pods with a rule like: Kafka pod should not run on same node as zookeeper pods. Here is an example of such spec: apiVersion : v1 kind : Pod metadata : name : with-pod-affinity spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : labelSelector : matchExpressions : - key : name operator : In values : - gc-zookeeper topologyKey : kubernetes.io/hostname We recommend reading the \"running zookeeper in k8s tutorial\" for understanding such configuration. Provision a fast storage class for persistence volume. Kafka uses the log.dirs property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support log.dirs. Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only. In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster. We are addressing offset management in one of our consumer projects here . For configuring ICP for HA on VmWare read this note . For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means: producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers. communications between zookeepers and cluster nodes are redundant and safe for data losses consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer. Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation. Kafka configuration is an art and you need to tune the parameters by use case: Partition replication for at least 3 replicas. Recall that in case of node failure, coordination of partition re-assignments is provided with ZooKeeper. End to end latency needs to be measured from producer (when a message is sent) to consumer when it is read. A consumer is able to get a message when the broker finishes replicating to all in-synch replicas. Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold. Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. Assess the retention hours to control when old messages in topic can be deleted Control the maximum message size the server can receive. Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two cpu per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot.","title":"High Availability in the context of Kubernetes deployment"},{"location":"kafka/arch/#kubernetes-operator","text":"It is important to note that the deployment and management of stateful application in Kubernetes should, now, use the proposed Operator Framework introduced by Red Hat and Google. One important contribution is the Strimzi Kafka operator that simplifies the deployment of Kafka within k8s by adding a set of operators to deploy and manage Kafka clusters, topics, users and more.","title":"Kubernetes Operator"},{"location":"kafka/arch/#performance-considerations","text":"Performance will vary depending of the current kafka broker nodes load: in kubernetes deployment, with small production topology, nodes may shared with other pods. It is recommended to control the environment with dedicated nodes for Kafka to achieve higher throughput. Performance will always depend on numerous factors including message throughput, message size, hardware, configuration settings, ... Performance may be linked to different focuses: Resilience: ensuring replication and not loosing data Throughput: ensuring message processing performance Payload size: support larger message","title":"Performance considerations"},{"location":"kafka/arch/#resilience","text":"When defining a topic, we need to specify the replicas factor to match the be at least 3 and then set the minimum number of in-sync replicas that specifies how may replicas must acknowledge a write to satisfy a producer that requests acknowledgments from all replicas. ( min.insync.replicas ). The replication of message data between brokers can consume a lot of network bandwidth so isolating replication traffic from application traffic can benefit performance. To achieve this, all replication traffic is configured to flow on a dedicated internal network.","title":"Resilience"},{"location":"kafka/arch/#throughput","text":"To achieve higher throughput the messages are not replicated across brokers and the acknowledgement can be set to only one broker. Expose resilienc to failures. The number of producers and consumers are aligned, and the number of partition match the number of consumers. All consumers are in the same consumer group. Measurement has to be done from the producer code. With 12 producers on a 3 brokers cluster and small payload (128 bytes), with 24 consumers the measured throughput is around 2.3 M messages / second.","title":"Throughput"},{"location":"kafka/arch/#payload-size","text":"From measurement tests done using Kafka producer performance tool, there is a 1/log(s) curve, where below 10k bytes the performances are correct and then slowly degrade from 3000 msg /s (10k bytes msg) to 65 msg/s (515kb msg). To do performance test the event-streams-sample-producer github provides producer tool in Java, using a group of threads to run in multi cores machine. This project can be dockerized, and deployed in k8s. It uses the kafka tool named: ProducerPerformance.java in the jar: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-tools </artifactId> </dependency>","title":"Payload size"},{"location":"kafka/arch/#parameter-considerations","text":"There are a lot of factors and parameters that needs to be tuned to improve performance at the brokers threading level ( num.replica.fetchers, num.io.threads, num.network.threads, log.cleaner.threads ) and the pod resources constraints. See configuration documentation .","title":"Parameter considerations"},{"location":"kafka/arch/#multi-regions-for-disaster-recovery","text":"With the current implementation it is recommended to have one cluster per data center / availability zone. Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the data replicated in both data center, you need to assess what kind of data can be aggregated, and if Kafka mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved. The above diagram is using Kafka MirrorMaker with a master to slave deployment. Within the data center 2, the brokers are here to manage the topics and events. When there is no consumer running, nothing happen. Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact, we could have consumers within the DC2 processing topics to manage a readonly model, keeping in memory their projection view, as presented in the CQRS pattern . The second solution is to use one mirror maker in each site, for each topic. This is an active - actice topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic. When you want to deploy solution that spreads over multiple regions to support global streaming, you need to address the following challenges: How do you make data available to applications across multiple data centers? How to serve data closer to the geography? How to be compliant on regulations, like GDPR? How to address no duplication of records? Kafka 2.4 introduces the capability for a consumer to read messages from the closest replica using some rack-id and specific algorithm. This capability will help to extend the cluster to multiple data center and avoid having consumers going over WAN communication.","title":"Multi regions for disaster recovery"},{"location":"kafka/arch/#mq-integration","text":"IBM has created a pair of connectors, available as source code or as part of IBM Event Streams product. The Source Connector responsible to support the integration from MQ queue to Kafka topic is available in the github repository named ibm-messaging/kafka-connect-mq-source while the sink connector, from Kafka topic to MQ queue is at ibm-messaging/kafka-connect-mq-sink The following figure illustrates the high level components. It is important to note that the Kafka connectors is a cluster deployment for local high availability and scalability. We are proposing an MQ to Kafka implementation sample in the container inventory repository where we mockup the integration of a legacy DB managing shipment container inventory, it runs as a java appm jms producer and consumer on MQ queues. This solution is integrated in the global EDA reference solution implementation and specially the Reefer container management microservice.","title":"MQ integration"},{"location":"kafka/connect/","text":"Kafka Connect Kafka connect is an open source component for easily integrate external systems with Kafka. It works with IBM Event Streams and Red Hat AMQ Streams. It uses the concepts of source and sink connectors to ingest or deliver data to / from Kafka topics. The generate concepts are detailed in this note or in the IBM Event streams product documentation . Here is a quick summary: connector represents a logical job to move data from / to kafka to / from external systems. A lot of existing connectors can be reused, or you can implement your owns. workers are JVM running the connector. For production deployment workers run in cluster or \"distributed mode\". tasks : each worker coordinates a set of tasks to copy data. Task states are saved in kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline. When a connector is submitted to the cluster, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work. Characteristics Copy vast quantities of data from source to kafka: work at the database level. Support streaming and batch. Scale at the organization level, even if it can support a standalone, mono connector approach to start small, it is possible to run in parallel on distributed cluster. Copy data, externalizing transformation in other framework. Kafka Connect defines three models: data model, worker model and connector model. Provide a REST interface to manage connectors and monitor jobs. Installation The Kafka connect framework fits well into a kubernetes deployment. We have different options for that deployment. We recommend reading the IBM event streams documentation for installing Kafka connect with IBM Event Streams or you can also leverage the Strimzi Kafka connect operator . With IBM Event Streams on premise deployment, the connectors setup is part of the user admin console toolbox: Deploying connectors against an IBM Event Streams cluster, you need to have API key with permissions to produce and consume messages for all topics. As an extendable framework, kafka connect, can have new connector plugins. To deploy new connector, the kafka docker image defining the connector needs to be updated with the connector jar and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL. Here is the list of supported connectors for IBM Event Streams. From the downloaded dockerfile we can build a new kafka connect environment image like: docker build -t ibmcase/kafkaconnect:0.0.1 . We will use this image to run the kafka connect in standalone mode or in the distributed mode section . Getting started with kafka connect standalone mode For development and test purposes, we can use Kafka connect in standalone mode, but still connected to IBM Event Streams running on-premise. Start a container with kafka connector, to run a standalone connector: you need to use a worker configuration and a connector properties files under the connectors folder. Those files will be mounted under the /opt/kafka/config folder. Also, as we want to test sending the content of a file, we mount to the /home/data the folder with input file: # in the refarch-kc/docker/kafka-connect folder docker run -ti --rm -v $( pwd ) /config:/opt/kafka/config -v $( pwd ) /data:/home/data --entrypoint bash -p 8083 :8083 ibmcase/kafkaconnect:0.0.1 Note You need to modify those property files to set the API key for your own event streams cluster, and set any other properties. Inside the container starts the standalone connector: cd /opt/kafka ./bin/connect-standalone.sh config/worker-standalone.properties config/file-source.properties config/file-sink.properties The file-source.properties configures a file reader to source the data/access_log.txt file to the clickstream topic: name = local-file-source connector.class = FileStreamSource tasks.max = 1 file = /home/kafka-connect/access_log.txt topic = clickstream While the config/file-sink.properties defines a file sink stream to create a json file by getting messages from the clickstream topic. The file sink connector can read from multiple topics to aggregate the content in the same file. The standalone connector worker configuration specifies where to connect, and what converters to use: bootstrap.servers = .... key.converter = org.apache.kafka.connect.json.JsonConverter value.converter = org.apache.kafka.connect.json.JsonConverter # Local storage file for offset data offset.storage.file.filename = /tmp/connect.offsets The execution trace shows the producer id, and the consumer id, and the task for each connector: INFO Creating task local - file - source - 0 ( org . apache . kafka . connect . runtime . Worker : 414 ) ... INFO TaskConfig values : task . class = class org . apache . kafka . connect . file . FileStreamSourceTask ... INFO Creating connector local - file - sink of type FileStreamSink ( org . apache . kafka . connect . runtime . Worker : 246 ) INFO Creating task local - file - sink - 0 ( org . apache . kafka . connect . runtime . Worker : 414 ) To validate the data are well published see the generated file under the data folder. As the Json converter was used, the message was wrapped into a json document with schema and payload. { \"schema\" :{ \"type\" : \"string\" , \"optional\" : false }, \"payload\" : \"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \\\"POST /xmlrpc.php HTTP/1.0\\\" 200 370 \\\"-\\\" \\\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\\\"\" } Connecting to IBM Cloud Event Streams remote cluster To connect to Event Streams on IBM Cloud the properties needs to define the broker adviser URLs and the API key that you get from the service crendentials. This API key must provide permission to produce and consume messages for all topics, and also to create topics. With Event streams on Cloud the following document explains what properties to add to the worker and connectors configuration. bootstrap.servers = broker-3-qnsdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprt... security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"98....\"; Using the same file source stream connector to send records and a simple consumer console to trace the output like: docker run -ti -v $( pwd ) /config:/opt/kafka/config --entrypoint bash ibmcase/kafkaconnect:0.0.1 esuser@3245874dcdd3: cd /opt/kafka/bin/ esuser@3245874dcdd3: ./kafka-console-consumer.sh --bootstrap-server eventstream140-ibm-es-proxy-route-broker-0-eventstreams.apps.green.ocp.csplab.local:443 --consumer.config /opt/kafka/config/console-consumer.properties --topic clickstream --from-beginning \" The console-consumer.properties specifies the SASL properties to connect to the remote broker using API key. Distributed mode When running in distributed mode, the connectors need three topics as presented in the create topics table here . connect-configs : This topic will store the connector and task configurations. connect-offsets : This topic is used to store offsets for Kafka Connect. connect-status : This topic will store status updates of connectors and tasks. Using IBM Event Streams CLI, the topics are created via the commands like: # log to the kubernetes cluster: cloudctl login -a https://icp-console.apps.green.ocp.csplab.local # initialize the event streams CLI plugin cloudctl es init # Create the Kafka topic cloudctl es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy = compact cloudctl es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy = compact cloudctl es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy = compact cloudctl es topics Then the connector configuration needs to specify some other properties (See kafka documentation ): group.id to specify the connect cluster name. key and value converters. replication factors and topic name for the three needed topics, if Kafka connect is able to create topic on the cluster. When using Event Streams as kafka cluster, add the sasl properties as described in the product documentation . With Event Streams as part of the Cloud Pak for integration, the administration console explains the steps to setup connectors, get distributed configuration and how to add connectors. See this properties file as an example. To start locally a Kafka connect in distributed mode, connected to Event Streams deployed on-premise use the following command (the entry point in the dockerfile use the connect-distributed mode script): docker run -v $( pwd ) /config:/opt/kafka/config -p 8083 :8083 ibmcase/kafkaconnect:0.0.1 To illustrate the Kakfa Connect distributed mode, we will add a source connector from a Mongo DB data source using this connector . When using as a source, the connector publishes data changes from MongoDB into Kafka topics for streaming to consuming apps. Data is captured via Change Streams within the MongoDB cluster and published into Kafka topics. The installation of a connector is done by adding the jars from the connector into the plugin path ( /opt/connectors ) as defined in the connector properties. In the case of mongodb kafka connector the manual installation instructions are in this github . The download page includes an uber jar. As we run the kakfa connect as docker container, the approach is to build a new docker image based one of the Kafka image publicly available. To define and start a connector, you do a POST to the REST API. Verifying the connectors via the REST api The documentation about the REST APIs for the distributed connector is in this site . For example the http://localhost:8083/connectors is the base URL when running locally. Deploy the Kafka connect as a service within Openshift cluster When you use IBM Event Streams on Openshift, you can deploy the IBM kafka connector environment as Docker containers, and define the needed connect-* topics as explained in previous section. The product documentation describes how to do that. Another approach is to use Strimzi operator. To Be done! Running with local kafka cluster We are using a local kafka cluster started with docker-compose as defined in the compose file here . The docker network should be kafkanet , if not already created do the following docker network create kafkanet Start the kafka broker (bitnami distribution) and zookeeper node using the command below under the refarch-kc/docker folder: docker-compose -f backbone-compose.yml up -d Start a container with kafka code, to run a standalone connector: you need to use a worker configuration and a connector properties files. Those files will be mounted under the /home folder: docker run -ti -rm --name kconnect -v $( pwd ) :/home --network kafkanet -p 8083 :8083 bitnami/kafka:2 bash Need to map the port 8083, to access the REST APIs. Inside the container starts the standalone connector: cd /opt/bitnami/kafka ./bin/connect-standalone.sh /home/kafka-connect/worker-standalone.properties /home/kafka-connect/file-source.properties The above file configures a file reader to source the access_log.txt file to the clickstream topic: name = local-file-source connector.class = FileStreamSource tasks.max = 1 file = /home/kafka-connect/access_log.txt topic = clickstream The standalone connector worker configuration specifies where to connect, and what converters to use: bootstrap.servers = kafka1:9092 key.converter = org.apache.kafka.connect.json.JsonConverter value.converter = org.apache.kafka.connect.json.JsonConverter # Local storage file for offset data offset.storage.file.filename = /tmp/connect.offsets The execution trace shows the producer id INFO [ Producer clientId = connector - producer - local - file - source - 0 ] Cluster ID : tj8y0hiZSYWHB9vLHGP1Ew ( org . apache . kafka . clients . Metadata : 261 ) To validate the data are well published run another container with the consumer console tool: docker run -ti --name sinktrace --rm --network kafkanet bitnami/kafka:2 bash -c \" /opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic clickstream --from-beginning\" As the Json converter was used the trace show the message was wrapped into a json document with schema and payload. { \"schema\" :{ \"type\" : \"string\" , \"optional\" : false }, \"payload\" : \"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \\\"POST /xmlrpc.php HTTP/1.0\\\" 200 370 \\\"-\\\" \\\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\\\"\" } Further Readings Apache Kafka connect documentation Confluent Connector Documentation IBM Event Streams Connectors or the list of supported connectors MongoDB Connector for Apache Kafka","title":"Kafka Connect"},{"location":"kafka/connect/#kafka-connect","text":"Kafka connect is an open source component for easily integrate external systems with Kafka. It works with IBM Event Streams and Red Hat AMQ Streams. It uses the concepts of source and sink connectors to ingest or deliver data to / from Kafka topics. The generate concepts are detailed in this note or in the IBM Event streams product documentation . Here is a quick summary: connector represents a logical job to move data from / to kafka to / from external systems. A lot of existing connectors can be reused, or you can implement your owns. workers are JVM running the connector. For production deployment workers run in cluster or \"distributed mode\". tasks : each worker coordinates a set of tasks to copy data. Task states are saved in kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline. When a connector is submitted to the cluster, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work.","title":"Kafka Connect"},{"location":"kafka/connect/#characteristics","text":"Copy vast quantities of data from source to kafka: work at the database level. Support streaming and batch. Scale at the organization level, even if it can support a standalone, mono connector approach to start small, it is possible to run in parallel on distributed cluster. Copy data, externalizing transformation in other framework. Kafka Connect defines three models: data model, worker model and connector model. Provide a REST interface to manage connectors and monitor jobs.","title":"Characteristics"},{"location":"kafka/connect/#installation","text":"The Kafka connect framework fits well into a kubernetes deployment. We have different options for that deployment. We recommend reading the IBM event streams documentation for installing Kafka connect with IBM Event Streams or you can also leverage the Strimzi Kafka connect operator . With IBM Event Streams on premise deployment, the connectors setup is part of the user admin console toolbox: Deploying connectors against an IBM Event Streams cluster, you need to have API key with permissions to produce and consume messages for all topics. As an extendable framework, kafka connect, can have new connector plugins. To deploy new connector, the kafka docker image defining the connector needs to be updated with the connector jar and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL. Here is the list of supported connectors for IBM Event Streams. From the downloaded dockerfile we can build a new kafka connect environment image like: docker build -t ibmcase/kafkaconnect:0.0.1 . We will use this image to run the kafka connect in standalone mode or in the distributed mode section .","title":"Installation"},{"location":"kafka/connect/#getting-started-with-kafka-connect-standalone-mode","text":"For development and test purposes, we can use Kafka connect in standalone mode, but still connected to IBM Event Streams running on-premise. Start a container with kafka connector, to run a standalone connector: you need to use a worker configuration and a connector properties files under the connectors folder. Those files will be mounted under the /opt/kafka/config folder. Also, as we want to test sending the content of a file, we mount to the /home/data the folder with input file: # in the refarch-kc/docker/kafka-connect folder docker run -ti --rm -v $( pwd ) /config:/opt/kafka/config -v $( pwd ) /data:/home/data --entrypoint bash -p 8083 :8083 ibmcase/kafkaconnect:0.0.1 Note You need to modify those property files to set the API key for your own event streams cluster, and set any other properties. Inside the container starts the standalone connector: cd /opt/kafka ./bin/connect-standalone.sh config/worker-standalone.properties config/file-source.properties config/file-sink.properties The file-source.properties configures a file reader to source the data/access_log.txt file to the clickstream topic: name = local-file-source connector.class = FileStreamSource tasks.max = 1 file = /home/kafka-connect/access_log.txt topic = clickstream While the config/file-sink.properties defines a file sink stream to create a json file by getting messages from the clickstream topic. The file sink connector can read from multiple topics to aggregate the content in the same file. The standalone connector worker configuration specifies where to connect, and what converters to use: bootstrap.servers = .... key.converter = org.apache.kafka.connect.json.JsonConverter value.converter = org.apache.kafka.connect.json.JsonConverter # Local storage file for offset data offset.storage.file.filename = /tmp/connect.offsets The execution trace shows the producer id, and the consumer id, and the task for each connector: INFO Creating task local - file - source - 0 ( org . apache . kafka . connect . runtime . Worker : 414 ) ... INFO TaskConfig values : task . class = class org . apache . kafka . connect . file . FileStreamSourceTask ... INFO Creating connector local - file - sink of type FileStreamSink ( org . apache . kafka . connect . runtime . Worker : 246 ) INFO Creating task local - file - sink - 0 ( org . apache . kafka . connect . runtime . Worker : 414 ) To validate the data are well published see the generated file under the data folder. As the Json converter was used, the message was wrapped into a json document with schema and payload. { \"schema\" :{ \"type\" : \"string\" , \"optional\" : false }, \"payload\" : \"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \\\"POST /xmlrpc.php HTTP/1.0\\\" 200 370 \\\"-\\\" \\\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\\\"\" }","title":"Getting started with kafka connect standalone mode"},{"location":"kafka/connect/#connecting-to-ibm-cloud-event-streams-remote-cluster","text":"To connect to Event Streams on IBM Cloud the properties needs to define the broker adviser URLs and the API key that you get from the service crendentials. This API key must provide permission to produce and consume messages for all topics, and also to create topics. With Event streams on Cloud the following document explains what properties to add to the worker and connectors configuration. bootstrap.servers = broker-3-qnsdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprt... security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"98....\"; Using the same file source stream connector to send records and a simple consumer console to trace the output like: docker run -ti -v $( pwd ) /config:/opt/kafka/config --entrypoint bash ibmcase/kafkaconnect:0.0.1 esuser@3245874dcdd3: cd /opt/kafka/bin/ esuser@3245874dcdd3: ./kafka-console-consumer.sh --bootstrap-server eventstream140-ibm-es-proxy-route-broker-0-eventstreams.apps.green.ocp.csplab.local:443 --consumer.config /opt/kafka/config/console-consumer.properties --topic clickstream --from-beginning \" The console-consumer.properties specifies the SASL properties to connect to the remote broker using API key.","title":"Connecting to IBM Cloud Event Streams remote cluster"},{"location":"kafka/connect/#distributed-mode","text":"When running in distributed mode, the connectors need three topics as presented in the create topics table here . connect-configs : This topic will store the connector and task configurations. connect-offsets : This topic is used to store offsets for Kafka Connect. connect-status : This topic will store status updates of connectors and tasks. Using IBM Event Streams CLI, the topics are created via the commands like: # log to the kubernetes cluster: cloudctl login -a https://icp-console.apps.green.ocp.csplab.local # initialize the event streams CLI plugin cloudctl es init # Create the Kafka topic cloudctl es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy = compact cloudctl es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy = compact cloudctl es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy = compact cloudctl es topics Then the connector configuration needs to specify some other properties (See kafka documentation ): group.id to specify the connect cluster name. key and value converters. replication factors and topic name for the three needed topics, if Kafka connect is able to create topic on the cluster. When using Event Streams as kafka cluster, add the sasl properties as described in the product documentation . With Event Streams as part of the Cloud Pak for integration, the administration console explains the steps to setup connectors, get distributed configuration and how to add connectors. See this properties file as an example. To start locally a Kafka connect in distributed mode, connected to Event Streams deployed on-premise use the following command (the entry point in the dockerfile use the connect-distributed mode script): docker run -v $( pwd ) /config:/opt/kafka/config -p 8083 :8083 ibmcase/kafkaconnect:0.0.1 To illustrate the Kakfa Connect distributed mode, we will add a source connector from a Mongo DB data source using this connector . When using as a source, the connector publishes data changes from MongoDB into Kafka topics for streaming to consuming apps. Data is captured via Change Streams within the MongoDB cluster and published into Kafka topics. The installation of a connector is done by adding the jars from the connector into the plugin path ( /opt/connectors ) as defined in the connector properties. In the case of mongodb kafka connector the manual installation instructions are in this github . The download page includes an uber jar. As we run the kakfa connect as docker container, the approach is to build a new docker image based one of the Kafka image publicly available. To define and start a connector, you do a POST to the REST API.","title":"Distributed mode"},{"location":"kafka/connect/#verifying-the-connectors-via-the-rest-api","text":"The documentation about the REST APIs for the distributed connector is in this site . For example the http://localhost:8083/connectors is the base URL when running locally.","title":"Verifying the connectors via the REST api"},{"location":"kafka/connect/#deploy-the-kafka-connect-as-a-service-within-openshift-cluster","text":"When you use IBM Event Streams on Openshift, you can deploy the IBM kafka connector environment as Docker containers, and define the needed connect-* topics as explained in previous section. The product documentation describes how to do that. Another approach is to use Strimzi operator. To Be done!","title":"Deploy the Kafka connect as a service within Openshift cluster"},{"location":"kafka/connect/#running-with-local-kafka-cluster","text":"We are using a local kafka cluster started with docker-compose as defined in the compose file here . The docker network should be kafkanet , if not already created do the following docker network create kafkanet Start the kafka broker (bitnami distribution) and zookeeper node using the command below under the refarch-kc/docker folder: docker-compose -f backbone-compose.yml up -d Start a container with kafka code, to run a standalone connector: you need to use a worker configuration and a connector properties files. Those files will be mounted under the /home folder: docker run -ti -rm --name kconnect -v $( pwd ) :/home --network kafkanet -p 8083 :8083 bitnami/kafka:2 bash Need to map the port 8083, to access the REST APIs. Inside the container starts the standalone connector: cd /opt/bitnami/kafka ./bin/connect-standalone.sh /home/kafka-connect/worker-standalone.properties /home/kafka-connect/file-source.properties The above file configures a file reader to source the access_log.txt file to the clickstream topic: name = local-file-source connector.class = FileStreamSource tasks.max = 1 file = /home/kafka-connect/access_log.txt topic = clickstream The standalone connector worker configuration specifies where to connect, and what converters to use: bootstrap.servers = kafka1:9092 key.converter = org.apache.kafka.connect.json.JsonConverter value.converter = org.apache.kafka.connect.json.JsonConverter # Local storage file for offset data offset.storage.file.filename = /tmp/connect.offsets The execution trace shows the producer id INFO [ Producer clientId = connector - producer - local - file - source - 0 ] Cluster ID : tj8y0hiZSYWHB9vLHGP1Ew ( org . apache . kafka . clients . Metadata : 261 ) To validate the data are well published run another container with the consumer console tool: docker run -ti --name sinktrace --rm --network kafkanet bitnami/kafka:2 bash -c \" /opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic clickstream --from-beginning\" As the Json converter was used the trace show the message was wrapped into a json document with schema and payload. { \"schema\" :{ \"type\" : \"string\" , \"optional\" : false }, \"payload\" : \"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \\\"POST /xmlrpc.php HTTP/1.0\\\" 200 370 \\\"-\\\" \\\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\\\"\" }","title":"Running with local kafka cluster"},{"location":"kafka/connect/#further-readings","text":"Apache Kafka connect documentation Confluent Connector Documentation IBM Event Streams Connectors or the list of supported connectors MongoDB Connector for Apache Kafka","title":"Further Readings"},{"location":"kafka/consumers/","text":"Consumers design and implementation considerations Important concepts Consumers belong to consumer groups . You specify the group name as part of the connection parameters. properties . put ( ConsumerConfig . GROUP_ID_CONFIG , groupid ); Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. Organized in cluster the coordinator servers are responsible for assigning partitions to the consumers in the group. The rebalancing of partition to consumer is done when a new consumer joins or leaves the group or when a new partition is added to an existing topic. There is always at least one consumer per partition. Implementing a Topic consumer is using the kafka KafkaConsumer class which the API documentation is a must read. The implementation is simple for a single thread consumer, and the code structure looks like: prepare the properties create an instance of KafkaConsumer to connect to a topic and a partition loop on polling events process the ConsumerRecords and commit the offset by code or use the autocommit attibute of the consumer Examples of Java consumers can be found in the order management microservice project under the order-command-ms folder. Example of Javascript implementation is in this repository/folder But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer. Assess number of consumers needed The KafkaConsumer is not thread safe so it is recommended to run in a unique thread. But if needed you can implement a multi-threads solution, but as each thread will open a TCP connection to the Kafka broker, be sure to close the connection to avoid memory leak. The alternate is to start n processus (JVM process). If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use fine-grained control over offset persistence. You\u2019ll use one consumer per partition of a topic. This consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per consumer you need to be sure the total number of threads across all instances do not exceed the total number of partitions in the topic. Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer that belong to a group. When creating a new consumer you can specify the group id in the options. Offset management Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets of messages they have received to show they have processed the message and in case of failure from where they should reconnect. It is possible to commit by calling API or by setting some properties at the consumer creation level to enable autocommit offset. When doing manual offet, there are two types of manually committed: offsets\u2014synchronous asynchronous When dealing with heavy load storing offset in zookeeper is non advisable. It is even now recognize as a bad practice. To manage offset use the new consumer API, and for example commits offset synchronously when a specified number of events are read from the topic and the persistence to the back end succeed. Assess if it is possible to loose messages from topic. If so, when a consumer restarts it will start consuming the topic from the end of the queue. Does this solution acceptable? As the operation to store a message and the storage of offsets are two separate operations, and in case of failure between them, it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. \"exactly-once\" means grouping record and offset persistence in an atomic operation. Repositories with consumer code Within the Reefer ontainer shipment solution we have a order events consumer: order event agent Nodejs kafka consumers and producers A lot of python consumer in the integration tests, with or without Avro schema Kafka useful Consumer APIs KafkaConsumer a topic consumer which support: transparently handles brokers failure transparently adapt to partition migration within the cluster support grouping for load balancing among consumers maintains TCP connections to the necessary brokers to fetch data subscribe to multiple topics and being part of consumer groups each partition is assigned to exactly one consumer in the group if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group ConsumerRecords holds the list ConsumerRecord per partition for a particular topic. ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp References IBM Event Streams - Consuming messages KafkaConsumer class","title":"Kafka consumer development practices"},{"location":"kafka/consumers/#consumers-design-and-implementation-considerations","text":"","title":"Consumers design and implementation considerations"},{"location":"kafka/consumers/#important-concepts","text":"Consumers belong to consumer groups . You specify the group name as part of the connection parameters. properties . put ( ConsumerConfig . GROUP_ID_CONFIG , groupid ); Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. Organized in cluster the coordinator servers are responsible for assigning partitions to the consumers in the group. The rebalancing of partition to consumer is done when a new consumer joins or leaves the group or when a new partition is added to an existing topic. There is always at least one consumer per partition. Implementing a Topic consumer is using the kafka KafkaConsumer class which the API documentation is a must read. The implementation is simple for a single thread consumer, and the code structure looks like: prepare the properties create an instance of KafkaConsumer to connect to a topic and a partition loop on polling events process the ConsumerRecords and commit the offset by code or use the autocommit attibute of the consumer Examples of Java consumers can be found in the order management microservice project under the order-command-ms folder. Example of Javascript implementation is in this repository/folder But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer.","title":"Important concepts"},{"location":"kafka/consumers/#assess-number-of-consumers-needed","text":"The KafkaConsumer is not thread safe so it is recommended to run in a unique thread. But if needed you can implement a multi-threads solution, but as each thread will open a TCP connection to the Kafka broker, be sure to close the connection to avoid memory leak. The alternate is to start n processus (JVM process). If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use fine-grained control over offset persistence. You\u2019ll use one consumer per partition of a topic. This consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per consumer you need to be sure the total number of threads across all instances do not exceed the total number of partitions in the topic. Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer that belong to a group. When creating a new consumer you can specify the group id in the options.","title":"Assess number of consumers needed"},{"location":"kafka/consumers/#offset-management","text":"Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets of messages they have received to show they have processed the message and in case of failure from where they should reconnect. It is possible to commit by calling API or by setting some properties at the consumer creation level to enable autocommit offset. When doing manual offet, there are two types of manually committed: offsets\u2014synchronous asynchronous When dealing with heavy load storing offset in zookeeper is non advisable. It is even now recognize as a bad practice. To manage offset use the new consumer API, and for example commits offset synchronously when a specified number of events are read from the topic and the persistence to the back end succeed. Assess if it is possible to loose messages from topic. If so, when a consumer restarts it will start consuming the topic from the end of the queue. Does this solution acceptable? As the operation to store a message and the storage of offsets are two separate operations, and in case of failure between them, it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. \"exactly-once\" means grouping record and offset persistence in an atomic operation.","title":"Offset management"},{"location":"kafka/consumers/#repositories-with-consumer-code","text":"Within the Reefer ontainer shipment solution we have a order events consumer: order event agent Nodejs kafka consumers and producers A lot of python consumer in the integration tests, with or without Avro schema","title":"Repositories with consumer code"},{"location":"kafka/consumers/#kafka-useful-consumer-apis","text":"KafkaConsumer a topic consumer which support: transparently handles brokers failure transparently adapt to partition migration within the cluster support grouping for load balancing among consumers maintains TCP connections to the necessary brokers to fetch data subscribe to multiple topics and being part of consumer groups each partition is assigned to exactly one consumer in the group if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group ConsumerRecords holds the list ConsumerRecord per partition for a particular topic. ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp","title":"Kafka useful Consumer APIs"},{"location":"kafka/consumers/#references","text":"IBM Event Streams - Consuming messages KafkaConsumer class","title":"References"},{"location":"kafka/kafka-stream/","text":"Kafka Streaming Kafka Streams is a graph of processing nodes to implement the logic to process event streams. Each node process events from the parent node. We recommend reading this excellent introduction from Jay Kreps @confluent: Kafka stream made simple to get a good understanding of why Kafka stream was created. To summarize, Kafka Stream has the following capabilities: Stream processing is helpful for handling out-of-order data, reprocessing input as code changes, and performing stateful computations. It uses producer / consumer APIs, stateful storage and consumer groups. It treats both past and future data the same way. This is an embedded library to integrate in your application. Integrate tables for state persistence combined streams of events. Consumes continuous real time flows of records and publish new flows. Can scale vertically, by increasing the number of threads for each Kafka Streams application on a single machine, or horizontally by adding an additional machine with the same application.id . Supports exactly-once processing semantics to guarantee that each record will be processed once and only once even when there is a failure. Stream APIs transform, aggregate and enrich data, per record with milli second latency, from one topic to another one. Supports stateful and windowing operations by processing one record at a time. Can be integrated in java application. No need for separate processing cluster. It is a Java API. But a Stream app is executed outside of the broker code, which is different than message flow in an ESB. Elastic, highly scalable, fault tolerance, it can recover from failure. An application's processor topology is scaled by breaking it into multiple tasks. Tasks can then instantiate their own processor topology based on the assigned partitions. In general code for processing event does the following: Set a properties object to specify which brokers to connect to and what kind of serialization to use. Define a stream client: if you want stream of record use KStream, if you want a changelog with the last value of a given key use KTable (Example of using KTable is to keep a user profile with userid as key). In the reeefer container shipment implementation we use KTable to keep the Reefer container inventory in memory. Create a topology of input source and sink target and the set of actions to perform in between. Start the stream client to consume records. Programming with KStream and Ktable is not easy at first, as there are a lot of concepts for data manipulations, serialization and operations chaining. It uses function programming and chaining. A stateful operator uses the streaming Domain Specific Language, with constructs for aggregation, join and time window operations. Stateful transformations require a state store associated with the stream processor. The code below comes from Kafka examples and is counting word occurrence in text: final StreamsBuilder builder = new StreamsBuilder (); // pattern to extract word final Pattern pattern = Pattern . compile ( \"\\\\W+\" ); // source is a kafka topic KStream < String , String > textLines = builder . stream ( source ); KTable < String , Long > wordCounts = textLines . flatMapValues ( textLine -> Arrays . asList ( pattern . split ( textLine . toLowerCase ()))) . print ( Printed . toSysOut () . groupBy (( key , word ) -> word ) . count ( Materialized .< String , Long , KeyValueStore < Bytes , byte []>> as ( \"counts-store\" )); // sink is another kafka topic. Produce for each word the number of occurence in the given doc wordCounts . toStream (). to ( sink , Produced . with ( Serdes . String (), Serdes . Long ())); KafkaStreams streams = new KafkaStreams ( builder . build (), props ); streams . start (); KStream represents KeyValue records coming as event stream from the topic. flatMapValues() transforms the value of each record in \"this\" stream into zero or more values with the same key in a new KStream (in memory). So here the text line is split into words. The parameter is a ValueMapper which applies transformation on values but keeps the key. groupBy() Group the records of this KStream on a new key that is selected using the provided KeyValueMapper. So here it creates new KStream with the extracted word as key. count() counts the number of records in this stream by the grouped key. Materialized is an class to define a \"store\" to persist state and data. So here the state store is \"counts-store\". As store is a in-memory table, but it could also be persisted in external database. Could be the Facebook's RocksDB key value persistence or a log-compacted topic in Kafka. Produced defines how to provide the optional parameter types when producing to new topics. KTable is an abstraction of a changelog stream from a primary-keyed table. See this article from Confluent for deeper kafka stream architecture presentation. Example to run the Word Count application: Be sure to create the needed different topics once the Kafka broker is started (test-topic, streams-wordcount-output): docker exec - ti Kafka / bin / bash cd / scripts . / createtopics . sh Start a terminal window and execute the command to be ready to send message. $ docker exec - ti Kafka / bin / bash # can use the / scripts / openProducer . sh or ... root > / opt / Kafka_2 . 11 - 0 . 10 . 1 . 0 / bin / Kafka - console - producer . sh -- broker - list localhost : 9092 -- topic streams - plaintext - input Start another terminal to listen to the output topic: $ docker exec - ti Kafka / bin / bash # can use the / scripts / consumeWordCount . sh or ... root > / opt / Kafka_2 . 11 - 0 . 10 . 1 . 0 / bin / Kafka - console - consumer . sh -- bootstrap - server localhost : 9092 -- topic streams - wordcount - output -- from - beginning -- formatter Kafka . tools . DefaultMessageFormatter -- property print . key = true -- property print . value = true -- property key . deserializer = org . apache . Kafka . common . serialization . StringDeserializer -- property value . deserializer = org . apache . Kafka . common . serialization . LongDeserializer Start the stream client to count word in the entered lines mvn exec : java - Dexec . mainClass = ibm . cte . Kafka . play . WordCount Outputs of the WordCount application is actually a continuous stream of updates, where each output record is an updated count of a single word. A KTable is counting the occurrence of word, and a KStream send the output message with updated count. Some design considerations Avoid external database lookup as part of the stream: As kafka can handle million of records per second, so a lookup to an external database to do a join between a primary key that is in the event and a table in the database to do a data enrichment, for example is a bad practice. The approach will be to use Ktable, with state store and perform a join in memory. Faust: a python library to do kafka streaming Faust is a python library to support stream processing. It does not have its own DSL as Kafka streams in Java has, but just python functions. It uses rocksdb to support tables. For the installation, in your python environment do a pipenv run pip install faust , or pip install faust . Then use faust as a CLI. So to start an agent as worker use: faust - A nameofthepythoncode - l info Multiple instances of a Faust worker can be started independently to distribute stream processing across machines and CPU cores. Other examples We have implemented the container microservice of the Container Shipment solution using kstreams processing. See the presentation here , and go to the following code to see tests for the different process flow. Basic kstream processing on order events Further reading The API and product documentation . Deep dive explanation for the differences between KStream and KTable from Michael Noll Distributed, Real-time Joins and Aggregations using Kafka Stream, from Michael Noll at Confluent","title":"Kafka streaming processing"},{"location":"kafka/kafka-stream/#kafka-streaming","text":"Kafka Streams is a graph of processing nodes to implement the logic to process event streams. Each node process events from the parent node. We recommend reading this excellent introduction from Jay Kreps @confluent: Kafka stream made simple to get a good understanding of why Kafka stream was created. To summarize, Kafka Stream has the following capabilities: Stream processing is helpful for handling out-of-order data, reprocessing input as code changes, and performing stateful computations. It uses producer / consumer APIs, stateful storage and consumer groups. It treats both past and future data the same way. This is an embedded library to integrate in your application. Integrate tables for state persistence combined streams of events. Consumes continuous real time flows of records and publish new flows. Can scale vertically, by increasing the number of threads for each Kafka Streams application on a single machine, or horizontally by adding an additional machine with the same application.id . Supports exactly-once processing semantics to guarantee that each record will be processed once and only once even when there is a failure. Stream APIs transform, aggregate and enrich data, per record with milli second latency, from one topic to another one. Supports stateful and windowing operations by processing one record at a time. Can be integrated in java application. No need for separate processing cluster. It is a Java API. But a Stream app is executed outside of the broker code, which is different than message flow in an ESB. Elastic, highly scalable, fault tolerance, it can recover from failure. An application's processor topology is scaled by breaking it into multiple tasks. Tasks can then instantiate their own processor topology based on the assigned partitions. In general code for processing event does the following: Set a properties object to specify which brokers to connect to and what kind of serialization to use. Define a stream client: if you want stream of record use KStream, if you want a changelog with the last value of a given key use KTable (Example of using KTable is to keep a user profile with userid as key). In the reeefer container shipment implementation we use KTable to keep the Reefer container inventory in memory. Create a topology of input source and sink target and the set of actions to perform in between. Start the stream client to consume records. Programming with KStream and Ktable is not easy at first, as there are a lot of concepts for data manipulations, serialization and operations chaining. It uses function programming and chaining. A stateful operator uses the streaming Domain Specific Language, with constructs for aggregation, join and time window operations. Stateful transformations require a state store associated with the stream processor. The code below comes from Kafka examples and is counting word occurrence in text: final StreamsBuilder builder = new StreamsBuilder (); // pattern to extract word final Pattern pattern = Pattern . compile ( \"\\\\W+\" ); // source is a kafka topic KStream < String , String > textLines = builder . stream ( source ); KTable < String , Long > wordCounts = textLines . flatMapValues ( textLine -> Arrays . asList ( pattern . split ( textLine . toLowerCase ()))) . print ( Printed . toSysOut () . groupBy (( key , word ) -> word ) . count ( Materialized .< String , Long , KeyValueStore < Bytes , byte []>> as ( \"counts-store\" )); // sink is another kafka topic. Produce for each word the number of occurence in the given doc wordCounts . toStream (). to ( sink , Produced . with ( Serdes . String (), Serdes . Long ())); KafkaStreams streams = new KafkaStreams ( builder . build (), props ); streams . start (); KStream represents KeyValue records coming as event stream from the topic. flatMapValues() transforms the value of each record in \"this\" stream into zero or more values with the same key in a new KStream (in memory). So here the text line is split into words. The parameter is a ValueMapper which applies transformation on values but keeps the key. groupBy() Group the records of this KStream on a new key that is selected using the provided KeyValueMapper. So here it creates new KStream with the extracted word as key. count() counts the number of records in this stream by the grouped key. Materialized is an class to define a \"store\" to persist state and data. So here the state store is \"counts-store\". As store is a in-memory table, but it could also be persisted in external database. Could be the Facebook's RocksDB key value persistence or a log-compacted topic in Kafka. Produced defines how to provide the optional parameter types when producing to new topics. KTable is an abstraction of a changelog stream from a primary-keyed table. See this article from Confluent for deeper kafka stream architecture presentation.","title":"Kafka Streaming"},{"location":"kafka/kafka-stream/#example-to-run-the-word-count-application","text":"Be sure to create the needed different topics once the Kafka broker is started (test-topic, streams-wordcount-output): docker exec - ti Kafka / bin / bash cd / scripts . / createtopics . sh Start a terminal window and execute the command to be ready to send message. $ docker exec - ti Kafka / bin / bash # can use the / scripts / openProducer . sh or ... root > / opt / Kafka_2 . 11 - 0 . 10 . 1 . 0 / bin / Kafka - console - producer . sh -- broker - list localhost : 9092 -- topic streams - plaintext - input Start another terminal to listen to the output topic: $ docker exec - ti Kafka / bin / bash # can use the / scripts / consumeWordCount . sh or ... root > / opt / Kafka_2 . 11 - 0 . 10 . 1 . 0 / bin / Kafka - console - consumer . sh -- bootstrap - server localhost : 9092 -- topic streams - wordcount - output -- from - beginning -- formatter Kafka . tools . DefaultMessageFormatter -- property print . key = true -- property print . value = true -- property key . deserializer = org . apache . Kafka . common . serialization . StringDeserializer -- property value . deserializer = org . apache . Kafka . common . serialization . LongDeserializer Start the stream client to count word in the entered lines mvn exec : java - Dexec . mainClass = ibm . cte . Kafka . play . WordCount Outputs of the WordCount application is actually a continuous stream of updates, where each output record is an updated count of a single word. A KTable is counting the occurrence of word, and a KStream send the output message with updated count.","title":"Example to run the Word Count application:"},{"location":"kafka/kafka-stream/#some-design-considerations","text":"Avoid external database lookup as part of the stream: As kafka can handle million of records per second, so a lookup to an external database to do a join between a primary key that is in the event and a table in the database to do a data enrichment, for example is a bad practice. The approach will be to use Ktable, with state store and perform a join in memory.","title":"Some design considerations"},{"location":"kafka/kafka-stream/#faust-a-python-library-to-do-kafka-streaming","text":"Faust is a python library to support stream processing. It does not have its own DSL as Kafka streams in Java has, but just python functions. It uses rocksdb to support tables. For the installation, in your python environment do a pipenv run pip install faust , or pip install faust . Then use faust as a CLI. So to start an agent as worker use: faust - A nameofthepythoncode - l info Multiple instances of a Faust worker can be started independently to distribute stream processing across machines and CPU cores.","title":"Faust: a python library to do kafka streaming"},{"location":"kafka/kafka-stream/#other-examples","text":"We have implemented the container microservice of the Container Shipment solution using kstreams processing. See the presentation here , and go to the following code to see tests for the different process flow. Basic kstream processing on order events","title":"Other examples"},{"location":"kafka/kafka-stream/#further-reading","text":"The API and product documentation . Deep dive explanation for the differences between KStream and KTable from Michael Noll Distributed, Real-time Joins and Aggregations using Kafka Stream, from Michael Noll at Confluent","title":"Further reading"},{"location":"kafka/monitoring/","text":"Monitoring Kafka with Prometheus and Grafana Author: Ana Giordano - IBM A comprehensive Kafka monitoring plan should collect metrics from the following components: Kafka Broker(s) Kafka Cluster (which should include ZooKeeper metrics as Kafka relies on it to maintain its state) Producer(s) / Consumer(s) Kafka Broker, Zookeeper and Java clients (producer/consumer) expose metrics via JMX (Java Management Extensions) and can be configured to report stats back to Prometheus using the JMX exporter maintained by Prometheus. There is also a number of exporters maintained by the community to explore. Some of them can be used in addition to the JMX export. To monitor Kafka, for example, the JMX exporter is often used to provide broker level metrics, while community exporters claim to provide more accurate cluster level metrics (e.g. Kafka exporter , Kafka Zookeeper Exporter by CloudFlare , and others). Alternatively, you can consider writing your own custom exporter . What to monitor A long list of metrics is made available by Kafka ( here ) and Zookeeper ( here ). The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or Kafka/Prometheus server; this will allow browsing all metrics with JMX. But you are still left to figure out which ones you want to actively monitor and the ones that you want to be actively alerted. An simple way to get started would be to start with the Grafana\u2019s sample dashboards for the Prometheus exporters you chose to use and then modify them as you learn more about the available metrics and/or your environment on ICP. The Monitoring Kafka metrics article by DataDog and How to monitor Kafka by Server Density provides guidance on key Kafka and Prometheus metrics, reasoning to why you should care about them and suggestions on thresholds to trigger alerts. In the next section, we will demonstrate exactly that; we will start with sample dashboards and make few modifications to exemplify how to configure key Kafka metrics to display in the dashboard. Configuring server and agents For convenience and easy configuration, we will use Docker images from DockerHub and make few modifications to DockerFiles to include few additional steps to install, configure and start the servers and exporter agents locally. Kafka and Zookeeper servers with JMX Exporter We will start with the DockerFile of the Spotify kafka image from DockerHub as it includes Zookeeper and Kafka in a single image. The DockerFile was modified as shown below to download, install the Prometheus JMX exporter. The exporter can be configured to scrape and expose mBeans of a JMX target. It runs as a Java Agent, exposing a HTTP server and serving metrics of the JVM. In the DockerFile below, Kafka is started with JMX exporter agent on port 7071 and metrics will be expose in the /metrics endpoint. FROM java : openjdk - 8 - jre ENV DEBIAN_FRONTEND noninteractive ENV SCALA_VERSION 2 . 11 ENV KAFKA_VERSION 0 . 10 . 2 . 2 ENV KAFKA_HOME / opt / kafka_ \" $SCALA_VERSION \" - \" $KAFKA_VERSION \" # Install Kafka , Zookeeper and other needed things RUN apt - get update && \\ apt - get install - y zookeeper wget supervisor dnsutils vim && \\ rm - rf / var / lib / apt / lists /* && \\ apt-get clean && \\ wget -q http://apache.mirrors.spacedump.net/kafka/\"$KAFKA_VERSION\"/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz -O /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz && \\ tar xfz /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz -C /opt && \\ rm /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz ADD scripts/start-kafka.sh /usr/bin/start-kafka.sh # ADD scripts/jmx_prometheus_javaagent-0.9.jar \"$KAFKA_HOME\"/jmx_prometheus_javaagent-0.9.jar # ADD scripts/kafka-0-8-2.yml \"$KAFKA_HOME\"/kafka-0-8-2.yml # Supervisor config ADD supervisor/kafka.conf supervisor/zookeeper.conf /etc/supervisor/conf.d/ # 2181 is zookeeper, 9092 is kafka EXPOSE 2181 9092 # ********** # start - modifications to run Prometheus JMX exporter and community Kafka exporter agents ENV KAFKA_OPTS \"-javaagent:$KAFKA_HOME/jmx_prometheus_javaagent-0.9.jar=7071:$KAFKA_HOME/kafka-0-8-2.yml\" RUN wget -q https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.9/jmx_prometheus_javaagent-0.9.jar -O \"$KAFKA_HOME\"/jmx_prometheus_javaagent-0.9.jar && \\ wget -q https://raw.githubusercontent.com/prometheus/jmx_exporter/master/example_configs/kafka-0-8-2.yml -O \"$KAFKA_HOME\"/kafka-0-8-2.yml EXPOSE 7071 # end - modifications # ********** CMD [\"supervisord\", \"-n\"] For your convenience, the modified DockerFile and scripts are available on this GitHub repository . You can run the following commands to create and run the container locally. download git repo with DockerFile and scripts mkdir / tmp / monitor git clone https : // github . com / anagiordano / ibm - artifacts . git / tmp / monitor / . Build image from DockerFile docker build --tag kafka_i /tmp/monitor/kafka/. Create/Run Docker container docker run - d - p 2181 : 2181 - p 9092 : 9092 - p 7071 : 7071 --env ADVERTISED_PORT=9092 --name kafka_c kafka_i Create kafka topics docker exec - it kafka_c / bin / bash cd / opt / kafka */ bin export KAFKA_OPTS = \"\" . / kafka - topics . sh -- create -- zookeeper localhost : 2181 -- replication - fact 1 -- partitions 1 -- topic my - topic1 . / kafka - topics . sh -- create -- zookeeper localhost : 2181 -- replication - fact 1 -- partitions 1 -- topic my - topic2 . / kafka - topics . sh -- list -- zookeeper localhost : 2181 (optional) Produce few message into topics from console and exit container . / kafka - console - producer . sh -- broker - list localhost : 9092 -- topic my - topic1 . / kafka - console - producer . sh -- broker - list localhost : 9092 -- topic my - topic2 exit Lastly you can validate that the /metrics endpoint is returning metrics from Kafka. On a browser, open the http://localhost:7071/metrics URL. Prometheus Server and scrape jobs Prometheus uses a configuration file in YAML format to define the scraping jobs and their instances . You can also use the configuration file to define recording rules and alerting rules : Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. This is especially useful for dashboards, which need to query the same expression repeatedly every time they refresh. Alerting rules allow you to define alert conditions based on Prometheus expression language expressions and to send notifications about firing alerts to an external service. Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, PagerDuty and others. Below, we will go thru the steps to stand-up a local Prometheus server as a Docker container and to modify the configuration file to scrape Kafka metrics. Create/run a docker container using Prometheus official image from DockerHub docker run - d - p 9090 : 9090 prom / prometheus Obtain the IP address of the Kafka container docker inspect kafka_c | grep IPAddress Edit the prometheus.yml to add Kafka as a target docker exec - it prometheus_c \\ sh vi / etc / prometheus / prometheus . yml Locate the scrape_configs section in the properties file and add the lines below to define the Kafka job, where the IP should be the IP of the kafka container - job_name : 'kafka' static_configs : - targets : [ '172.17.0.4:7071' ] 5. Reload the configuration file ps - ef kill - HUP < prometheus PID > You can now verify that Kafka is listed as a target job in Prometheus. On a Browser, open the http://localhost:9090/targets URL. Grafana Server and dashboards We will use Grafana for visualization of the metrics scraped by Prometheus for that, we will need to: Stand-up a local Grafana server as a Docker container Configure Prometheus as a data source in Grafana Import sample dashboards provided by Grafana and/or community Modify the sample dashboards as we see fit Let\u2019s get started: Create a docker container using Prometheus official image from DockerHub docker run - d --name=grafana_c -p 3000:3000 grafana/grafana On a Browser, open the http://localhost:3000 URL. Login as admin/admin . You will be prompted to change the password. Once logged in, Grafana provides visual guidance on what the next steps are: a) Add data sources b) Create first dashboard and others Configure Prometheus as a data source: Enter a Name for the data source (e.g. Prometheus) Select Prometheus as Type Enter http://localhost:9090 for HTTP URL In our simple server configuration, select Browser for HTTP Access Click Save and Test to validate configuration Back to Home, click Dashboards -> Manage to import sample dashboards Click the +Import button and paste this URL https://grafana.com/dashboards/721 Make sure to select Prometheus as the data source. NOTE: You can also explore other sample dashboard options at https://grafana.com/dashboards. For instance, there is a Kubernetes Kafka resource metrics sample dashboard that you could use instead as the starting point when configuring Kafka monitoring on ICP. The six graphs displayed in the dashboard are configured as follows: NOTE: You might want to go back to your Kafka Docker container and push messages into the topics you have created above to see changes to the graph. Or, if you have already pushed messages, you can change the Quick Range from last 5 minutes to something else (e.g. last 6 hours ) on the top right hand corner of the dashboard. Graph Formula Format As CPU Usage rate(process_cpu_seconds_total{job=\"kafka\"}[1m]) Time Series JVM Memory Used sum without(area)(jvm_memory_bytes_used{job=\"kafka\"}) Time Series Time spent in GC sum without(gc)(rate(jvm_gc_collection_seconds_sum{job=\"kafka\"}[5m])) Time Series Messages In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_messagesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes Out per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesout_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Prometheus provides a functional expression language that lets the user select and aggregate time series data in real time. Before proceeding review the information on these pages to gain basic understanding of: Prometheus Expression language - http://docs.grafana.org/features/datasources/prometheus/ Grafana Query Editor - http://docs.grafana.org/features/datasources/prometheus/ As you make modifications to the dashboard it is also important to understand the data returned by the scrape jobs in the first place. For two of the metrics above, this is what the Kafka JMX exportex returns. You can go to https://localhost:7071/metrics to inspect others returned in /metrics endpoint response: Messages in Per Topic Time spent in GC","title":"Kafka monitoring"},{"location":"kafka/monitoring/#monitoring-kafka-with-prometheus-and-grafana","text":"Author: Ana Giordano - IBM A comprehensive Kafka monitoring plan should collect metrics from the following components: Kafka Broker(s) Kafka Cluster (which should include ZooKeeper metrics as Kafka relies on it to maintain its state) Producer(s) / Consumer(s) Kafka Broker, Zookeeper and Java clients (producer/consumer) expose metrics via JMX (Java Management Extensions) and can be configured to report stats back to Prometheus using the JMX exporter maintained by Prometheus. There is also a number of exporters maintained by the community to explore. Some of them can be used in addition to the JMX export. To monitor Kafka, for example, the JMX exporter is often used to provide broker level metrics, while community exporters claim to provide more accurate cluster level metrics (e.g. Kafka exporter , Kafka Zookeeper Exporter by CloudFlare , and others). Alternatively, you can consider writing your own custom exporter .","title":"Monitoring Kafka with Prometheus and Grafana"},{"location":"kafka/monitoring/#what-to-monitor","text":"A long list of metrics is made available by Kafka ( here ) and Zookeeper ( here ). The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or Kafka/Prometheus server; this will allow browsing all metrics with JMX. But you are still left to figure out which ones you want to actively monitor and the ones that you want to be actively alerted. An simple way to get started would be to start with the Grafana\u2019s sample dashboards for the Prometheus exporters you chose to use and then modify them as you learn more about the available metrics and/or your environment on ICP. The Monitoring Kafka metrics article by DataDog and How to monitor Kafka by Server Density provides guidance on key Kafka and Prometheus metrics, reasoning to why you should care about them and suggestions on thresholds to trigger alerts. In the next section, we will demonstrate exactly that; we will start with sample dashboards and make few modifications to exemplify how to configure key Kafka metrics to display in the dashboard.","title":"What to monitor"},{"location":"kafka/monitoring/#configuring-server-and-agents","text":"For convenience and easy configuration, we will use Docker images from DockerHub and make few modifications to DockerFiles to include few additional steps to install, configure and start the servers and exporter agents locally.","title":"Configuring server and agents"},{"location":"kafka/monitoring/#kafka-and-zookeeper-servers-with-jmx-exporter","text":"We will start with the DockerFile of the Spotify kafka image from DockerHub as it includes Zookeeper and Kafka in a single image. The DockerFile was modified as shown below to download, install the Prometheus JMX exporter. The exporter can be configured to scrape and expose mBeans of a JMX target. It runs as a Java Agent, exposing a HTTP server and serving metrics of the JVM. In the DockerFile below, Kafka is started with JMX exporter agent on port 7071 and metrics will be expose in the /metrics endpoint. FROM java : openjdk - 8 - jre ENV DEBIAN_FRONTEND noninteractive ENV SCALA_VERSION 2 . 11 ENV KAFKA_VERSION 0 . 10 . 2 . 2 ENV KAFKA_HOME / opt / kafka_ \" $SCALA_VERSION \" - \" $KAFKA_VERSION \" # Install Kafka , Zookeeper and other needed things RUN apt - get update && \\ apt - get install - y zookeeper wget supervisor dnsutils vim && \\ rm - rf / var / lib / apt / lists /* && \\ apt-get clean && \\ wget -q http://apache.mirrors.spacedump.net/kafka/\"$KAFKA_VERSION\"/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz -O /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz && \\ tar xfz /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz -C /opt && \\ rm /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz ADD scripts/start-kafka.sh /usr/bin/start-kafka.sh # ADD scripts/jmx_prometheus_javaagent-0.9.jar \"$KAFKA_HOME\"/jmx_prometheus_javaagent-0.9.jar # ADD scripts/kafka-0-8-2.yml \"$KAFKA_HOME\"/kafka-0-8-2.yml # Supervisor config ADD supervisor/kafka.conf supervisor/zookeeper.conf /etc/supervisor/conf.d/ # 2181 is zookeeper, 9092 is kafka EXPOSE 2181 9092 # ********** # start - modifications to run Prometheus JMX exporter and community Kafka exporter agents ENV KAFKA_OPTS \"-javaagent:$KAFKA_HOME/jmx_prometheus_javaagent-0.9.jar=7071:$KAFKA_HOME/kafka-0-8-2.yml\" RUN wget -q https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.9/jmx_prometheus_javaagent-0.9.jar -O \"$KAFKA_HOME\"/jmx_prometheus_javaagent-0.9.jar && \\ wget -q https://raw.githubusercontent.com/prometheus/jmx_exporter/master/example_configs/kafka-0-8-2.yml -O \"$KAFKA_HOME\"/kafka-0-8-2.yml EXPOSE 7071 # end - modifications # ********** CMD [\"supervisord\", \"-n\"] For your convenience, the modified DockerFile and scripts are available on this GitHub repository . You can run the following commands to create and run the container locally. download git repo with DockerFile and scripts mkdir / tmp / monitor git clone https : // github . com / anagiordano / ibm - artifacts . git / tmp / monitor / . Build image from DockerFile docker build --tag kafka_i /tmp/monitor/kafka/. Create/Run Docker container docker run - d - p 2181 : 2181 - p 9092 : 9092 - p 7071 : 7071 --env ADVERTISED_PORT=9092 --name kafka_c kafka_i Create kafka topics docker exec - it kafka_c / bin / bash cd / opt / kafka */ bin export KAFKA_OPTS = \"\" . / kafka - topics . sh -- create -- zookeeper localhost : 2181 -- replication - fact 1 -- partitions 1 -- topic my - topic1 . / kafka - topics . sh -- create -- zookeeper localhost : 2181 -- replication - fact 1 -- partitions 1 -- topic my - topic2 . / kafka - topics . sh -- list -- zookeeper localhost : 2181 (optional) Produce few message into topics from console and exit container . / kafka - console - producer . sh -- broker - list localhost : 9092 -- topic my - topic1 . / kafka - console - producer . sh -- broker - list localhost : 9092 -- topic my - topic2 exit Lastly you can validate that the /metrics endpoint is returning metrics from Kafka. On a browser, open the http://localhost:7071/metrics URL.","title":"Kafka and Zookeeper servers with JMX Exporter"},{"location":"kafka/monitoring/#prometheus-server-and-scrape-jobs","text":"Prometheus uses a configuration file in YAML format to define the scraping jobs and their instances . You can also use the configuration file to define recording rules and alerting rules : Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. This is especially useful for dashboards, which need to query the same expression repeatedly every time they refresh. Alerting rules allow you to define alert conditions based on Prometheus expression language expressions and to send notifications about firing alerts to an external service. Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, PagerDuty and others. Below, we will go thru the steps to stand-up a local Prometheus server as a Docker container and to modify the configuration file to scrape Kafka metrics. Create/run a docker container using Prometheus official image from DockerHub docker run - d - p 9090 : 9090 prom / prometheus Obtain the IP address of the Kafka container docker inspect kafka_c | grep IPAddress Edit the prometheus.yml to add Kafka as a target docker exec - it prometheus_c \\ sh vi / etc / prometheus / prometheus . yml Locate the scrape_configs section in the properties file and add the lines below to define the Kafka job, where the IP should be the IP of the kafka container - job_name : 'kafka' static_configs : - targets : [ '172.17.0.4:7071' ] 5. Reload the configuration file ps - ef kill - HUP < prometheus PID > You can now verify that Kafka is listed as a target job in Prometheus. On a Browser, open the http://localhost:9090/targets URL.","title":"Prometheus Server and scrape jobs"},{"location":"kafka/monitoring/#grafana-server-and-dashboards","text":"We will use Grafana for visualization of the metrics scraped by Prometheus for that, we will need to: Stand-up a local Grafana server as a Docker container Configure Prometheus as a data source in Grafana Import sample dashboards provided by Grafana and/or community Modify the sample dashboards as we see fit Let\u2019s get started: Create a docker container using Prometheus official image from DockerHub docker run - d --name=grafana_c -p 3000:3000 grafana/grafana On a Browser, open the http://localhost:3000 URL. Login as admin/admin . You will be prompted to change the password. Once logged in, Grafana provides visual guidance on what the next steps are: a) Add data sources b) Create first dashboard and others Configure Prometheus as a data source: Enter a Name for the data source (e.g. Prometheus) Select Prometheus as Type Enter http://localhost:9090 for HTTP URL In our simple server configuration, select Browser for HTTP Access Click Save and Test to validate configuration Back to Home, click Dashboards -> Manage to import sample dashboards Click the +Import button and paste this URL https://grafana.com/dashboards/721 Make sure to select Prometheus as the data source. NOTE: You can also explore other sample dashboard options at https://grafana.com/dashboards. For instance, there is a Kubernetes Kafka resource metrics sample dashboard that you could use instead as the starting point when configuring Kafka monitoring on ICP. The six graphs displayed in the dashboard are configured as follows: NOTE: You might want to go back to your Kafka Docker container and push messages into the topics you have created above to see changes to the graph. Or, if you have already pushed messages, you can change the Quick Range from last 5 minutes to something else (e.g. last 6 hours ) on the top right hand corner of the dashboard. Graph Formula Format As CPU Usage rate(process_cpu_seconds_total{job=\"kafka\"}[1m]) Time Series JVM Memory Used sum without(area)(jvm_memory_bytes_used{job=\"kafka\"}) Time Series Time spent in GC sum without(gc)(rate(jvm_gc_collection_seconds_sum{job=\"kafka\"}[5m])) Time Series Messages In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_messagesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes Out per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesout_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Prometheus provides a functional expression language that lets the user select and aggregate time series data in real time. Before proceeding review the information on these pages to gain basic understanding of: Prometheus Expression language - http://docs.grafana.org/features/datasources/prometheus/ Grafana Query Editor - http://docs.grafana.org/features/datasources/prometheus/ As you make modifications to the dashboard it is also important to understand the data returned by the scrape jobs in the first place. For two of the metrics above, this is what the Kafka JMX exportex returns. You can go to https://localhost:7071/metrics to inspect others returned in /metrics endpoint response: Messages in Per Topic Time spent in GC","title":"Grafana Server and dashboards"},{"location":"kafka/producers/","text":"Producers considerations A producer is a thread safe kafka client API that publishes records to the cluster. It uses buffers, thread pool, and serializer to send data. They are stateless: the consumers is responsible to manage the offsets of the message they read. When the producer connects via the initial bootstrap connection, it gets the metadata about the topic - partition and the leader broker to connect to. The assignment of message to partition is done following different algorithms: round-robin if there is no key specified, using the hash code of the key, or custom defined. We recommend reading IBM Event streams producer guidelines to understand how producers work and some configuration parameters. Design considerations When developing a record producer you need to assess the following: What is the event payload to send? Is is an aggregate, as domain driven design concept, with value objects that needs to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions from Martin Kleppmann on confluent web site . Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowleges may take time to come: within a batch of n records, if the producer did not get all the n acknowledge on time, it may resend the batch. This is where 'idempotence' becomes important. Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible avro model . What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with buffer.memory property. (See producer configuration API Can the producer batch events together to send them in batch over one send operation? By design kafka producers batch events. Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5 brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection. When deploying kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see section below). Partitions help to scale the consumer processing of message, but it also helps the producer to be more efficient as it can send message in parallel to different partition. Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that LogAppendTime is considered to be processing time, and CreateTime is considered to be event time. Typical producer code structure The producer code, using java or python API, does the following steps: define producer properties create a producer instance send event records and get resulting metadata. Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgement. Kafka useful Producer APIs Here is a list of common API to use in your producer and consumer code. KafkaProducer A Kafka client that publishes records to the Kafka cluster. The send method is asynchronous. A producer is thread safe so we can have per topic to interface. ProducerRecord to be published to a topic RecordMetadata metadata for a record that has been acknowledged by the server. Properties to consider The following properties are helpful to tune at each topic and producer and will vary depending on the deployment: Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to All . How to support exactly once delivery Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events. Producer can set acknowledge level to control the delivery semantic: At least once : means the producer set ACKS_CONFIG=1 and get an acknowledgement message when the message sent, has been written at least one time in the cluster (assume replicas = 3). If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message. At most semantic : means the producer will not do retry in case of no acknowldege received. It may create log and compensation, but the message is lost. Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event. With the idempotence property (ENABLE_IDEMPOTENCE_CONFIG = true), the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. The sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system. If the producer resends its batch, the broker will drop the messages with a sequence less or equals to the last committed sequence number, and accept the other ones. Note The replication mechanism guarantees that when a message is written to the leader replica, it will be replicated to all available replicas. As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance. To add to these, as topic may have multiple partitions, kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a unique transaction identifier to keep integrated state. Here is an example of such configuration that can be done in a producer contructor method: producerProps . put ( \"enable.idempotence\" , \"true\" ); producerProps . put ( \"transactional.id\" , \"prod-1\" ); kafkaProducer . initTransactions () initTransactions() registers the producer with the broker as one that can use transactions, identifying it by its transactional.id and a sequence number, or epoch. In case of multiple partitions, the broker will store a list of all updated partitions for a given transaction. See the code in order command microservice. The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration consumerProps . put ( \"enable.auto.commit\" , \"false\" ); consumerProps . put ( \"isolation.level\" , \"read_committed\" ); With read_committed , no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written. The consumer commits its offset with code, and specifies the last offset to read. offsetsToCommit . put ( partition , new OffsetAndMetadata ( offset + 1 )) producer . sendOffsetsToTransaction ( offsetsToCommit , \"order-group-id\" ); The producer then commits the transaction. try { kafkaProducer . beginTransaction (); ProducerRecord < String , String > record = new ProducerRecord <>( ApplicationConfig . ORDER_COMMAND_TOPIC , key , value ); Future < RecordMetadata > send = kafkaProducer . send ( record ); send . get ( ApplicationConfig . PRODUCER_TIMEOUT_SECS , TimeUnit . SECONDS ); kafkaProducer . commitTransaction (); } catch ( KafkaException e ){ kafkaProducer . abortTransaction (); } There is an interesting article from the Baeldung team about exactly once processing in kafka with code example. Code Examples Simple text message Order management with CQRS in Java Ship movement and container metrics event producers Springboot with kafka template More readings Creating advanced kafka producer in java - Cloudurable Confluent blog: Exactly-once Semantics are Possible: Here\u2019s How Kafka Does it","title":"Kafka producer development practices"},{"location":"kafka/producers/#producers-considerations","text":"A producer is a thread safe kafka client API that publishes records to the cluster. It uses buffers, thread pool, and serializer to send data. They are stateless: the consumers is responsible to manage the offsets of the message they read. When the producer connects via the initial bootstrap connection, it gets the metadata about the topic - partition and the leader broker to connect to. The assignment of message to partition is done following different algorithms: round-robin if there is no key specified, using the hash code of the key, or custom defined. We recommend reading IBM Event streams producer guidelines to understand how producers work and some configuration parameters.","title":"Producers considerations"},{"location":"kafka/producers/#design-considerations","text":"When developing a record producer you need to assess the following: What is the event payload to send? Is is an aggregate, as domain driven design concept, with value objects that needs to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions from Martin Kleppmann on confluent web site . Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowleges may take time to come: within a batch of n records, if the producer did not get all the n acknowledge on time, it may resend the batch. This is where 'idempotence' becomes important. Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible avro model . What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with buffer.memory property. (See producer configuration API Can the producer batch events together to send them in batch over one send operation? By design kafka producers batch events. Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5 brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection. When deploying kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see section below). Partitions help to scale the consumer processing of message, but it also helps the producer to be more efficient as it can send message in parallel to different partition. Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that LogAppendTime is considered to be processing time, and CreateTime is considered to be event time.","title":"Design considerations"},{"location":"kafka/producers/#typical-producer-code-structure","text":"The producer code, using java or python API, does the following steps: define producer properties create a producer instance send event records and get resulting metadata. Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgement.","title":"Typical producer code structure"},{"location":"kafka/producers/#kafka-useful-producer-apis","text":"Here is a list of common API to use in your producer and consumer code. KafkaProducer A Kafka client that publishes records to the Kafka cluster. The send method is asynchronous. A producer is thread safe so we can have per topic to interface. ProducerRecord to be published to a topic RecordMetadata metadata for a record that has been acknowledged by the server.","title":"Kafka useful Producer APIs"},{"location":"kafka/producers/#properties-to-consider","text":"The following properties are helpful to tune at each topic and producer and will vary depending on the deployment: Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to All .","title":"Properties to consider"},{"location":"kafka/producers/#how-to-support-exactly-once-delivery","text":"Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events. Producer can set acknowledge level to control the delivery semantic: At least once : means the producer set ACKS_CONFIG=1 and get an acknowledgement message when the message sent, has been written at least one time in the cluster (assume replicas = 3). If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message. At most semantic : means the producer will not do retry in case of no acknowldege received. It may create log and compensation, but the message is lost. Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event. With the idempotence property (ENABLE_IDEMPOTENCE_CONFIG = true), the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. The sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system. If the producer resends its batch, the broker will drop the messages with a sequence less or equals to the last committed sequence number, and accept the other ones. Note The replication mechanism guarantees that when a message is written to the leader replica, it will be replicated to all available replicas. As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance. To add to these, as topic may have multiple partitions, kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a unique transaction identifier to keep integrated state. Here is an example of such configuration that can be done in a producer contructor method: producerProps . put ( \"enable.idempotence\" , \"true\" ); producerProps . put ( \"transactional.id\" , \"prod-1\" ); kafkaProducer . initTransactions () initTransactions() registers the producer with the broker as one that can use transactions, identifying it by its transactional.id and a sequence number, or epoch. In case of multiple partitions, the broker will store a list of all updated partitions for a given transaction. See the code in order command microservice. The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration consumerProps . put ( \"enable.auto.commit\" , \"false\" ); consumerProps . put ( \"isolation.level\" , \"read_committed\" ); With read_committed , no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written. The consumer commits its offset with code, and specifies the last offset to read. offsetsToCommit . put ( partition , new OffsetAndMetadata ( offset + 1 )) producer . sendOffsetsToTransaction ( offsetsToCommit , \"order-group-id\" ); The producer then commits the transaction. try { kafkaProducer . beginTransaction (); ProducerRecord < String , String > record = new ProducerRecord <>( ApplicationConfig . ORDER_COMMAND_TOPIC , key , value ); Future < RecordMetadata > send = kafkaProducer . send ( record ); send . get ( ApplicationConfig . PRODUCER_TIMEOUT_SECS , TimeUnit . SECONDS ); kafkaProducer . commitTransaction (); } catch ( KafkaException e ){ kafkaProducer . abortTransaction (); } There is an interesting article from the Baeldung team about exactly once processing in kafka with code example.","title":"How to support exactly once delivery"},{"location":"kafka/producers/#code-examples","text":"Simple text message Order management with CQRS in Java Ship movement and container metrics event producers Springboot with kafka template","title":"Code Examples"},{"location":"kafka/producers/#more-readings","text":"Creating advanced kafka producer in java - Cloudurable Confluent blog: Exactly-once Semantics are Possible: Here\u2019s How Kafka Does it","title":"More readings"},{"location":"kafka/readme/","text":"Apache Kafka In this article we are summarizing what Apache Kafka is and are grouping some references and notes we gathered during our different implementations and Kafka deployment within Kubernetes cluster. We are documenting how to deploy Kafka on IBM Cloud Private or deploying IBM Event Streams product . This content does not replace the excellent introduction every developer using Kafka should read. Introduction Kafka is a distributed real time event streaming platform with the following key capabilities: Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far. It can handle hundreds of read and write operations per second from many producers and consumers Atomic broadcast, send a record once, every subscriber gets it once. Store streams of data records on disk and replicate within the distributed cluster for fault-tolerance. Persist data for a given time period before delete. Can grow elastically and transparently with no downtime. Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available. Use cases The typical use cases where Kafka helps are: Centralize online data pipeline to decouple applications and microservices. Pub/sub messaging for cloud native application inter microservices. Aggregation of event coming from multiple producers. Monitor distributed applications to produce centralized feed of operational data. Logs collector from multiple services. Implement event soucing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between brokers within the cluster and cross availability zones if needed. Manage loosely coupled communication between microservices. (See this note where we present a way to support a service mesh solution using asynchronous event) Key concepts The diagram below presents Kafka's key components: Brokers Kafka runs as a cluster of one or more broker servers that can, in theory, span multiple data centers. It is really possible if the network latency between data centers is very low, at the 10ms or better, as there are a lot of communication between kafka brokers and between kafka and zookeepers. So the advice is to avoid cross data centers deployment. The Kafka cluster stores streams of records in topics . Topic is referenced by producer to send data to, and subscribed by consumers to get data. Data in topic is persisted to file systems for a retention time period (Defined at the topic level). The file system can be network based. In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated two times. In production, it is recommended to use at least five nodes to authorise planned failure and un-planned failure, and when doing replicas, use a replica factor equals to the number of brokers. Topics Topics represent end points to put or get records to. Each record consists of a key, a value (the data payload as byte array), and a timestamp. Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffers to keep records not yet transmitted to the server. Kafka store log data in its log.dir and topic maps to subdirectories in this log directory. Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups. Consumer performs asynchronous pull to the connected brokers via the subscription to a topic. The figure below illustrates one topic having multiple partitions, replicated within the broker cluster: Partitions Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages, Kafka uses partitions. Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use. Kafka tolerates up to N-1 server failure without losing any messages. N is the replication factor for a given partition. Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log. Consumers see messages in the order they are stored in the log. Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... You can have 2000 partitions per broker. Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are inmutable. When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from. Brokers keep offset information in an hidden topic. Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Replication Each partition can be replicated accross a number of server. The replication factor is capted by the number of brokers. Partitions have one leader and zero or more followers. The leader manages all the read and write requests for the partition. Leader is also responsible to track the \"in sync\" replicas. The followers replicate the leader content. If a leader fails, followers elect a new one. When a producer sends message, it can control how to get the response from the committed message: wait for all replicas to succeed, wait for one acknowledge, fire and forget. Consumers receive only committed messages. Zookeeper Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup. Kafka does not keep state regarding consumers and producers. Depends on kafka version, offsets are maintained in Zookeeper or in Kafka : newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages. Access Controls are saved in Zookeeper Consumer group This is the way to group consumers so the processing of event is parallelized. The number of consumers in a group is the same as the number of partition defined in a topic. We are detailing consumer group implementation in this note Architecture See this separate note addresses high availability and kubernetes deployment. Solution considerations There are a set of design considerations to assess for each Kafka solution: Topics Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node. What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic. The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions. When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together. Other best practices: When event order is important use the same topic and use the entity unique identifier as partition key. When two entities are related together by containment relationship then they can be in the same topic. Different entities are separated to different topics. It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics. Clearly define the partition key as it could be an compound key based on multiple entities. With Kafka stream, state store or KTable, you should separate the changelog topic from the others. Producers When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size Assess once to exactly once delivery requirement. Look at idempotent producer. See implementation considerations discussion Consumers From the consumer point of view a set of items need to be addressed during design phase: Do you need to group consumers for parallel consumption of events? What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?. How to persist consumer committed position? (the last offset that has been stored securely) Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code. Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records? Do the consumer needs to perform joins, aggregations between multiple partitions? See implementation considerations discussion See also the compendium note for more readings.","title":"Kafka concepts summary"},{"location":"kafka/readme/#apache-kafka","text":"In this article we are summarizing what Apache Kafka is and are grouping some references and notes we gathered during our different implementations and Kafka deployment within Kubernetes cluster. We are documenting how to deploy Kafka on IBM Cloud Private or deploying IBM Event Streams product . This content does not replace the excellent introduction every developer using Kafka should read.","title":"Apache Kafka"},{"location":"kafka/readme/#introduction","text":"Kafka is a distributed real time event streaming platform with the following key capabilities: Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far. It can handle hundreds of read and write operations per second from many producers and consumers Atomic broadcast, send a record once, every subscriber gets it once. Store streams of data records on disk and replicate within the distributed cluster for fault-tolerance. Persist data for a given time period before delete. Can grow elastically and transparently with no downtime. Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available.","title":"Introduction"},{"location":"kafka/readme/#use-cases","text":"The typical use cases where Kafka helps are: Centralize online data pipeline to decouple applications and microservices. Pub/sub messaging for cloud native application inter microservices. Aggregation of event coming from multiple producers. Monitor distributed applications to produce centralized feed of operational data. Logs collector from multiple services. Implement event soucing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between brokers within the cluster and cross availability zones if needed. Manage loosely coupled communication between microservices. (See this note where we present a way to support a service mesh solution using asynchronous event)","title":"Use cases"},{"location":"kafka/readme/#key-concepts","text":"The diagram below presents Kafka's key components:","title":"Key concepts"},{"location":"kafka/readme/#brokers","text":"Kafka runs as a cluster of one or more broker servers that can, in theory, span multiple data centers. It is really possible if the network latency between data centers is very low, at the 10ms or better, as there are a lot of communication between kafka brokers and between kafka and zookeepers. So the advice is to avoid cross data centers deployment. The Kafka cluster stores streams of records in topics . Topic is referenced by producer to send data to, and subscribed by consumers to get data. Data in topic is persisted to file systems for a retention time period (Defined at the topic level). The file system can be network based. In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated two times. In production, it is recommended to use at least five nodes to authorise planned failure and un-planned failure, and when doing replicas, use a replica factor equals to the number of brokers.","title":"Brokers"},{"location":"kafka/readme/#topics","text":"Topics represent end points to put or get records to. Each record consists of a key, a value (the data payload as byte array), and a timestamp. Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffers to keep records not yet transmitted to the server. Kafka store log data in its log.dir and topic maps to subdirectories in this log directory. Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups. Consumer performs asynchronous pull to the connected brokers via the subscription to a topic. The figure below illustrates one topic having multiple partitions, replicated within the broker cluster:","title":"Topics"},{"location":"kafka/readme/#partitions","text":"Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages, Kafka uses partitions. Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use. Kafka tolerates up to N-1 server failure without losing any messages. N is the replication factor for a given partition. Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log. Consumers see messages in the order they are stored in the log. Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... You can have 2000 partitions per broker. Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are inmutable. When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from. Brokers keep offset information in an hidden topic. Partitions guarantee that data with the same keys will be sent to the same consumer and in order.","title":"Partitions"},{"location":"kafka/readme/#replication","text":"Each partition can be replicated accross a number of server. The replication factor is capted by the number of brokers. Partitions have one leader and zero or more followers. The leader manages all the read and write requests for the partition. Leader is also responsible to track the \"in sync\" replicas. The followers replicate the leader content. If a leader fails, followers elect a new one. When a producer sends message, it can control how to get the response from the committed message: wait for all replicas to succeed, wait for one acknowledge, fire and forget. Consumers receive only committed messages.","title":"Replication"},{"location":"kafka/readme/#zookeeper","text":"Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup. Kafka does not keep state regarding consumers and producers. Depends on kafka version, offsets are maintained in Zookeeper or in Kafka : newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages. Access Controls are saved in Zookeeper","title":"Zookeeper"},{"location":"kafka/readme/#consumer-group","text":"This is the way to group consumers so the processing of event is parallelized. The number of consumers in a group is the same as the number of partition defined in a topic. We are detailing consumer group implementation in this note","title":"Consumer group"},{"location":"kafka/readme/#architecture","text":"See this separate note addresses high availability and kubernetes deployment.","title":"Architecture"},{"location":"kafka/readme/#solution-considerations","text":"There are a set of design considerations to assess for each Kafka solution:","title":"Solution considerations"},{"location":"kafka/readme/#topics_1","text":"Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node. What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic. The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions. When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together. Other best practices: When event order is important use the same topic and use the entity unique identifier as partition key. When two entities are related together by containment relationship then they can be in the same topic. Different entities are separated to different topics. It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics. Clearly define the partition key as it could be an compound key based on multiple entities. With Kafka stream, state store or KTable, you should separate the changelog topic from the others.","title":"Topics"},{"location":"kafka/readme/#producers","text":"When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size Assess once to exactly once delivery requirement. Look at idempotent producer. See implementation considerations discussion","title":"Producers"},{"location":"kafka/readme/#consumers","text":"From the consumer point of view a set of items need to be addressed during design phase: Do you need to group consumers for parallel consumption of events? What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?. How to persist consumer committed position? (the last offset that has been stored securely) Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code. Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records? Do the consumer needs to perform joins, aggregations between multiple partitions? See implementation considerations discussion See also the compendium note for more readings.","title":"Consumers"},{"location":"methodology/ddd/","text":"This section describes how to apply domain driven design with event based application and describe the high level steps which uses output from the event storming session and derives a set of micro services design specifications. Event storming is part of the domain driven design methodology. And domain-driven design was deeply describe in Eric Evans's \"Domain Driven Design: Tackling Complexity in the Heart of Software\" book from 2004. At IBM we also summarized the concepts needed for implementing microservice in Kyle Brown's article . The goals for the design step are: To support highly modular cloud native microservices. To adopt event coupled microservices - facilitating independent modification and evolution of each microservice separately. To allow applying event-driven patterns such as event sourcing, CQRS and SAGA to address some of the challenges of microservice implementation: data consitency, transaction cross domain, and complex queries between aggregates managed by different services. Starting materials generated during the Event Storming workshop We make use of the following materials generated during Event Storming and analysis workshop: Event Sequence flow. Events \u2013 business description. Critical events. Aggregates and services: Users \u2013 role based user stories. Commands. Event linkages. Policies. Event prediction and probability flows. The derivation of these material was described in: the event storming introduction . Here is an example of starting material illustrating the beginning of the process for the Reefer shipping business process: From there we complement the analysis and start the domain design. Steps in the design process Step 1: Assess domain and subdomains Domain is what an organization does, and it includes the how to perform its operations. It is composed of sub domains. A Core Domain is a part of the business Domain that is of primary importance to the success of the organization, the organization needs to excel at it. During the event storming analysis, you define the domain and groups a set of subdomains like orders, invoice, customer, ... and external systems. Here is an example of such domain and subdomains: We have three core subdomains and the rest are supports. Step 2: Defined the potential application At the high level, when doing the analysis, you should have some insight decision of the top level application to develop, but it is also important to also list other systems to interact with. A classic system context diagram is a nice tool to represent that. The EDA reference implementation solution includes such system context diagram . Each interface needs to be documented using an efficient tools of interface characteristics . full set of interface characteristics is shown below: FUNCTIONAL DEFINITION Principal data objects Operation/function Read or change Request/response objects TECHNICAL INTERFACE Transport Protocol Data format INTERACTION TYPE Request-response or fire-forget Thread-blocking or asynchronous Batch or individual Message size PERFORMANCE Response times Throughput Volumes Concurrency INTEGRITY Validation Transactionality Statefulness Event sequence Idempotence SECURITY Identity/authentication Authorization Data ownership Privacy RELIABILITY Availability Delivery assurance ERROR HANDLING Error management capabilities Known exception conditions Unexpected error presentation Step 3: Define the ubiquitous language This is where the work from the event storming and the relation with the business experts should help. Domain experts use their jargon while technical team members have their own language tuned for discussing the domain in terms of design. The terminology of day-to-day discussions is disconnected from the terminology embedded in the code, so the ubiquitous language helps to allign knowledge with design elements, code and tests. (Think about EJB, JPA entity, all the JEE design jargon versus ShippingOrder, order provisioning, fullfillment... ) The vocabulary of that ubiquitous language includes the class names and prominent operation names. The language includes terms to discuss rules that have been made explicit in the model. Be sure to commit the team to exercising that language relentlessly in all communication within the business and in the code. Use the same language in diagrams, writing, and especially speech. Play with the model as you talk about the system. Describe scenarios out loud using the elements and interactions of the model, combining concepts in ways allowed by the model. Find easier ways to say what you need to say, and then take those new ideas back down to the diagrams and code. Entities and Value Objects Entities are part of the ubiquitous language, and represent business concepts that can be uniquely identified by some atributes. They have a life cycle that is important to model. Value Object also represent things in the domain but without identity, and they are frequently transient, created for an operation and then discarded. Some time in a certain context a value object could become an entity. As an example, an Address will be most of the time a value object, excepts in a mapping app or in an agenda app. Below is an example of entities (Customer and Shipping Order) and value objects (delivery history and delivery specification): Aggregate boundaries An aggregate is a cluster of associated objects that we treat as a unit for the purpose of data changes. An entity is most likely and aggregate and every things related to it define its boundaries. Bounded Contexts Bounded Context explicitly defines the boundaries of your model. This concept is critical in large software projects. A Bounded Context sets the limits around what a specific team works on and helps them to define their own vocabulary within that particular context. When you define a bounded context, you define who uses it, how they use it, where it applies within a larger application context, and what it consists of in terms of things like Swagger documentation and code repositories. Within a business context every use of a given domain term, phrase, or sentence, the Ubiquitous Language inside the boundary has a specific contextual meaning. So order context is a boundary context and groups order, ordered product type, pickup and shipping addresses, delivery specifications, delivery history. Repositories Repository represents the infrastructure service to persist the root aggregate during its full life cycle. Client applications request objects from the repository using query methods that select objects based on criteria specified by the client, typically the value of certain attributes. Event linked microservices design - structure A complete event driven microservices specification (the target of this design step) includes specifications of the following: Event Topics Used to configure the Event Backbone Mapped to the life cycle of the root entity Topics can be chained to address different consumer semantic Single partition for keeping order and support exactly once delivery Event types within each event topic Microservices: They may be finer grained than aggregates or mapped to aggregate boundaries. They may separate query and command; possibly multiple queries. They could define demonstration control and serve main User Interface. Reference the related Entities and value objects within each microservice. Define APIs ( Synchronous or asynchronous) using standards like openAPI. Could be done bottom up from the code, as most of TDD implementation will lead to. Topics and events Subscribed to. Events published / emitted. List of end to end interactions: List of logic segments per microservice Recovery processing, scaling: We expect this to be highly patterned and template driven not requiring example-specific design. Step 4: Define modules Each aggregate will be implemented as some composition of: (1) a command microservice managing state changes to the entities in this aggregate (2) possibly one or more separate (CQRS) query services providing internal or external API query capabilities (3) additional simulation, predictive analytics or User Interface microservices The command microservice will be built around a collection of active entites for the aggregate, keyed by some primary key. The separation of each aggregate into specific component microservices as outlined above, will be a complete list of microservices for the build / sprint. Identify the data collections, and collection organization (keying structure) in each command and query microservice for this build. Step 5: Limit the context and scope for this particular build / sprint We assume that we are developing a particular build for a sprint within some agile development approach, deferring additional functions and complexity to later sprints: Working from the initial list of aggregates, select which aggregates will be included in this build For each aggregate the possible choices are: to completely skip and workaround the aggregate in this build. to include a full lifecycle implementation of the aggregate to provide a simplified lifecycle implementation - typically a table of entities is initialized at start up, and state changes to existing entities are tracked Determine whether there are simulation services or predictive analytics service to be included in the build Identify the external query APIs and command APIs which this build should support Create entity lifecycle diagrams for entites having a full lifecycle implementation in this build / sprint. Step 6: Generate microservice interaction diagrams for the build The diagram will show API calls initiating state change. They should map the commands discovered during the event storming. It shows for each interaction whether this is a synchronous API calls or an asynchronous event interaction via the event backbone. The diagram labels each specific event interaction between microservices trigerring a state change. Typically queries are synchronous API calls since the caller cannot usefully proceeed until a result is returned. From these, we can extract: a complete list of event types on each event backbone topic, with information passed on each event type. the complete list of \u201clogic segments\u201d for each microservice processing action in response to an API call or initiating event. When, at the next level of detail, the individual fields in each event are specified and typed, the CloudEvents standard may be used as a starting point. Step 7: Specify recovery approach in case a microservice fails If a microservice fails it will need to recover its internal state by reloading data from one or more topics, from the latest committed read. In general, command and query microservices will have a standard pattern for doing this. Any custom event filtering and service specific logic should be specified. Concepts and rationale underlying the design approach What is the difference between event information stored in the event backbone and state data stored in the microservices? The event information stored persistently in the event backbone is organized by topic and, within each topic, entirely by event time-of-occurrence. While the state information in a microservice is a list (collection) of all currently active entities of the owning aggregate (e.g. all orders, all voyages etc) and the current state of each such entity. The entity records are keyed by primary key, like an OrderID. While implementing microservice using event sourcing, CQRS, the persisted entity records are complementary to the historically organized information in the event backbone. When is it acceptable to be using synchronous interactions between services instead of asynchronous event interacts through the event backbone? For non-state-changing queries, for which the response is always instantaneously available a synchronous query call may be acceptable and will provide a simpler more understandable interface. Any processing which can be though of as being triggered by some state change in another aggregate should be modelled with an asynchronous event, because as the solution evolves other new microservices may also need to be aware of such event. We do not want to have to go back and change logic existing service where this event originated to have that microservice actively report the event to all potential consumers. How do we save microservices from having to maintain data collections with complex secondary indexing for which eventual consistency will be hard to implement? Each command microservice should do all its state changing updates using the primary key lookup only for its entities. Each asynchronous event interaction between microservices should carry primary entityIds ( orderID, VoyageID, shipID) for any entities associated with the interaction. Each query which might require speciaoized secondary indexing to respond to queries can be implemented in a separate CQRS query service which subscribes to events to do all internal updating and receives events from the event backbone in a ( Consistent) eventually correct order. This allows for recovery of any failed service by rebuilding it in \"eventually correct\" order. Applied DDD for the reference implementation See this note for details.","title":"Domain driven design for event based solution"},{"location":"methodology/ddd/#starting-materials-generated-during-the-event-storming-workshop","text":"We make use of the following materials generated during Event Storming and analysis workshop: Event Sequence flow. Events \u2013 business description. Critical events. Aggregates and services: Users \u2013 role based user stories. Commands. Event linkages. Policies. Event prediction and probability flows. The derivation of these material was described in: the event storming introduction . Here is an example of starting material illustrating the beginning of the process for the Reefer shipping business process: From there we complement the analysis and start the domain design.","title":"Starting materials generated during the Event Storming workshop"},{"location":"methodology/ddd/#steps-in-the-design-process","text":"","title":"Steps in the design process"},{"location":"methodology/ddd/#step-1-assess-domain-and-subdomains","text":"Domain is what an organization does, and it includes the how to perform its operations. It is composed of sub domains. A Core Domain is a part of the business Domain that is of primary importance to the success of the organization, the organization needs to excel at it. During the event storming analysis, you define the domain and groups a set of subdomains like orders, invoice, customer, ... and external systems. Here is an example of such domain and subdomains: We have three core subdomains and the rest are supports.","title":"Step 1: Assess domain and subdomains"},{"location":"methodology/ddd/#step-2-defined-the-potential-application","text":"At the high level, when doing the analysis, you should have some insight decision of the top level application to develop, but it is also important to also list other systems to interact with. A classic system context diagram is a nice tool to represent that. The EDA reference implementation solution includes such system context diagram . Each interface needs to be documented using an efficient tools of interface characteristics . full set of interface characteristics is shown below: FUNCTIONAL DEFINITION Principal data objects Operation/function Read or change Request/response objects TECHNICAL INTERFACE Transport Protocol Data format INTERACTION TYPE Request-response or fire-forget Thread-blocking or asynchronous Batch or individual Message size PERFORMANCE Response times Throughput Volumes Concurrency INTEGRITY Validation Transactionality Statefulness Event sequence Idempotence SECURITY Identity/authentication Authorization Data ownership Privacy RELIABILITY Availability Delivery assurance ERROR HANDLING Error management capabilities Known exception conditions Unexpected error presentation","title":"Step 2: Defined the potential application"},{"location":"methodology/ddd/#step-3-define-the-ubiquitous-language","text":"This is where the work from the event storming and the relation with the business experts should help. Domain experts use their jargon while technical team members have their own language tuned for discussing the domain in terms of design. The terminology of day-to-day discussions is disconnected from the terminology embedded in the code, so the ubiquitous language helps to allign knowledge with design elements, code and tests. (Think about EJB, JPA entity, all the JEE design jargon versus ShippingOrder, order provisioning, fullfillment... ) The vocabulary of that ubiquitous language includes the class names and prominent operation names. The language includes terms to discuss rules that have been made explicit in the model. Be sure to commit the team to exercising that language relentlessly in all communication within the business and in the code. Use the same language in diagrams, writing, and especially speech. Play with the model as you talk about the system. Describe scenarios out loud using the elements and interactions of the model, combining concepts in ways allowed by the model. Find easier ways to say what you need to say, and then take those new ideas back down to the diagrams and code.","title":"Step 3: Define the ubiquitous language"},{"location":"methodology/ddd/#entities-and-value-objects","text":"Entities are part of the ubiquitous language, and represent business concepts that can be uniquely identified by some atributes. They have a life cycle that is important to model. Value Object also represent things in the domain but without identity, and they are frequently transient, created for an operation and then discarded. Some time in a certain context a value object could become an entity. As an example, an Address will be most of the time a value object, excepts in a mapping app or in an agenda app. Below is an example of entities (Customer and Shipping Order) and value objects (delivery history and delivery specification):","title":"Entities and Value Objects"},{"location":"methodology/ddd/#aggregate-boundaries","text":"An aggregate is a cluster of associated objects that we treat as a unit for the purpose of data changes. An entity is most likely and aggregate and every things related to it define its boundaries.","title":"Aggregate boundaries"},{"location":"methodology/ddd/#bounded-contexts","text":"Bounded Context explicitly defines the boundaries of your model. This concept is critical in large software projects. A Bounded Context sets the limits around what a specific team works on and helps them to define their own vocabulary within that particular context. When you define a bounded context, you define who uses it, how they use it, where it applies within a larger application context, and what it consists of in terms of things like Swagger documentation and code repositories. Within a business context every use of a given domain term, phrase, or sentence, the Ubiquitous Language inside the boundary has a specific contextual meaning. So order context is a boundary context and groups order, ordered product type, pickup and shipping addresses, delivery specifications, delivery history.","title":"Bounded Contexts"},{"location":"methodology/ddd/#repositories","text":"Repository represents the infrastructure service to persist the root aggregate during its full life cycle. Client applications request objects from the repository using query methods that select objects based on criteria specified by the client, typically the value of certain attributes.","title":"Repositories"},{"location":"methodology/ddd/#event-linked-microservices-design-structure","text":"A complete event driven microservices specification (the target of this design step) includes specifications of the following: Event Topics Used to configure the Event Backbone Mapped to the life cycle of the root entity Topics can be chained to address different consumer semantic Single partition for keeping order and support exactly once delivery Event types within each event topic Microservices: They may be finer grained than aggregates or mapped to aggregate boundaries. They may separate query and command; possibly multiple queries. They could define demonstration control and serve main User Interface. Reference the related Entities and value objects within each microservice. Define APIs ( Synchronous or asynchronous) using standards like openAPI. Could be done bottom up from the code, as most of TDD implementation will lead to. Topics and events Subscribed to. Events published / emitted. List of end to end interactions: List of logic segments per microservice Recovery processing, scaling: We expect this to be highly patterned and template driven not requiring example-specific design.","title":"Event linked microservices design - structure"},{"location":"methodology/ddd/#step-4-define-modules","text":"Each aggregate will be implemented as some composition of: (1) a command microservice managing state changes to the entities in this aggregate (2) possibly one or more separate (CQRS) query services providing internal or external API query capabilities (3) additional simulation, predictive analytics or User Interface microservices The command microservice will be built around a collection of active entites for the aggregate, keyed by some primary key. The separation of each aggregate into specific component microservices as outlined above, will be a complete list of microservices for the build / sprint. Identify the data collections, and collection organization (keying structure) in each command and query microservice for this build.","title":"Step 4: Define modules"},{"location":"methodology/ddd/#step-5-limit-the-context-and-scope-for-this-particular-build-sprint","text":"We assume that we are developing a particular build for a sprint within some agile development approach, deferring additional functions and complexity to later sprints: Working from the initial list of aggregates, select which aggregates will be included in this build For each aggregate the possible choices are: to completely skip and workaround the aggregate in this build. to include a full lifecycle implementation of the aggregate to provide a simplified lifecycle implementation - typically a table of entities is initialized at start up, and state changes to existing entities are tracked Determine whether there are simulation services or predictive analytics service to be included in the build Identify the external query APIs and command APIs which this build should support Create entity lifecycle diagrams for entites having a full lifecycle implementation in this build / sprint.","title":"Step 5: Limit the context and scope for this particular build / sprint"},{"location":"methodology/ddd/#step-6-generate-microservice-interaction-diagrams-for-the-build","text":"The diagram will show API calls initiating state change. They should map the commands discovered during the event storming. It shows for each interaction whether this is a synchronous API calls or an asynchronous event interaction via the event backbone. The diagram labels each specific event interaction between microservices trigerring a state change. Typically queries are synchronous API calls since the caller cannot usefully proceeed until a result is returned. From these, we can extract: a complete list of event types on each event backbone topic, with information passed on each event type. the complete list of \u201clogic segments\u201d for each microservice processing action in response to an API call or initiating event. When, at the next level of detail, the individual fields in each event are specified and typed, the CloudEvents standard may be used as a starting point.","title":"Step 6: Generate microservice interaction diagrams for the build"},{"location":"methodology/ddd/#step-7-specify-recovery-approach-in-case-a-microservice-fails","text":"If a microservice fails it will need to recover its internal state by reloading data from one or more topics, from the latest committed read. In general, command and query microservices will have a standard pattern for doing this. Any custom event filtering and service specific logic should be specified.","title":"Step 7: Specify recovery approach in case a microservice fails"},{"location":"methodology/ddd/#concepts-and-rationale-underlying-the-design-approach","text":"What is the difference between event information stored in the event backbone and state data stored in the microservices? The event information stored persistently in the event backbone is organized by topic and, within each topic, entirely by event time-of-occurrence. While the state information in a microservice is a list (collection) of all currently active entities of the owning aggregate (e.g. all orders, all voyages etc) and the current state of each such entity. The entity records are keyed by primary key, like an OrderID. While implementing microservice using event sourcing, CQRS, the persisted entity records are complementary to the historically organized information in the event backbone. When is it acceptable to be using synchronous interactions between services instead of asynchronous event interacts through the event backbone? For non-state-changing queries, for which the response is always instantaneously available a synchronous query call may be acceptable and will provide a simpler more understandable interface. Any processing which can be though of as being triggered by some state change in another aggregate should be modelled with an asynchronous event, because as the solution evolves other new microservices may also need to be aware of such event. We do not want to have to go back and change logic existing service where this event originated to have that microservice actively report the event to all potential consumers. How do we save microservices from having to maintain data collections with complex secondary indexing for which eventual consistency will be hard to implement? Each command microservice should do all its state changing updates using the primary key lookup only for its entities. Each asynchronous event interaction between microservices should carry primary entityIds ( orderID, VoyageID, shipID) for any entities associated with the interaction. Each query which might require speciaoized secondary indexing to respond to queries can be implemented in a separate CQRS query service which subscribes to events to do all internal updating and receives events from the event backbone in a ( Consistent) eventually correct order. This allows for recovery of any failed service by rebuilding it in \"eventually correct\" order.","title":"Concepts and rationale underlying the design approach"},{"location":"methodology/ddd/#applied-ddd-for-the-reference-implementation","text":"See this note for details.","title":"Applied DDD for the reference implementation"},{"location":"methodology/eventstorming/","text":"Event driven solution implementation methodology In this article we are presenting an end to end set of activities to run a successful Minimum Viable Product for an event-driven solution using cloud native microservices and event backbone as the core technology approach. The discovery and analysis of the MVP scope starts with an event storming workshop where designer, architect work hand to hand with business users and domain subject matter experts. From the different outcomes of the workshop, the development team starts to outline components, microservices, business entity life cycle, etc... in a short design iteration . The scope is well defined Epics, Hill and user stories defined, at least for the first iterations, and the MVP can start. Event Storming workshop Event storming is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. A domain event is something meaningful to the experts that happened in the domain. The workshop focuses on communication between product owner, domain experts and developers. The event storming method was introduced and publicized by Alberto Brandolini in \"Introducing event storming book\" . This approach is recognized in the Domain Driven Design (DDD) community as a technique for rapid capture of a solution design and improved team understanding of the domain. Domain represents some area of the business that has the analysis focus. This article outlines the method and describes refinements and extensions that are useful in designs for an event-driven architecture. This extended approach adds an insight storming step to identify and capture value adding predictive insights about possible future events. The predictive insights are generated by using data science analysis, data models, artificial intelligence (AI), or machine learning (ML). This article describes in general terms all the steps to run an event storming workshop. The output of an actual workshop done on a sample problem - a world wide container shipment, is further detailed in the container shipment analysis example . Conducting the event and insight storming workshop Before conducting an event storming workshop, complete a Design Thinking Workshop in which Personas and Empathy Maps are developed and business pains and goals are defined. The event storming workshop adds more specific design on the events occuring at each step of the process, natural contexts for microservices and predictive insights to guide operation of the system. With this approach, a team that includes business owners and stakeholders can define a Minimal Viable Prototype (MVP) design for the solution. The resulting design is organized as a collection of loosely coupled microservices linked through an event-driven architecture and one or more event backbones. This style of design can be deployed into multicloud execution environments and allows for scaling and agile deployment. Preparations for the event storming workshop include the following steps: Get a room big enough to hold at least 6 to 8 persons and with enough wall space on which to stick big paper sheets: you will need a lot of wall space to define the models. Obtain green, orange, blue, and red square sticky notes, black sharpie pens and blue painter's tape. Discourage the use of open laptops during the meeting. Limit the number of chairs so that the team stays focused and connected and conversation flows easily. Concepts Many of the concepts addressed during the event storming workshop are defined in the Domain Driven Design approach. The following diagrams present the elements used during the analysis. The first diagram shows the initial set of concepts that are used in the process. Domain events are also named business events . An event is some action or happening which occurred in the system at a specific time in the past. The first step in the event storming process consists of these actions: Identifying all relevant events in the domain and specific process being analyzed, Writing a very short description of each event on a \"sticky\" note and placing all the event \"sticky\" notes in sequence on a timeline. The act of writing event descriptions often results in questions to be resolved later, or discussions about definitions that need to be recorded to ensure that everyone agrees on basic domain concepts. A timeline of domain events is the critical output of the first step in the event storming process. The timeline gives everyone a common understanding of when events take place in relation to each other. You still need to be able to take this initial level of understanding and move it towards an implementation. In making that step, you must expand your thinking to encompass the idea of a command, which is the action that kicks off the processing that triggers an event. As part of understanding the role of the command, you will also want to know who invokes a command (actors) and what information is needed to allow the command to be executed. This diagram show how those analysis elements are linked together: One-View Figure. Actors consume data by using a user interface and use the UI to interact with the system via commands. Actors could also be replace by articial intelligent agents. Commands are the result of some user decision or policy, and act on relevant data which are part of a Read model in the CQRS pattern. Policies (represented by lilac stickies) are reactive logics that take place after an event occurs, and trigger other commands. Policies always start with the phrase \"whenever...\". They can be a manual step a human follows, such as a documented procedure or guidance, or they may be automated. When applying the Agile Business Rule Development methodology it will be mapped to a Decision within the Decision Model Notation . External systems produce events. Data can be presented to users in a user interface or modified by the system. Events can be created by commands or by external systems including IOT devices. They can be triggerred by the processing of other events or by some period of elapsed time. When an event is repeated or occurs regularly on a schedule, draw a clock or calendar icon in the corner of the sticky note for that event. As the events are identified and sequenced into a time line, you might find multiple independent subsequences that are not directly coupled to each other and that represent different perspectives of the system, but occur in overlapped periods of time. These parallel event streams can be addressed by putting them into separate swimlanes delineated by using horizontal blue painter's tape. As the events are organized into a timeline, possibly with swim lanes, you can identify pivotal events. Pivotal events indicate major changes in the domain and often form the boundary between one phase of the system and another. Pivotal events will typically separate (a bounded context in DDD terms). Pivotal events are identified with vertical blue painters tape (crossing all the swimlanes). An example of a section of a completed event time line with pivotal events and swimlanes is shown below. Conducting the workshop The goal of the workshop is to better understand the business problem to address with a future application. But the approach can also apply to finding solutions to bottlenecks or other issues in existing applications. The workshop helps the team to understand the big picture of the solution by building a timeline of domain events as they occur during the business process life span. During the workshop, avoid documenting processing steps. The event storming method is not trying to specify a particular implementation. Instead, the focus in initial stages of the workshop is on identifying and sequencing the events that occur in the solution. The event timeline is a useful representation of the overall steps, communicating what must happen while remaining open to many possible implementation approaches. Step 1: Domain events discovery Begin by writing each domain event on an orange sticky note with a few words and a verb in a past tense. Describe What's happened . At first just \"storm\" the events by having each domain expert generate an individual lists of domain events. You might not need to initially place the events on the ordered timeline as they write them. The events must be worded in a way that is meaningful to the domain experts and business stakeholder. You are explaining what happens in business terms, not what happens inside the implementation of the system. You don't need to describe all the events in your domain, but you must cover the process that you are interested in exploring from end to end. Therefore, make sure that you identify the start and end events and place them on the timeline at the beginning and end of the wall covered with paper. Place the other events that you identified between these two endpoints in the closest approximation that the team can agree to a sequential order. Don\u2019t worry about overlaps at this point; overlaps are addressed later. Step 2: Tell the story In this step, you retell the story by talking about how to relate events to particular personas. A member of the team (often the facilitator, but others can do this as well) acts this out by taking on the perspective of a persona in the domain, such as a \"manufacturer\" who wants to ship a widget to a customer, and asking which events follow which other events. Start at the beginning of that persona's interaction and ask \"what happens next?\". Pick up and rearrange the events that the team storms. If you discover events that are duplicates, take those off the board. If events are in the wrong order, move them into the right order. When some parts are unclear, add questions or comments by using the red stickies.. Red stickies indicate that the team needs to follow up and clarify issues later. Likewise you want to use this time to document assumptions on the definition stickies. This is also a good time to rephrase events as you proceed through the story. Sometimes you need to rephrase an event description by putting the verbing in past tense, or adjusting the terms that are used to relate clearly to other identified events. In this step you focus on the mainline \"happy\" end-to-end path to avoid getting bogged down in details of exceptions and error handling. Exceptions can be added later Step 3: Find the Boundaries The next step of this part of the process is to find the boundaries of your system by looking at the events. Two types of boundaries can emerge; the first type of boundary is a time boundary. Often specific key \"pivotal events\" indicate a change from one aspect of a system to another. This can happen at a hand-off from one persona to another, but it can also happen at a change of geographical, legal, or other type of boundary. If you notice that the terms that are used on the event stickies change at these boundaries, you are seeing a \"bounded context\" in Domain Driven Design terms. Highlight pivotal events by putting up blue painter\u2019s tape vertically behind the event. The second type of boundary is a subject boundary. You can detect a subject boundary by looking for the following conditions: You have multiple simultaneous series of events that only come together at a later time. You see the same terms being used in the event descriptions for a particular series of events. You can \u201cread\u201d a series of events from the point of view of a different persona when you are replaying them. You can delineate these different sets of simultaneous event streams by applying blue painter\u2019s tape horizontally, dividing the board into different swim lanes. Below is an example of a set of ordered domain events with pivotal events and subject swim lanes indicated. This example comes from applying event storming to the domain of container shipping process and is discussed in more detail in the container shipment analysis example . When the reefer container is plugged to the Vessel, it starts to emit telemetries, we change context. Step 4: Locate the Commands In this step you shift from analysis of the domain to the first stages of system design. Up until this point, you are simply trying to understand how the events in the domain relate to one another - this is why the participation of domain experts is so critical. However, to build a system that implements the business process that you are interested in, you have to move on to the question of how these events come into being. Commands are the most common mechanism by which events are created. The key to finding commands is to ask the question: \"Why did this event occur?\". In this step, the focus of the process moves to the sequence of actions that lead to events. Your goal is to find the causes for which the events record the effects. Expected event trigger types are: A human operator makes a decision and issues a command Some external system or sensor provides a stimulus An event results from some policy - typically automated processing of a precursor event The completion of some determined period of elapsed time. The triggering command is identified in a blue (sticky) note. Command may become a microservice operation exposed via API. The human persona issuing the command is identified and shown in a yellow note. Some events may be created by applying business policies. The diagram below illustrates the manufacturer actor using the place a shipment order command to create a shipment order placed event, as well as . It is possible to chain events and commands as presented in the \"one view\" figure above in the concepts section. Step 5: Describe the Data You can't truly define a command without understanding the data that is needed for the command to execute in order to produce the event. You can identify several types of data during this step. First, users (personas) need data from the user interface in order to make decisions before executing a command. That data forms part of the read model in a CQRS implementation. For each command and event pair, you add a data description of the expected attributes and data elements needed to take such a decision. Here is a simple example for a shipment order placed event created from a place a shipment order action . Another important part of the process that becomes more fully fleshed out at this step is the description of policies that can trigger the generation of an event from a previous event (or set of events). Assess if the data element is a main business entity, uniquely identified by a key, supported by multiple commands. It has a life span over the business process. This will lead to develop an entity life cycle analysis. This first level of data definition helps to assess the microservice scope and responsibility as you start to see commonalities emerge from the data used among several related events. Those concepts become more obvious in the next step. Step 6: Identify the Aggregates In DDD, entities and value objects can exist independently, but often, the relations are such that an entity or a value object has no value without its context. Aggregates provide that context by being those \"roots\" that comprise one or more entities and value objects that are linked together through a lifecycle. The following diagram illustrates a detail example of aggregates for a fresh product shipment over sea. In event storming, we may not be able to get this level of detail during the first workshop, but aggregates emerge through the process by grouping events and commands that are related together. This grouping not only consists of related data (entities and value objects) but also related actions (commands) that are connected by the lifecycle of that aggregate. Aggregates ultimately suggest microservice boundaries. In the container shipment example, you can see that you can group several commands and event pairs (with their associated data) together that are related through the lifecycle of an order for shipping. Step 7: Define Bounded Context In this step, you define terms and concepts with a clear meaning valid in a clear boundary and you define the context within which a model applies. (The term definition can change outside of the business unit for which an application is developed). The following items may be addressed: Which team owns the model? Which part of the model transit between team organization? What are the different code bases foreseen we need to implement? What are the different data schema ? (database or json or xml schemas) Here is an example of bounded context that will, most likely, lead to a microservice: Keep the model strictly consistent within these bounds. Step 8: Looking forward with insight storming In event atorming for Event Driven Architecture (EDA) solutions it is helpful to include an additional method step at this point identifying useful predictive analytics insights. Insights storming extends the basic methodology by looking forward and considering what if you could know in advance that an event is going to occur. How would this change your actions, and what would you do in advance of that event actually happening? You can think of insight storming as extending the analysis to Derived Events . Rather than being the factual recording of a past event, a derived event is a forward-looking or predictive event, that is, \"this event is probably going to happen at some time in the next n hours\u201d. By using this forward-looking insight combined with the known business data from earlier events, human actors and event triggering policies can make better decisions about how to react to new events as they occur. Insight storming amounts to asking workshop participants the question: \"What data would be helpful at each event trigger to assist the human user or automated event triggering policy make the best possible decision of how and when to act?\" An important motivation that drives the use of an event-driven architecture is that it simplifies design and realization of highly responsive systems that react immediately and intelligently, that is, in a personalized and context-aware way, and optimally to new events as they occur. This immediately suggests that predictive analytics and models to generate predictive insights have an important role to play. Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the likely properties of those events. These probabilistic statements are typicaly generated by using models created by data scientists or using AI or ML. Correlating or joining independently gathered sources of information can also generate important predictive insights or be input to predictive analytic models. Business owners and stakeholders in the event storming workshop can offer good intuitions in several areas: Which probabilistic insights are likely to lead to improved or optimal decision making and action? The action could take the form of an immediate response to an event when it occurs. The action could be proactive behavior to avoid an undesirable event. What combined sources of information are likely to help create a model to predict this insight? With basic event storming, you look backwards at each event because an event is something that has already happened. When you identify data needed for an actor or policy to decide when and how to issue a command, there is a tendency to restrict consideration to properties of earlier known and captured business events. In insight storming you extend the approach to explicitly look forward and consider what is the probability that a particular event will occur at some future time and what would be its expected property values? How would this change the best action to take when and if this event occurs? Is there action we can take now proactively in advance of an expected undesirable event to prevent it happening or mitigate the consequences? The insight method step amounts to getting workshop participants to identify derived events and the data sources needed for the models that generate them. Adding an insight storming step using the questions above into the workshop will improve decision making and proactive behavior in the resulting design. Insights can be published into a bus and subscribed to by any decision step guidance. By identifying derived events, you can integrate analytic models and machine learning into the designed solution. Event and derived event feeds can be processed, filtered, joined, aggregated, modeled and scored to create valuable predictive insights. Use the following new notations for the insight step: Pale blue stickies for derived events. Parallelogram shape to show when events and derived events are combined to enable deeper insight models and predictions. Identify predictive insights as early as possible in the development life cycle. The best opportunity to do this is to add this step to the event storming workshop. The two diagrams below show the results of the insight storming step for the use case of container shipment analysis. The first diagram captures insights and associated linkages for each refrigerated container, identifying when automated changes to the thermostat settings can be made, when unit maintenance should be scheduled and when the container contents must be considered spoiled. The second diagram captures insights that could trigger recommendations to adjust ship course or speed in response to expected severe weather forcasts for the route ahead or predicted congestion and expected docking and unloading delays at the next port of call. Design iteration Attention we are not proposing to apply a waterfall approach, but before starting the deeper implementation with iterations, we want to spend sometime to define in more details what we want to build, how to organize the CI/CD projects and pipeline, select the development, test and product plaform, and define epics, user stories, components, microservices... This iteration can take from few hours to a week, depending on the expected MVP goals. For an event-driven solution a MVP for a single application should not take more than 3 to 4 iterations. Event storming to user stories and epics In agile methodology, creating user stories or epics is one of the most important elements in project management. The commands and policies related to events can be easily described as user stories, because commands and decisions are done by actors. The actor could be a system as well. For the data you must model the \"Create, Read, Update, Delete\" operations as user stories, mostly supported by a system actor. An event is the result or outcome of a user story. Events can be added as part of the acceptance criteria of the user stories to verify that the event really occurs. Applying to the container shipment use case The K Container Shipment use case demonstrates an implementation solution to validate the event-driven architecture. The container shipment analysis example , shows event storming and design thinking main artifacts, including artifacts for the monitoring of refrigerated containers. Some practice notes you can apply event storming at different level: for example at the beginning of a project to understand the high level process at stake. with a big group of people, you will stay at the high level. But it can be used to model a lower level microservice, to assess event consumed and produced. Further Readings Introduction to event storming from Alberto Brandolini Event Storming Guide Wikipedia Domain Driven Design Eric Evans: \"Domain Driven Design - Tacking complexity in the heart of software\" Domain drive design with event storming introduction video Patterns related to Domain Driven Design by Martin Fowler Kyle Brown - IBM - Apply Domain-Driven Design to microservices architecture Applying DDD and event storming for event-driven microservice implementation from our own work","title":"Event Storming"},{"location":"methodology/eventstorming/#event-driven-solution-implementation-methodology","text":"In this article we are presenting an end to end set of activities to run a successful Minimum Viable Product for an event-driven solution using cloud native microservices and event backbone as the core technology approach. The discovery and analysis of the MVP scope starts with an event storming workshop where designer, architect work hand to hand with business users and domain subject matter experts. From the different outcomes of the workshop, the development team starts to outline components, microservices, business entity life cycle, etc... in a short design iteration . The scope is well defined Epics, Hill and user stories defined, at least for the first iterations, and the MVP can start.","title":"Event driven solution implementation methodology"},{"location":"methodology/eventstorming/#event-storming-workshop","text":"Event storming is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. A domain event is something meaningful to the experts that happened in the domain. The workshop focuses on communication between product owner, domain experts and developers. The event storming method was introduced and publicized by Alberto Brandolini in \"Introducing event storming book\" . This approach is recognized in the Domain Driven Design (DDD) community as a technique for rapid capture of a solution design and improved team understanding of the domain. Domain represents some area of the business that has the analysis focus. This article outlines the method and describes refinements and extensions that are useful in designs for an event-driven architecture. This extended approach adds an insight storming step to identify and capture value adding predictive insights about possible future events. The predictive insights are generated by using data science analysis, data models, artificial intelligence (AI), or machine learning (ML). This article describes in general terms all the steps to run an event storming workshop. The output of an actual workshop done on a sample problem - a world wide container shipment, is further detailed in the container shipment analysis example .","title":"Event Storming workshop"},{"location":"methodology/eventstorming/#conducting-the-event-and-insight-storming-workshop","text":"Before conducting an event storming workshop, complete a Design Thinking Workshop in which Personas and Empathy Maps are developed and business pains and goals are defined. The event storming workshop adds more specific design on the events occuring at each step of the process, natural contexts for microservices and predictive insights to guide operation of the system. With this approach, a team that includes business owners and stakeholders can define a Minimal Viable Prototype (MVP) design for the solution. The resulting design is organized as a collection of loosely coupled microservices linked through an event-driven architecture and one or more event backbones. This style of design can be deployed into multicloud execution environments and allows for scaling and agile deployment. Preparations for the event storming workshop include the following steps: Get a room big enough to hold at least 6 to 8 persons and with enough wall space on which to stick big paper sheets: you will need a lot of wall space to define the models. Obtain green, orange, blue, and red square sticky notes, black sharpie pens and blue painter's tape. Discourage the use of open laptops during the meeting. Limit the number of chairs so that the team stays focused and connected and conversation flows easily.","title":"Conducting the event and insight storming workshop"},{"location":"methodology/eventstorming/#concepts","text":"Many of the concepts addressed during the event storming workshop are defined in the Domain Driven Design approach. The following diagrams present the elements used during the analysis. The first diagram shows the initial set of concepts that are used in the process. Domain events are also named business events . An event is some action or happening which occurred in the system at a specific time in the past. The first step in the event storming process consists of these actions: Identifying all relevant events in the domain and specific process being analyzed, Writing a very short description of each event on a \"sticky\" note and placing all the event \"sticky\" notes in sequence on a timeline. The act of writing event descriptions often results in questions to be resolved later, or discussions about definitions that need to be recorded to ensure that everyone agrees on basic domain concepts. A timeline of domain events is the critical output of the first step in the event storming process. The timeline gives everyone a common understanding of when events take place in relation to each other. You still need to be able to take this initial level of understanding and move it towards an implementation. In making that step, you must expand your thinking to encompass the idea of a command, which is the action that kicks off the processing that triggers an event. As part of understanding the role of the command, you will also want to know who invokes a command (actors) and what information is needed to allow the command to be executed. This diagram show how those analysis elements are linked together: One-View Figure. Actors consume data by using a user interface and use the UI to interact with the system via commands. Actors could also be replace by articial intelligent agents. Commands are the result of some user decision or policy, and act on relevant data which are part of a Read model in the CQRS pattern. Policies (represented by lilac stickies) are reactive logics that take place after an event occurs, and trigger other commands. Policies always start with the phrase \"whenever...\". They can be a manual step a human follows, such as a documented procedure or guidance, or they may be automated. When applying the Agile Business Rule Development methodology it will be mapped to a Decision within the Decision Model Notation . External systems produce events. Data can be presented to users in a user interface or modified by the system. Events can be created by commands or by external systems including IOT devices. They can be triggerred by the processing of other events or by some period of elapsed time. When an event is repeated or occurs regularly on a schedule, draw a clock or calendar icon in the corner of the sticky note for that event. As the events are identified and sequenced into a time line, you might find multiple independent subsequences that are not directly coupled to each other and that represent different perspectives of the system, but occur in overlapped periods of time. These parallel event streams can be addressed by putting them into separate swimlanes delineated by using horizontal blue painter's tape. As the events are organized into a timeline, possibly with swim lanes, you can identify pivotal events. Pivotal events indicate major changes in the domain and often form the boundary between one phase of the system and another. Pivotal events will typically separate (a bounded context in DDD terms). Pivotal events are identified with vertical blue painters tape (crossing all the swimlanes). An example of a section of a completed event time line with pivotal events and swimlanes is shown below.","title":"Concepts"},{"location":"methodology/eventstorming/#conducting-the-workshop","text":"The goal of the workshop is to better understand the business problem to address with a future application. But the approach can also apply to finding solutions to bottlenecks or other issues in existing applications. The workshop helps the team to understand the big picture of the solution by building a timeline of domain events as they occur during the business process life span. During the workshop, avoid documenting processing steps. The event storming method is not trying to specify a particular implementation. Instead, the focus in initial stages of the workshop is on identifying and sequencing the events that occur in the solution. The event timeline is a useful representation of the overall steps, communicating what must happen while remaining open to many possible implementation approaches.","title":"Conducting the workshop"},{"location":"methodology/eventstorming/#step-1-domain-events-discovery","text":"Begin by writing each domain event on an orange sticky note with a few words and a verb in a past tense. Describe What's happened . At first just \"storm\" the events by having each domain expert generate an individual lists of domain events. You might not need to initially place the events on the ordered timeline as they write them. The events must be worded in a way that is meaningful to the domain experts and business stakeholder. You are explaining what happens in business terms, not what happens inside the implementation of the system. You don't need to describe all the events in your domain, but you must cover the process that you are interested in exploring from end to end. Therefore, make sure that you identify the start and end events and place them on the timeline at the beginning and end of the wall covered with paper. Place the other events that you identified between these two endpoints in the closest approximation that the team can agree to a sequential order. Don\u2019t worry about overlaps at this point; overlaps are addressed later.","title":"Step 1: Domain events discovery"},{"location":"methodology/eventstorming/#step-2-tell-the-story","text":"In this step, you retell the story by talking about how to relate events to particular personas. A member of the team (often the facilitator, but others can do this as well) acts this out by taking on the perspective of a persona in the domain, such as a \"manufacturer\" who wants to ship a widget to a customer, and asking which events follow which other events. Start at the beginning of that persona's interaction and ask \"what happens next?\". Pick up and rearrange the events that the team storms. If you discover events that are duplicates, take those off the board. If events are in the wrong order, move them into the right order. When some parts are unclear, add questions or comments by using the red stickies.. Red stickies indicate that the team needs to follow up and clarify issues later. Likewise you want to use this time to document assumptions on the definition stickies. This is also a good time to rephrase events as you proceed through the story. Sometimes you need to rephrase an event description by putting the verbing in past tense, or adjusting the terms that are used to relate clearly to other identified events. In this step you focus on the mainline \"happy\" end-to-end path to avoid getting bogged down in details of exceptions and error handling. Exceptions can be added later","title":"Step 2: Tell the story"},{"location":"methodology/eventstorming/#step-3-find-the-boundaries","text":"The next step of this part of the process is to find the boundaries of your system by looking at the events. Two types of boundaries can emerge; the first type of boundary is a time boundary. Often specific key \"pivotal events\" indicate a change from one aspect of a system to another. This can happen at a hand-off from one persona to another, but it can also happen at a change of geographical, legal, or other type of boundary. If you notice that the terms that are used on the event stickies change at these boundaries, you are seeing a \"bounded context\" in Domain Driven Design terms. Highlight pivotal events by putting up blue painter\u2019s tape vertically behind the event. The second type of boundary is a subject boundary. You can detect a subject boundary by looking for the following conditions: You have multiple simultaneous series of events that only come together at a later time. You see the same terms being used in the event descriptions for a particular series of events. You can \u201cread\u201d a series of events from the point of view of a different persona when you are replaying them. You can delineate these different sets of simultaneous event streams by applying blue painter\u2019s tape horizontally, dividing the board into different swim lanes. Below is an example of a set of ordered domain events with pivotal events and subject swim lanes indicated. This example comes from applying event storming to the domain of container shipping process and is discussed in more detail in the container shipment analysis example . When the reefer container is plugged to the Vessel, it starts to emit telemetries, we change context.","title":"Step 3: Find the Boundaries"},{"location":"methodology/eventstorming/#step-4-locate-the-commands","text":"In this step you shift from analysis of the domain to the first stages of system design. Up until this point, you are simply trying to understand how the events in the domain relate to one another - this is why the participation of domain experts is so critical. However, to build a system that implements the business process that you are interested in, you have to move on to the question of how these events come into being. Commands are the most common mechanism by which events are created. The key to finding commands is to ask the question: \"Why did this event occur?\". In this step, the focus of the process moves to the sequence of actions that lead to events. Your goal is to find the causes for which the events record the effects. Expected event trigger types are: A human operator makes a decision and issues a command Some external system or sensor provides a stimulus An event results from some policy - typically automated processing of a precursor event The completion of some determined period of elapsed time. The triggering command is identified in a blue (sticky) note. Command may become a microservice operation exposed via API. The human persona issuing the command is identified and shown in a yellow note. Some events may be created by applying business policies. The diagram below illustrates the manufacturer actor using the place a shipment order command to create a shipment order placed event, as well as . It is possible to chain events and commands as presented in the \"one view\" figure above in the concepts section.","title":"Step 4: Locate the Commands"},{"location":"methodology/eventstorming/#step-5-describe-the-data","text":"You can't truly define a command without understanding the data that is needed for the command to execute in order to produce the event. You can identify several types of data during this step. First, users (personas) need data from the user interface in order to make decisions before executing a command. That data forms part of the read model in a CQRS implementation. For each command and event pair, you add a data description of the expected attributes and data elements needed to take such a decision. Here is a simple example for a shipment order placed event created from a place a shipment order action . Another important part of the process that becomes more fully fleshed out at this step is the description of policies that can trigger the generation of an event from a previous event (or set of events). Assess if the data element is a main business entity, uniquely identified by a key, supported by multiple commands. It has a life span over the business process. This will lead to develop an entity life cycle analysis. This first level of data definition helps to assess the microservice scope and responsibility as you start to see commonalities emerge from the data used among several related events. Those concepts become more obvious in the next step.","title":"Step 5: Describe the Data"},{"location":"methodology/eventstorming/#step-6-identify-the-aggregates","text":"In DDD, entities and value objects can exist independently, but often, the relations are such that an entity or a value object has no value without its context. Aggregates provide that context by being those \"roots\" that comprise one or more entities and value objects that are linked together through a lifecycle. The following diagram illustrates a detail example of aggregates for a fresh product shipment over sea. In event storming, we may not be able to get this level of detail during the first workshop, but aggregates emerge through the process by grouping events and commands that are related together. This grouping not only consists of related data (entities and value objects) but also related actions (commands) that are connected by the lifecycle of that aggregate. Aggregates ultimately suggest microservice boundaries. In the container shipment example, you can see that you can group several commands and event pairs (with their associated data) together that are related through the lifecycle of an order for shipping.","title":"Step 6: Identify the Aggregates"},{"location":"methodology/eventstorming/#step-7-define-bounded-context","text":"In this step, you define terms and concepts with a clear meaning valid in a clear boundary and you define the context within which a model applies. (The term definition can change outside of the business unit for which an application is developed). The following items may be addressed: Which team owns the model? Which part of the model transit between team organization? What are the different code bases foreseen we need to implement? What are the different data schema ? (database or json or xml schemas) Here is an example of bounded context that will, most likely, lead to a microservice: Keep the model strictly consistent within these bounds.","title":"Step 7: Define Bounded Context"},{"location":"methodology/eventstorming/#step-8-looking-forward-with-insight-storming","text":"In event atorming for Event Driven Architecture (EDA) solutions it is helpful to include an additional method step at this point identifying useful predictive analytics insights. Insights storming extends the basic methodology by looking forward and considering what if you could know in advance that an event is going to occur. How would this change your actions, and what would you do in advance of that event actually happening? You can think of insight storming as extending the analysis to Derived Events . Rather than being the factual recording of a past event, a derived event is a forward-looking or predictive event, that is, \"this event is probably going to happen at some time in the next n hours\u201d. By using this forward-looking insight combined with the known business data from earlier events, human actors and event triggering policies can make better decisions about how to react to new events as they occur. Insight storming amounts to asking workshop participants the question: \"What data would be helpful at each event trigger to assist the human user or automated event triggering policy make the best possible decision of how and when to act?\" An important motivation that drives the use of an event-driven architecture is that it simplifies design and realization of highly responsive systems that react immediately and intelligently, that is, in a personalized and context-aware way, and optimally to new events as they occur. This immediately suggests that predictive analytics and models to generate predictive insights have an important role to play. Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the likely properties of those events. These probabilistic statements are typicaly generated by using models created by data scientists or using AI or ML. Correlating or joining independently gathered sources of information can also generate important predictive insights or be input to predictive analytic models. Business owners and stakeholders in the event storming workshop can offer good intuitions in several areas: Which probabilistic insights are likely to lead to improved or optimal decision making and action? The action could take the form of an immediate response to an event when it occurs. The action could be proactive behavior to avoid an undesirable event. What combined sources of information are likely to help create a model to predict this insight? With basic event storming, you look backwards at each event because an event is something that has already happened. When you identify data needed for an actor or policy to decide when and how to issue a command, there is a tendency to restrict consideration to properties of earlier known and captured business events. In insight storming you extend the approach to explicitly look forward and consider what is the probability that a particular event will occur at some future time and what would be its expected property values? How would this change the best action to take when and if this event occurs? Is there action we can take now proactively in advance of an expected undesirable event to prevent it happening or mitigate the consequences? The insight method step amounts to getting workshop participants to identify derived events and the data sources needed for the models that generate them. Adding an insight storming step using the questions above into the workshop will improve decision making and proactive behavior in the resulting design. Insights can be published into a bus and subscribed to by any decision step guidance. By identifying derived events, you can integrate analytic models and machine learning into the designed solution. Event and derived event feeds can be processed, filtered, joined, aggregated, modeled and scored to create valuable predictive insights. Use the following new notations for the insight step: Pale blue stickies for derived events. Parallelogram shape to show when events and derived events are combined to enable deeper insight models and predictions. Identify predictive insights as early as possible in the development life cycle. The best opportunity to do this is to add this step to the event storming workshop. The two diagrams below show the results of the insight storming step for the use case of container shipment analysis. The first diagram captures insights and associated linkages for each refrigerated container, identifying when automated changes to the thermostat settings can be made, when unit maintenance should be scheduled and when the container contents must be considered spoiled. The second diagram captures insights that could trigger recommendations to adjust ship course or speed in response to expected severe weather forcasts for the route ahead or predicted congestion and expected docking and unloading delays at the next port of call.","title":"Step 8: Looking forward with insight storming"},{"location":"methodology/eventstorming/#design-iteration","text":"Attention we are not proposing to apply a waterfall approach, but before starting the deeper implementation with iterations, we want to spend sometime to define in more details what we want to build, how to organize the CI/CD projects and pipeline, select the development, test and product plaform, and define epics, user stories, components, microservices... This iteration can take from few hours to a week, depending on the expected MVP goals. For an event-driven solution a MVP for a single application should not take more than 3 to 4 iterations.","title":"Design iteration"},{"location":"methodology/eventstorming/#event-storming-to-user-stories-and-epics","text":"In agile methodology, creating user stories or epics is one of the most important elements in project management. The commands and policies related to events can be easily described as user stories, because commands and decisions are done by actors. The actor could be a system as well. For the data you must model the \"Create, Read, Update, Delete\" operations as user stories, mostly supported by a system actor. An event is the result or outcome of a user story. Events can be added as part of the acceptance criteria of the user stories to verify that the event really occurs.","title":"Event storming to user stories and epics"},{"location":"methodology/eventstorming/#applying-to-the-container-shipment-use-case","text":"The K Container Shipment use case demonstrates an implementation solution to validate the event-driven architecture. The container shipment analysis example , shows event storming and design thinking main artifacts, including artifacts for the monitoring of refrigerated containers.","title":"Applying to the container shipment use case"},{"location":"methodology/eventstorming/#some-practice-notes","text":"you can apply event storming at different level: for example at the beginning of a project to understand the high level process at stake. with a big group of people, you will stay at the high level. But it can be used to model a lower level microservice, to assess event consumed and produced.","title":"Some practice notes"},{"location":"methodology/eventstorming/#further-readings","text":"Introduction to event storming from Alberto Brandolini Event Storming Guide Wikipedia Domain Driven Design Eric Evans: \"Domain Driven Design - Tacking complexity in the heart of software\" Domain drive design with event storming introduction video Patterns related to Domain Driven Design by Martin Fowler Kyle Brown - IBM - Apply Domain-Driven Design to microservices architecture Applying DDD and event storming for event-driven microservice implementation from our own work","title":"Further Readings"},{"location":"methodology/readme/","text":"Event driven methodology Warning Still under development The event-driven microservices implementation methodology does not differ from agile microservice development, but some specific development activities needs to be added to the traditional workflow. The following figure illustrates the iterative loops aroung sprint activities, combined with a larger loop for release software to production and learn from there. The main actors represented here are the architect or technical squad leader responsible to conduct the technical assessments and to solve problems, and the developers responsible to develop components and microservices. The activities start by understanding business objectives , hypothesis and measurement metrics for evaluating the business impacts of this new software project. This activity is based on design thinking techniques and IBM Garage methodology . As part of an event driven implementation we want to focus on identifying the events , using the event storming workshop which engages the business and product owner into defining the process flow from an events point of view. The event storming workshop uses predefined constructs to discover the process, events, commands, data, actors and business policies. The combination of event storming and Domain Driven Design helps to build bounded context and the Ubiquitous Language. Applying and iterative approach for design , and development, developers define the microservices to develop and define the way to manage the communication between those services. The goal is to address the data flow and how to support the data eventual consistency. As part of the devops adoption practice, it is very important that the project team and developers put in place an efficient development pipeline, and a workspace adapted to continuously deploy code to the different environments and continuously validate the code quality with unit tests (for the business logic) and integration tests for the contract testing and interactions testing. A specific problem of distributed system is to define how the integration logic is performed, specially around data consistency and transaction. The two approaches are orchestration and choreography . The choice has an impact on the implementation of each microservices and should not be overseen. It is possible to start by the choreography as it impacts the less the microservices coding and enforce loosely coupling and move to orchestration when needed. Once those foundations are defined, the traditional iterative and incremental development tasks are performed. As any integration implementation, the challenge is to maintain clear support for the interface definition and the data model used for communication. In the case of events the payload is most likely a json or xml type. With XML, XSD are used and per construct the schema management is well defined. With Json you can use schema registry and may use approach like Apache Avro to define schema.","title":"Event driven methodology"},{"location":"methodology/readme/#event-driven-methodology","text":"Warning Still under development The event-driven microservices implementation methodology does not differ from agile microservice development, but some specific development activities needs to be added to the traditional workflow. The following figure illustrates the iterative loops aroung sprint activities, combined with a larger loop for release software to production and learn from there. The main actors represented here are the architect or technical squad leader responsible to conduct the technical assessments and to solve problems, and the developers responsible to develop components and microservices. The activities start by understanding business objectives , hypothesis and measurement metrics for evaluating the business impacts of this new software project. This activity is based on design thinking techniques and IBM Garage methodology . As part of an event driven implementation we want to focus on identifying the events , using the event storming workshop which engages the business and product owner into defining the process flow from an events point of view. The event storming workshop uses predefined constructs to discover the process, events, commands, data, actors and business policies. The combination of event storming and Domain Driven Design helps to build bounded context and the Ubiquitous Language. Applying and iterative approach for design , and development, developers define the microservices to develop and define the way to manage the communication between those services. The goal is to address the data flow and how to support the data eventual consistency. As part of the devops adoption practice, it is very important that the project team and developers put in place an efficient development pipeline, and a workspace adapted to continuously deploy code to the different environments and continuously validate the code quality with unit tests (for the business logic) and integration tests for the contract testing and interactions testing. A specific problem of distributed system is to define how the integration logic is performed, specially around data consistency and transaction. The two approaches are orchestration and choreography . The choice has an impact on the implementation of each microservices and should not be overseen. It is possible to start by the choreography as it impacts the less the microservices coding and enforce loosely coupling and move to orchestration when needed. Once those foundations are defined, the traditional iterative and incremental development tasks are performed. As any integration implementation, the challenge is to maintain clear support for the interface definition and the data model used for communication. In the case of events the payload is most likely a json or xml type. With XML, XSD are used and per construct the schema management is well defined. With Json you can use schema registry and may use approach like Apache Avro to define schema.","title":"Event driven methodology"},{"location":"rt-analytics/","text":"Process continuous streaming events One of the essential elements of modern event-driven solutions is the ability to process continuous event streams to derive real time insights and intelligence. In this section we will take more detailed look at what this means in terms of required capabilities and the technology choices that are available to provide these as part of the Event Driven Architecture. Streaming analytics (real-time analytics) Streaming analytics provides the capabilities to look into and understand the events flowing through unbounded real-time event streams. Streaming applications process the event flow and allow data and analytical functions to be applied to information in the stream. Streaming applications are written as multistep flows across the following capabilities: Ingest many sources of events. Prepare data by transforming, filtering, correlating, aggregating on some metrics and leveraging other data sources for data enrichment. Detect and predict event patterns using scoring and classification. Decide by applying business rules and business logic. Act by directly executing an action, or in event-driven systems publishing an event notification or command. Basic streaming analytics capabilities To support the real-time analytical processing of the unbounded event streams, the following capabilities are essential to the event stream processing component: Continuous event ingestion and analytical processing. Processing across multiple event streams. Low latency processing, where data do not have to be stored. Processing of high-volume and high-velocity streams of data. Continuous query and analysis of the feed. Correlation across events and streams. Windowing and stateful processing. Query and analysis of stored data. Development and execution of data pipelines. Development and execution of analytics pipelines. Scoring of machine learning models in line in the real-time event stream processing. Support for real-time analytics and decision-making Beyond the basic capabilities, consider supporting other frequently-seen event stream types and processing capabilities in your event stream processing component. By creating functions for these stream types and processes in the streaming application code, you can simplify the problem and reduce the development time. These capabilities include the following: Geospatial Location-based analytics Geofencing & map matching Spatio-temporal hangout detection Time series analysis Timestamped data analysis Anomaly detection & forecasting Text analytics Natural Language Processing & Natural Language Understanding Sentiment analysis & entity extraction Video and audio Speech-to-text conversion Image recognition Rules Decisions described as business logic Complex Event Processing (CEP) Temporal pattern detection Entity Analytics Relationships between entities Probabilistic matching Application programming languages and standards Few standards exist for event stream applications and languages. Typically, streaming engines have provided language-specific programming models tied to a specific platform. The commonly used languages include the following: * Python supports working with data and is popular with data scientists and data engineers. * Java is the pervasive application development language. * Scala adds functional programming and immutable objects to Java. Other platform specific languages have emerged when real-time processing demands stringent performance requirements real time processing performance is required. More recently Google initiated the Apache Beam project https://beam.apache.org/ to provide a unified programming model for streaming analytics applications. Beam is a higher-level unified programming model that provides a standard way of writing streaming analytics applications in many supported languages, including Java, Python, Go and SQL. Streaming analytics engines typically support this unified programming model through a Beam runner that takes the code and converts it to platform-native executable code for the specific engine. See https://beam.apache.org/documentation/runners/capability-matrix/ for details of supporting engines and the capabilities. Leading engines include Google Cloud DataFlow, Apache Flink, Apache Spark, Apache Apex, and IBM Streams. Run time characteristics In operational terms streaming analytics engines must receive and analyze arriving data continuously: The \"Feed Never Ends\" The collection is unbounded. Not a request response set based model. The \"Firehose Doesn\u2019t Stop\" Keep drinking and keep up. The processing rate is greater than or equal to the feed rate. The analytics engine must be resilient and self-healing. These specialized demands and concerns, which are not found in many other information processing environments, have led to highly-optimized runtimes and engines for stateful, parallel processing of analytical workloads across multiple event streams. Products Streaming Analytics The market for streaming analytics products is quite confused with lots of different offering and very few standards to bring them together. The potential product selection list for the streaming analytics component in the event driven architecture would need to consider: Top Open Source projects: * Flink - real time streaming engine, both real time and batch analytics in one tool. * Spark Streaming - micro batch processing through spark engine. * Storm - Has not shown enough adoption. * Kafka Streams - new/emerging API access for processing event streams in Kafka using a graph of operators Major Cloud Platform Providers support: * Google Cloud DataFlow \u2013 proprietary engine open source streams application language ( Beam ) * Azure Stream Analytics \u2013 proprietary engine , SQL interface * Amazon Kinesis - proprietary AWS IBM offerings * IBM Streams/streaming Analytics (High performing parallel processing engine for real time analytics work loads) * IBM Event streams (Kafka based event log/streaming platform) Evaluation of the various options, highlights * The proprietary engines from the major providers, Google, MicroSoft, Amazon and IBM Streams continue to provide significant benefits in terms of performance and functionality for real time analysis of high volume realtime event streams. * Kafka streams provides a convenient programming model for microservices to interact with the event stream data, but doesnt provide the optimized stream processing engine required for high volume real time analytics. Our decision for the Event Driven Architecture is to include: IBM streams as the performant, functionally rich real time event stream processing engine Event Streams (Kafka Streams), for manipulation of event streams within microservices IBM streams also supports Apache Beam as the open source Streams Application language, which would allow portability of streams applications across, Flink, Spark, Google DataFlow... Decision Insights Decision insight is a stateful operator to manage business decision on enriched event linked to business context and business entities. This is the cornerstone to apply business logic and best action using time related business rules. See this note too IBM [Operational Decision Manager Product documentation](https://www.ibm.com/support/knowledgecenter/en/SSQP76_8.9.1/com.ibm.odm.itoa.overview/topics/con_what_is_i2a.html","title":"Concepts"},{"location":"rt-analytics/#process-continuous-streaming-events","text":"One of the essential elements of modern event-driven solutions is the ability to process continuous event streams to derive real time insights and intelligence. In this section we will take more detailed look at what this means in terms of required capabilities and the technology choices that are available to provide these as part of the Event Driven Architecture.","title":"Process continuous streaming events"},{"location":"rt-analytics/#streaming-analytics-real-time-analytics","text":"Streaming analytics provides the capabilities to look into and understand the events flowing through unbounded real-time event streams. Streaming applications process the event flow and allow data and analytical functions to be applied to information in the stream. Streaming applications are written as multistep flows across the following capabilities: Ingest many sources of events. Prepare data by transforming, filtering, correlating, aggregating on some metrics and leveraging other data sources for data enrichment. Detect and predict event patterns using scoring and classification. Decide by applying business rules and business logic. Act by directly executing an action, or in event-driven systems publishing an event notification or command.","title":"Streaming analytics (real-time analytics)"},{"location":"rt-analytics/#basic-streaming-analytics-capabilities","text":"To support the real-time analytical processing of the unbounded event streams, the following capabilities are essential to the event stream processing component: Continuous event ingestion and analytical processing. Processing across multiple event streams. Low latency processing, where data do not have to be stored. Processing of high-volume and high-velocity streams of data. Continuous query and analysis of the feed. Correlation across events and streams. Windowing and stateful processing. Query and analysis of stored data. Development and execution of data pipelines. Development and execution of analytics pipelines. Scoring of machine learning models in line in the real-time event stream processing.","title":"Basic streaming analytics capabilities"},{"location":"rt-analytics/#support-for-real-time-analytics-and-decision-making","text":"Beyond the basic capabilities, consider supporting other frequently-seen event stream types and processing capabilities in your event stream processing component. By creating functions for these stream types and processes in the streaming application code, you can simplify the problem and reduce the development time. These capabilities include the following: Geospatial Location-based analytics Geofencing & map matching Spatio-temporal hangout detection Time series analysis Timestamped data analysis Anomaly detection & forecasting Text analytics Natural Language Processing & Natural Language Understanding Sentiment analysis & entity extraction Video and audio Speech-to-text conversion Image recognition Rules Decisions described as business logic Complex Event Processing (CEP) Temporal pattern detection Entity Analytics Relationships between entities Probabilistic matching","title":"Support for real-time analytics and decision-making"},{"location":"rt-analytics/#application-programming-languages-and-standards","text":"Few standards exist for event stream applications and languages. Typically, streaming engines have provided language-specific programming models tied to a specific platform. The commonly used languages include the following: * Python supports working with data and is popular with data scientists and data engineers. * Java is the pervasive application development language. * Scala adds functional programming and immutable objects to Java. Other platform specific languages have emerged when real-time processing demands stringent performance requirements real time processing performance is required. More recently Google initiated the Apache Beam project https://beam.apache.org/ to provide a unified programming model for streaming analytics applications. Beam is a higher-level unified programming model that provides a standard way of writing streaming analytics applications in many supported languages, including Java, Python, Go and SQL. Streaming analytics engines typically support this unified programming model through a Beam runner that takes the code and converts it to platform-native executable code for the specific engine. See https://beam.apache.org/documentation/runners/capability-matrix/ for details of supporting engines and the capabilities. Leading engines include Google Cloud DataFlow, Apache Flink, Apache Spark, Apache Apex, and IBM Streams.","title":"Application programming languages and standards"},{"location":"rt-analytics/#run-time-characteristics","text":"In operational terms streaming analytics engines must receive and analyze arriving data continuously: The \"Feed Never Ends\" The collection is unbounded. Not a request response set based model. The \"Firehose Doesn\u2019t Stop\" Keep drinking and keep up. The processing rate is greater than or equal to the feed rate. The analytics engine must be resilient and self-healing. These specialized demands and concerns, which are not found in many other information processing environments, have led to highly-optimized runtimes and engines for stateful, parallel processing of analytical workloads across multiple event streams.","title":"Run time characteristics"},{"location":"rt-analytics/#products","text":"","title":"Products"},{"location":"rt-analytics/#streaming-analytics","text":"The market for streaming analytics products is quite confused with lots of different offering and very few standards to bring them together. The potential product selection list for the streaming analytics component in the event driven architecture would need to consider: Top Open Source projects: * Flink - real time streaming engine, both real time and batch analytics in one tool. * Spark Streaming - micro batch processing through spark engine. * Storm - Has not shown enough adoption. * Kafka Streams - new/emerging API access for processing event streams in Kafka using a graph of operators Major Cloud Platform Providers support: * Google Cloud DataFlow \u2013 proprietary engine open source streams application language ( Beam ) * Azure Stream Analytics \u2013 proprietary engine , SQL interface * Amazon Kinesis - proprietary AWS IBM offerings * IBM Streams/streaming Analytics (High performing parallel processing engine for real time analytics work loads) * IBM Event streams (Kafka based event log/streaming platform) Evaluation of the various options, highlights * The proprietary engines from the major providers, Google, MicroSoft, Amazon and IBM Streams continue to provide significant benefits in terms of performance and functionality for real time analysis of high volume realtime event streams. * Kafka streams provides a convenient programming model for microservices to interact with the event stream data, but doesnt provide the optimized stream processing engine required for high volume real time analytics. Our decision for the Event Driven Architecture is to include: IBM streams as the performant, functionally rich real time event stream processing engine Event Streams (Kafka Streams), for manipulation of event streams within microservices IBM streams also supports Apache Beam as the open source Streams Application language, which would allow portability of streams applications across, Flink, Spark, Google DataFlow...","title":"Streaming Analytics"},{"location":"rt-analytics/#decision-insights","text":"Decision insight is a stateful operator to manage business decision on enriched event linked to business context and business entities. This is the cornerstone to apply business logic and best action using time related business rules. See this note too IBM [Operational Decision Manager Product documentation](https://www.ibm.com/support/knowledgecenter/en/SSQP76_8.9.1/com.ibm.odm.itoa.overview/topics/con_what_is_i2a.html","title":"Decision Insights"},{"location":"serverless/","text":"Function as a Service Kubeless To install kubeless on ICP we first connect to the cluster and then use the command below: $ kubectl create namespace kubeless $ kubectl create -f https://github.com/kubeless/kubeless/releases/download/v1.0.0-alpha.8/kubeless-v1.0.0-alpha.8.yaml -n kubeless The image is using RBAC: The deployment creates one pod with 3 containers inside: We need to have the kubeless CLI install: $ export OS = $( uname -s | tr '[:upper:]' '[:lower:]' ) $ curl -OL https://github.com/kubeless/kubeless/releases/download/ $RELEASE /kubeless_ $OS -amd64.zip && \\ unzip kubeless_ $OS -amd64.zip && \\ sudo mv bundles/kubeless_ $OS -amd64/kubeless /usr/local/bin/ To deploy a simple hello The code for this function is under this repository but it is a simple python function implementing the serverless 'interface': def hello ( event , context ): print ( event ) return event [ 'data' ] To deploy we can use the command: $ kubeless function deploy hellojb -- runtime python3 . 6 -- trigger - http -- from - file functionHello . py -- handler functionHello . hello INFO [ 0000 ] Deploying function ... INFO [ 0001 ] Function hellojb submitted for deployment INFO [ 0001 ] Check the deployment status executing ' kubeless function ls hellojb ' To see the functions deployed $ kubectl get functions or $ kubeless function ls NAME NAMESPACE HANDLER RUNTIME DEPENDENCIES STATUS hellojb greencompute test.hello python3.6 1 /1 READY The deployment of a function creates automatically a pod: $ kubectl describe pod hellojb ... Containers: hellojb: Container ID: docker://53ca1131747e5b18bfeb67609b4e7bb2400cf45202ade2c03274b9df1eff9bc2 Image: kubeless/python@sha256:0c9f8f727d42625a4e25230cfe612df7488b65f283e7972f84108d87e7443d72 Image ID: docker-pullable://kubeless/python@sha256:0c9f8f727d42625a4e25230cfe612df7488b65f283e7972f84108d87e7443d72 Port: 8080 /TCP Host Port: 0 /TCP State: Running Started: Thu, 06 Sep 2018 17 :12:52 -0700 Ready: True Restart Count: 0 Liveness: http-get http://:8080/healthz delay = 3s timeout = 1s period = 30s #success=1 #failure=3 Environment: FUNC_HANDLER: hello MOD_NAME: test FUNC_TIMEOUT: 180 FUNC_RUNTIME: python3.6 FUNC_MEMORY_LIMIT: 0 FUNC_PORT: 8080 PYTHONPATH: /kubeless/lib/python3.6/site-packages:/kubeless Mounts: /kubeless from hellojb ( rw ) /var/run/secrets/kubernetes.io/serviceaccount from default-token-9nw2z ( ro ) and kubeless create service for each function: $ kubectl describe svc hellojb Name : hellojb Namespace : greencompute Labels : created - by = kubeless function = hellojb Annotations : < none > Selector : created - by = kubeless , function = hellojb Type : ClusterIP IP : 10.10 . 10.41 Port : http - function - port 8080 / TCP TargetPort : 8080 / TCP Endpoints : 192.168 . 130.101 : 8080 Session Affinity : None Events : < none > Remove the function kubeless delete Calling the function The quickest way is to proxy the server and then call the local URL: $ kubectl proxy -p 8080 & $ kubeless function call hellojb --data 'Hello Bill!' A second way is to test using HTTP client. Developing a predictive scoring function In this project we are addressing how to develop a scoring service using Python, sklearn and serveless to deploy the model as function. Compendium Excellent article from Martin Fowler Serverless framework : The Framework uses new event-driven compute services, like AWS Lambda, Google Cloud Functions, and more. It's a command-line tool, providing scaffolding, workflow automation and best practices for developing and deploying your serverless architecture. Apache OpenWhisk Claudia to deploy nodejs on AWS lambda Zappa : Zappa makes it super easy to build and deploy server-less, event-driven Python applications on AWS Lambda + API Gateway. Serverless conf: operational best practices Evaluating cost for FaaS","title":"Serverless"},{"location":"serverless/#function-as-a-service","text":"","title":"Function as a Service"},{"location":"serverless/#kubeless","text":"To install kubeless on ICP we first connect to the cluster and then use the command below: $ kubectl create namespace kubeless $ kubectl create -f https://github.com/kubeless/kubeless/releases/download/v1.0.0-alpha.8/kubeless-v1.0.0-alpha.8.yaml -n kubeless The image is using RBAC: The deployment creates one pod with 3 containers inside: We need to have the kubeless CLI install: $ export OS = $( uname -s | tr '[:upper:]' '[:lower:]' ) $ curl -OL https://github.com/kubeless/kubeless/releases/download/ $RELEASE /kubeless_ $OS -amd64.zip && \\ unzip kubeless_ $OS -amd64.zip && \\ sudo mv bundles/kubeless_ $OS -amd64/kubeless /usr/local/bin/","title":"Kubeless"},{"location":"serverless/#to-deploy-a-simple-hello","text":"The code for this function is under this repository but it is a simple python function implementing the serverless 'interface': def hello ( event , context ): print ( event ) return event [ 'data' ] To deploy we can use the command: $ kubeless function deploy hellojb -- runtime python3 . 6 -- trigger - http -- from - file functionHello . py -- handler functionHello . hello INFO [ 0000 ] Deploying function ... INFO [ 0001 ] Function hellojb submitted for deployment INFO [ 0001 ] Check the deployment status executing ' kubeless function ls hellojb ' To see the functions deployed $ kubectl get functions or $ kubeless function ls NAME NAMESPACE HANDLER RUNTIME DEPENDENCIES STATUS hellojb greencompute test.hello python3.6 1 /1 READY The deployment of a function creates automatically a pod: $ kubectl describe pod hellojb ... Containers: hellojb: Container ID: docker://53ca1131747e5b18bfeb67609b4e7bb2400cf45202ade2c03274b9df1eff9bc2 Image: kubeless/python@sha256:0c9f8f727d42625a4e25230cfe612df7488b65f283e7972f84108d87e7443d72 Image ID: docker-pullable://kubeless/python@sha256:0c9f8f727d42625a4e25230cfe612df7488b65f283e7972f84108d87e7443d72 Port: 8080 /TCP Host Port: 0 /TCP State: Running Started: Thu, 06 Sep 2018 17 :12:52 -0700 Ready: True Restart Count: 0 Liveness: http-get http://:8080/healthz delay = 3s timeout = 1s period = 30s #success=1 #failure=3 Environment: FUNC_HANDLER: hello MOD_NAME: test FUNC_TIMEOUT: 180 FUNC_RUNTIME: python3.6 FUNC_MEMORY_LIMIT: 0 FUNC_PORT: 8080 PYTHONPATH: /kubeless/lib/python3.6/site-packages:/kubeless Mounts: /kubeless from hellojb ( rw ) /var/run/secrets/kubernetes.io/serviceaccount from default-token-9nw2z ( ro ) and kubeless create service for each function: $ kubectl describe svc hellojb Name : hellojb Namespace : greencompute Labels : created - by = kubeless function = hellojb Annotations : < none > Selector : created - by = kubeless , function = hellojb Type : ClusterIP IP : 10.10 . 10.41 Port : http - function - port 8080 / TCP TargetPort : 8080 / TCP Endpoints : 192.168 . 130.101 : 8080 Session Affinity : None Events : < none > Remove the function kubeless delete","title":"To deploy a simple hello"},{"location":"serverless/#calling-the-function","text":"The quickest way is to proxy the server and then call the local URL: $ kubectl proxy -p 8080 & $ kubeless function call hellojb --data 'Hello Bill!' A second way is to test using HTTP client.","title":"Calling the function"},{"location":"serverless/#developing-a-predictive-scoring-function","text":"In this project we are addressing how to develop a scoring service using Python, sklearn and serveless to deploy the model as function.","title":"Developing a predictive scoring function"},{"location":"serverless/#compendium","text":"Excellent article from Martin Fowler Serverless framework : The Framework uses new event-driven compute services, like AWS Lambda, Google Cloud Functions, and more. It's a command-line tool, providing scaffolding, workflow automation and best practices for developing and deploying your serverless architecture. Apache OpenWhisk Claudia to deploy nodejs on AWS lambda Zappa : Zappa makes it super easy to build and deploy server-less, event-driven Python applications on AWS Lambda + API Gateway. Serverless conf: operational best practices Evaluating cost for FaaS","title":"Compendium"},{"location":"training/core-techno-getstarted/","text":"Getting started around the core technologies used in EDA Abstract We expect you have some good understanding of the following technologies: Nodejs / Javascript / Typescripts Java 1.8 amd microprofile architecture Python 3.6 Angular 7, HTML, CSS - This is for the user interface but this is more optional. Maven, npm, bash WebSphere Liberty or OpenLiberty Docker Docker compose Helm Kubernetes Apache Kafka, Kafka API From the list above, the following getting started and tutorials can be studied to get a good pre-requisite knowledge for EDA adoption. You can skip those tutorials if you are already confortable on those technologies. Java From zero to hero in Java 1.8 - an infoworld good article Getting started with Open Liberty from the openliberty.io site. Another Open Liberty getting started application from IBM Cloud team Getting started with Apache Maven Java microprofile application Deploy MicroProfile-based Java microservices on Kubernetes Nodejs Getting started Nodejs and npm Angular One of the repository includes an Angular app, so if you want to be familiar with Angular here are two good articles: Angular tutorial - This is for the user interface but this is more optional. Applying a test driven practice for angular application Python The integration tests are done in Python to illustrate how to integrate kafka with python, and also because it is simple to do develop tests with this scripting language: A good getting started in Python Kubernetes, docker In case you do not know it, there is this Docker getting started tutorial As we can use docker compose to control the dependencies between microservices and run all the solution as docker containers, it is important to read the Docker compose - getting started article. Kubernetes and IBM developer learning path for Kubernetes and the Garage course Kubernetes 101 . Use the \"Develop a Kubernetes app with Helm\" toolchain on IBM Cloud Understand docker networking as we use docker compose to run the reference implementation locally. This is still optional but as we will adopt Knative, this introduction is relevant to read. How to deploy, manage, and secure your container-based workloads on IKS and part 2","title":"Getting started around the core technologies used in EDA"},{"location":"training/core-techno-getstarted/#getting-started-around-the-core-technologies-used-in-eda","text":"Abstract We expect you have some good understanding of the following technologies: Nodejs / Javascript / Typescripts Java 1.8 amd microprofile architecture Python 3.6 Angular 7, HTML, CSS - This is for the user interface but this is more optional. Maven, npm, bash WebSphere Liberty or OpenLiberty Docker Docker compose Helm Kubernetes Apache Kafka, Kafka API From the list above, the following getting started and tutorials can be studied to get a good pre-requisite knowledge for EDA adoption. You can skip those tutorials if you are already confortable on those technologies.","title":"Getting started around the core technologies used in EDA"},{"location":"training/core-techno-getstarted/#java","text":"From zero to hero in Java 1.8 - an infoworld good article Getting started with Open Liberty from the openliberty.io site. Another Open Liberty getting started application from IBM Cloud team Getting started with Apache Maven Java microprofile application Deploy MicroProfile-based Java microservices on Kubernetes","title":"Java"},{"location":"training/core-techno-getstarted/#nodejs","text":"Getting started Nodejs and npm","title":"Nodejs"},{"location":"training/core-techno-getstarted/#angular","text":"One of the repository includes an Angular app, so if you want to be familiar with Angular here are two good articles: Angular tutorial - This is for the user interface but this is more optional. Applying a test driven practice for angular application","title":"Angular"},{"location":"training/core-techno-getstarted/#python","text":"The integration tests are done in Python to illustrate how to integrate kafka with python, and also because it is simple to do develop tests with this scripting language: A good getting started in Python","title":"Python"},{"location":"training/core-techno-getstarted/#kubernetes-docker","text":"In case you do not know it, there is this Docker getting started tutorial As we can use docker compose to control the dependencies between microservices and run all the solution as docker containers, it is important to read the Docker compose - getting started article. Kubernetes and IBM developer learning path for Kubernetes and the Garage course Kubernetes 101 . Use the \"Develop a Kubernetes app with Helm\" toolchain on IBM Cloud Understand docker networking as we use docker compose to run the reference implementation locally. This is still optional but as we will adopt Knative, this introduction is relevant to read. How to deploy, manage, and secure your container-based workloads on IKS and part 2","title":"Kubernetes, docker"},{"location":"training/eda-skill-journey/","text":"EDA Skill Journey Implementing cloud native, event-driven solution with microservices deployed on kubernetes involves a broad skill set. In this article, we are proposing a deep dive learning journey for developers to adopt event-driven microservice implementation. This EDA project includes best practices around the technologies used in any event-driven microservice solution implementation. Our Reefer solution implementation tries to illustrate most of those patterns. It includes a set of technologies that represent the modern landscape of cloud native applications (Kafka, maven, java, microprofile, kafka API, Kafka Stream API, Spring boot, Python, Nodejs, and Postgresql) but also some specific analytics and AI components like IBM Streams analytics and machine learning with Jupyter notebook to develop predictive scoring model. A developer who wants to consume this content does not need to know everything at the expert level. You can progress steps by steps and it will take a 2 to 3 weeks to digest everything. Note We expect you have some good understanding of the following technologies: Nodejs / Javascript / Typescripts Java 1.8 amd microprofile architecture Python 3.6 Angular 7, HTML, CSS - This is for the user interface but this is more optional. Maven, npm, bash WebSphere Liberty or OpenLiberty Docker Docker compose Helm Kubernetes Apache Kafka, Kafka API We have build a getting started and tutorial list for you to study here . Event Driven Concepts Now the development of event driven solution involves specific technologies and practices. The following links should be studied in the proposed order: Why Event Driven Architecture now? 1- Reading: Understand EDA fundamentals (8 hours) The following content is for architects, and developers who want to understand the technologies and capabilities of an event driven architecture. Understand the Key EDA concepts like events, event streams, events and messages differences... Be confortable with the EDA reference architecture with event backbone, microservices and real time analytics . Which is extended with machine learning and AI integrated with real time analytics reference architecture , integrating machine learning workbench and event sourcing as data source, and real time analytics for deployment. Review Event sources - as event producers article . Read the concept of Event backbone where Kafka is the main implementation . As kafka is the event backbone, review its key concepts and building blocks and then review how to support high availability (HA) and disaster recovery (DR) with IBM Event Streams or Kafka Architecture Considerations . Review one of the common industry use case using Kafka, to perform data replication using kafka and Change Data Capture . 2- Hands on Lab: Getting started with Event Streams and Kafka (3 hours) Start by creating an Event Stream service in IBM Cloud by performing the IBM Event Stream Getting started . To provision your service, go to the IBM Cloud Catalog and search for Event Streams . It is in the Integration category. Create the service and specify a name, a region/location (select the same as your cluster), and a resource group, add a tag if you want to, then select the standard plan. Warning If you are using a non default resource group, you need to be sure your userid as editor role to the resource group to be able to create service under the resource group. See this note for creating Event Stream with Enterprise plan. Review another getting started from Apache Kafka quickstart Finally review the Confluent point of view on getting started as it covers some of the persona involved in a kafka based solution. 3 - Readings : Methodology (2 hours read) It is important to understand how to start a project using event. This lab persents how to start an event driven solution implementation using the event storming and domain driven design: We are proposing a set of activities to develop and release an event-driven solution based on the agile development approach described in the IBM Garage method. The adopted approach for event identification is the Event storming methodology introduced by Alberto Brandolini in \"Introducing event storming book\". We have extended it with the event insight practice used to identify potential real time analytics use cases. Once the event storming deliver events, commands, aggregates we can start doing some Domain Design Driven exercises and apply it to the business application. For those of you, who are interested to know how to apply the event storming and domain driven design methodology to the Reefer shipment process, you can review the following articles: The solution introduction to get a sense of the goals of this application. (10 minutes read) followed by the event storming analysis report (30 minutes read). and the derived design from this analysis. (15 minutes reading) Hands-on labs As next steps beyond getting started and reading our technical point of view, you can try different hands-on exercises based on our \"Reefer container shipment solution\" . This application is quite complex and includes different components. You do not need to do all, but you should get a good understanding of all those component implementation as most of the code and approach is reusable for your future implementation. Note At the end of this training you should have the following solution up and running (See detailed description here ): You can run the solution locally, on IBM Cloud Private, on IBM Kubernetes Services or Openshift. 4 - Hands on lab: Prepare your local environment (30 mn) Goals Install Kafka - zookeeper and postgresql docker images and start them in docker-compose or minikube environment First be sure to complete the pre-requisites by following those steps . Then do one of the following choice: To run a local Kafka / zookeeper backbone using docker compose, in less than 3 minutes follow the steps described in this note . Or use Minikube/ docker kubernetes to get kafka, zookeeper and postgreSQl up and running on a unique node kubernetes cluster. 5 - Hands on lab: Prepare IBM Cloud IKS Openshift environment You can run the solution locally, but you can also deploy it to a kubernetes cluster. So this lab is more optional or you can do it later when you are familar with the solution. If you want to build and run the solution locally go to step 8 below. So if you are interested by a public cloud deployment, you can follow this tutorial: \"Creating an IBM Cloud Red Hat OpenShift Container Platform cluster\" . Be sure to have administration privilege, within your account, to be able to create cluster. It will take less than 30 minutes to get the cluster provisioned. You can follow the steps to create the cluster with the console or use the IBM Cloud CLI. Be sure to be logged to your IBM Cloud account: ibmcloud login - a https : // cloud . ibm . com - u < userid > - p < password > - c < accoundID > Get the private and public vlan IP address for your zone: ibmcloud ks vlans --zone wdc06 It will return something like ID Name Number Type Router Supports Virtual Workers < private_VLAN_ID to keep secret > 2445 private bcr01a . wdc06 true < public_VLAN_ID to keep secret > 1305 public fcr01a . wdc06 true Create a 3 nodes kubernetes cluster using the small hardware footprint, and openshift 3.11 image: ibmcloud ks cluster - create --name greencluster --location wdc06 --kube-version 3.11_openshift --machine-type b3c.4x16.encrypted --workers 3 --public-vlan <public_VLAN_ID> --private-vlan <private_VLAN_ID> Verify your cluster once created: ibmcloud ks cluster - get --cluster greencluster Retrieving cluster greencluster ... OK Name : greencluster ID : < keep it secret > State : normal Created : 2019 - 07 - 16 T20 : 47 : 34 + 0000 Location : wdc06 Master URL : https : //< secret_too > . us - east . containers . cloud . ibm . com : 21070 Public Service Endpoint URL : https : //< secret_too > . us - east . containers . cloud . ibm . com : 21070 Private Service Endpoint URL : - Master Location : Washington D . C . Master Status : Ready ( 2 days ago ) Master State : deployed Master Health : normal Ingress Subdomain : greencluster . us - east . containers . appdomain . cloud Ingress Secret : greencluster Workers : 3 Worker Zones : wdc06 Version : 3 . 11 . 104 _1507_openshift Owner : < secret_too > Monitoring Dashboard : - Resource Group ID : < secret_too > Resource Group Name : default Download the configuration files to connect to your cluster ibmcloud ks cluster - config --cluster greencluster Then export the KUBECONFIG variable. export KUBECONFIG =/ Users /< you on your computer >/ . bluemix / plugins / container - service / clusters / greencluster / kube - config - wdc06 - greencluster . yml Now any oc command will work against your remote cluster. Access the Openshift container platform console using the master URL Something like: https:// .us-east.containers.cloud.ibm.com:21070 6 - Hands on lab: Create Kafka topics and get service credentials From the Event Stream services you created in Lab 2 go to the service credentials page, create new credentials to get the Kafka brokers list, the admim URL and the api_key needed to authenticate the consumer or producer code. The Event streams broker API key is needed to connect any consumers or producers to access the service in IBM Cloud. When those producers and consumers are deployed to kubernetes, the way to share security keys, is to define a kubernetes secret and deploy it to the IKS cluster. Define a Event Stream API key secret: to configure a secret under the greencompute namespace. For kubernetes platform: kubectl create secret generic eventstreams-apikey --from-literal = binding = '<replace with api key>' -n greencompute For Openshift use the name of the project as namespace kubectl create secret generic eventstreams - apikey --from-literal=binding='<replace with api key>' -n reefer-shipment-solution Verify the secrets: kubectl describe secrets - n greencompute or for openshift using project: kubectl describe secrets - n reefer - shipment - solution This secret is used by all the solution microservices which are using Kafka / Event Streams. The detail of how we use it with environment variables, is described in one of the project here. Finally in the Manage panel add the topics needed for the solution. We need at least the following: Note In your local deployment the kafka topics are created automatically using the launch script. 7 - Hands on lab: Get a simple getting started event producer deployed on openshift with Event Stream Note For local deployment, smoke test scripts helps to show the event produced and consumed. To validate the IBM Event Stream and the openshift app are communicating, we use a simple order producer app we have developed as part of the Reefer container solution. It is done with python and can be found in this repository . We use Openshift source to image workflow to deploy this app to Openshift. Follow the steps in the readme as part of the lab. There are other tools to use to quickly 8 - Hands on lab: Build and run the solution locally Goals Build and run the solution so you can understand the Java-maven, Nodejs build process with docker stage build. Build and deploy the solution locally using docker compose 9 - Hands on: Perform tests Perform the smoke tests locally Finally, run the smoke tests to assess all the components are running well. For that in the refarch-kc project run the script: For docker-compose: . / scripts / smokeTests . sh LOCAL for MINIKUBE: . / scripts / smokeTests . sh MINIKUBE You should see an Order created for the \"GoodManuf\" customer. Then the order is visible in the command and the query microservices. Warning To stop docker-compose deployment use the following command: . / scripts / stopLocalEnv . sh LOCAL or for the minikube stopLocalEnv . sh MINIKUBE Optional: Execute integration tests on the local environment Execute the integration tests to validate the solution end to end. Optional: Execute the demonstration script Execute the demonstration script 10 - Reading: Review the CQRS patterns implementation Read Event driven design patterns for microservice with the Command Query Responsability Segregation, event sourcing and saga patterns. Review the Event sourcing design pattern explanations and how it is tested with some integration tests: Review the CQRS pattern . Review the CQRS code in the order management microservice implementation Kafka Python API and some examples in our integration tests project Kafka Nodejs API used in the voyage microservice Lab 11: Run the solution on IBM Cloud Deploying the solution on IBM Cloud Kubernetes Service Perform smokeTests locally on the solution running on IKS. Lab 12: Optional - Run the solution on IBM Cloud Private Deploying the solution on IBM Cloud Private Lab 13: Data replication with Kafka One of the common usage of using Kafka is to combine it with a Change Data Capture component to get update from a \"legacy\" data base to the new microservice runtime environment. We are detailing an approach in this article . Lab 14: Real time analytics and Machine learning IBM Cloud Streaming Analytics introduction and getting started Apply predictive analytics on container metrics for predictive maintenance use case Other subjects Develop a toolchain for one of the container manager service Our Kubernetes troubleshooting notes Kafka monitoring IBM Event Streams - stream analytics app Event detection on continuous feed using Streaming Analytics in IBM Cloud. Read how to process continuous streaming events Event-driven cloud-native applications The Cloud Private IBM Event Streams product running on private cloud Read introduction to act on events with IBM Cloud Functions Slack channel Contact us on '#eda-ac` channel under the ibmcase.slack.com workspace.","title":"Skill journey"},{"location":"training/eda-skill-journey/#eda-skill-journey","text":"Implementing cloud native, event-driven solution with microservices deployed on kubernetes involves a broad skill set. In this article, we are proposing a deep dive learning journey for developers to adopt event-driven microservice implementation. This EDA project includes best practices around the technologies used in any event-driven microservice solution implementation. Our Reefer solution implementation tries to illustrate most of those patterns. It includes a set of technologies that represent the modern landscape of cloud native applications (Kafka, maven, java, microprofile, kafka API, Kafka Stream API, Spring boot, Python, Nodejs, and Postgresql) but also some specific analytics and AI components like IBM Streams analytics and machine learning with Jupyter notebook to develop predictive scoring model. A developer who wants to consume this content does not need to know everything at the expert level. You can progress steps by steps and it will take a 2 to 3 weeks to digest everything. Note We expect you have some good understanding of the following technologies: Nodejs / Javascript / Typescripts Java 1.8 amd microprofile architecture Python 3.6 Angular 7, HTML, CSS - This is for the user interface but this is more optional. Maven, npm, bash WebSphere Liberty or OpenLiberty Docker Docker compose Helm Kubernetes Apache Kafka, Kafka API We have build a getting started and tutorial list for you to study here .","title":"EDA Skill Journey"},{"location":"training/eda-skill-journey/#event-driven-concepts","text":"Now the development of event driven solution involves specific technologies and practices. The following links should be studied in the proposed order: Why Event Driven Architecture now?","title":"Event Driven Concepts"},{"location":"training/eda-skill-journey/#1-reading-understand-eda-fundamentals-8-hours","text":"The following content is for architects, and developers who want to understand the technologies and capabilities of an event driven architecture. Understand the Key EDA concepts like events, event streams, events and messages differences... Be confortable with the EDA reference architecture with event backbone, microservices and real time analytics . Which is extended with machine learning and AI integrated with real time analytics reference architecture , integrating machine learning workbench and event sourcing as data source, and real time analytics for deployment. Review Event sources - as event producers article . Read the concept of Event backbone where Kafka is the main implementation . As kafka is the event backbone, review its key concepts and building blocks and then review how to support high availability (HA) and disaster recovery (DR) with IBM Event Streams or Kafka Architecture Considerations . Review one of the common industry use case using Kafka, to perform data replication using kafka and Change Data Capture .","title":"1- Reading: Understand EDA fundamentals (8 hours)"},{"location":"training/eda-skill-journey/#2-hands-on-lab-getting-started-with-event-streams-and-kafka-3-hours","text":"Start by creating an Event Stream service in IBM Cloud by performing the IBM Event Stream Getting started . To provision your service, go to the IBM Cloud Catalog and search for Event Streams . It is in the Integration category. Create the service and specify a name, a region/location (select the same as your cluster), and a resource group, add a tag if you want to, then select the standard plan. Warning If you are using a non default resource group, you need to be sure your userid as editor role to the resource group to be able to create service under the resource group. See this note for creating Event Stream with Enterprise plan. Review another getting started from Apache Kafka quickstart Finally review the Confluent point of view on getting started as it covers some of the persona involved in a kafka based solution.","title":"2- Hands on Lab: Getting started with Event Streams and Kafka (3 hours)"},{"location":"training/eda-skill-journey/#3-readings-methodology-2-hours-read","text":"It is important to understand how to start a project using event. This lab persents how to start an event driven solution implementation using the event storming and domain driven design: We are proposing a set of activities to develop and release an event-driven solution based on the agile development approach described in the IBM Garage method. The adopted approach for event identification is the Event storming methodology introduced by Alberto Brandolini in \"Introducing event storming book\". We have extended it with the event insight practice used to identify potential real time analytics use cases. Once the event storming deliver events, commands, aggregates we can start doing some Domain Design Driven exercises and apply it to the business application. For those of you, who are interested to know how to apply the event storming and domain driven design methodology to the Reefer shipment process, you can review the following articles: The solution introduction to get a sense of the goals of this application. (10 minutes read) followed by the event storming analysis report (30 minutes read). and the derived design from this analysis. (15 minutes reading)","title":"3 - Readings : Methodology (2 hours read)"},{"location":"training/eda-skill-journey/#hands-on-labs","text":"As next steps beyond getting started and reading our technical point of view, you can try different hands-on exercises based on our \"Reefer container shipment solution\" . This application is quite complex and includes different components. You do not need to do all, but you should get a good understanding of all those component implementation as most of the code and approach is reusable for your future implementation. Note At the end of this training you should have the following solution up and running (See detailed description here ): You can run the solution locally, on IBM Cloud Private, on IBM Kubernetes Services or Openshift.","title":"Hands-on labs"},{"location":"training/eda-skill-journey/#4-hands-on-lab-prepare-your-local-environment-30-mn","text":"Goals Install Kafka - zookeeper and postgresql docker images and start them in docker-compose or minikube environment First be sure to complete the pre-requisites by following those steps . Then do one of the following choice: To run a local Kafka / zookeeper backbone using docker compose, in less than 3 minutes follow the steps described in this note . Or use Minikube/ docker kubernetes to get kafka, zookeeper and postgreSQl up and running on a unique node kubernetes cluster.","title":"4 - Hands on lab: Prepare your local environment (30 mn)"},{"location":"training/eda-skill-journey/#5-hands-on-lab-prepare-ibm-cloud-iks-openshift-environment","text":"You can run the solution locally, but you can also deploy it to a kubernetes cluster. So this lab is more optional or you can do it later when you are familar with the solution. If you want to build and run the solution locally go to step 8 below. So if you are interested by a public cloud deployment, you can follow this tutorial: \"Creating an IBM Cloud Red Hat OpenShift Container Platform cluster\" . Be sure to have administration privilege, within your account, to be able to create cluster. It will take less than 30 minutes to get the cluster provisioned. You can follow the steps to create the cluster with the console or use the IBM Cloud CLI. Be sure to be logged to your IBM Cloud account: ibmcloud login - a https : // cloud . ibm . com - u < userid > - p < password > - c < accoundID > Get the private and public vlan IP address for your zone: ibmcloud ks vlans --zone wdc06 It will return something like ID Name Number Type Router Supports Virtual Workers < private_VLAN_ID to keep secret > 2445 private bcr01a . wdc06 true < public_VLAN_ID to keep secret > 1305 public fcr01a . wdc06 true Create a 3 nodes kubernetes cluster using the small hardware footprint, and openshift 3.11 image: ibmcloud ks cluster - create --name greencluster --location wdc06 --kube-version 3.11_openshift --machine-type b3c.4x16.encrypted --workers 3 --public-vlan <public_VLAN_ID> --private-vlan <private_VLAN_ID> Verify your cluster once created: ibmcloud ks cluster - get --cluster greencluster Retrieving cluster greencluster ... OK Name : greencluster ID : < keep it secret > State : normal Created : 2019 - 07 - 16 T20 : 47 : 34 + 0000 Location : wdc06 Master URL : https : //< secret_too > . us - east . containers . cloud . ibm . com : 21070 Public Service Endpoint URL : https : //< secret_too > . us - east . containers . cloud . ibm . com : 21070 Private Service Endpoint URL : - Master Location : Washington D . C . Master Status : Ready ( 2 days ago ) Master State : deployed Master Health : normal Ingress Subdomain : greencluster . us - east . containers . appdomain . cloud Ingress Secret : greencluster Workers : 3 Worker Zones : wdc06 Version : 3 . 11 . 104 _1507_openshift Owner : < secret_too > Monitoring Dashboard : - Resource Group ID : < secret_too > Resource Group Name : default Download the configuration files to connect to your cluster ibmcloud ks cluster - config --cluster greencluster Then export the KUBECONFIG variable. export KUBECONFIG =/ Users /< you on your computer >/ . bluemix / plugins / container - service / clusters / greencluster / kube - config - wdc06 - greencluster . yml Now any oc command will work against your remote cluster. Access the Openshift container platform console using the master URL Something like: https:// .us-east.containers.cloud.ibm.com:21070","title":"5 - Hands on lab: Prepare IBM Cloud IKS Openshift environment"},{"location":"training/eda-skill-journey/#6-hands-on-lab-create-kafka-topics-and-get-service-credentials","text":"From the Event Stream services you created in Lab 2 go to the service credentials page, create new credentials to get the Kafka brokers list, the admim URL and the api_key needed to authenticate the consumer or producer code. The Event streams broker API key is needed to connect any consumers or producers to access the service in IBM Cloud. When those producers and consumers are deployed to kubernetes, the way to share security keys, is to define a kubernetes secret and deploy it to the IKS cluster. Define a Event Stream API key secret: to configure a secret under the greencompute namespace. For kubernetes platform: kubectl create secret generic eventstreams-apikey --from-literal = binding = '<replace with api key>' -n greencompute For Openshift use the name of the project as namespace kubectl create secret generic eventstreams - apikey --from-literal=binding='<replace with api key>' -n reefer-shipment-solution Verify the secrets: kubectl describe secrets - n greencompute or for openshift using project: kubectl describe secrets - n reefer - shipment - solution This secret is used by all the solution microservices which are using Kafka / Event Streams. The detail of how we use it with environment variables, is described in one of the project here. Finally in the Manage panel add the topics needed for the solution. We need at least the following: Note In your local deployment the kafka topics are created automatically using the launch script.","title":"6 - Hands on lab: Create Kafka topics and get service credentials"},{"location":"training/eda-skill-journey/#7-hands-on-lab-get-a-simple-getting-started-event-producer-deployed-on-openshift-with-event-stream","text":"Note For local deployment, smoke test scripts helps to show the event produced and consumed. To validate the IBM Event Stream and the openshift app are communicating, we use a simple order producer app we have developed as part of the Reefer container solution. It is done with python and can be found in this repository . We use Openshift source to image workflow to deploy this app to Openshift. Follow the steps in the readme as part of the lab. There are other tools to use to quickly","title":"7 - Hands on lab: Get a simple getting started event producer deployed on openshift with Event Stream"},{"location":"training/eda-skill-journey/#8-hands-on-lab-build-and-run-the-solution-locally","text":"Goals Build and run the solution so you can understand the Java-maven, Nodejs build process with docker stage build. Build and deploy the solution locally using docker compose","title":"8 - Hands on lab: Build and run the solution locally"},{"location":"training/eda-skill-journey/#9-hands-on-perform-tests","text":"","title":"9 - Hands on: Perform tests"},{"location":"training/eda-skill-journey/#perform-the-smoke-tests-locally","text":"Finally, run the smoke tests to assess all the components are running well. For that in the refarch-kc project run the script: For docker-compose: . / scripts / smokeTests . sh LOCAL for MINIKUBE: . / scripts / smokeTests . sh MINIKUBE You should see an Order created for the \"GoodManuf\" customer. Then the order is visible in the command and the query microservices. Warning To stop docker-compose deployment use the following command: . / scripts / stopLocalEnv . sh LOCAL or for the minikube stopLocalEnv . sh MINIKUBE","title":"Perform the smoke tests locally"},{"location":"training/eda-skill-journey/#optional-execute-integration-tests-on-the-local-environment","text":"Execute the integration tests to validate the solution end to end.","title":"Optional: Execute integration tests on the local environment"},{"location":"training/eda-skill-journey/#optional-execute-the-demonstration-script","text":"Execute the demonstration script","title":"Optional: Execute the demonstration script"},{"location":"training/eda-skill-journey/#10-reading-review-the-cqrs-patterns-implementation","text":"Read Event driven design patterns for microservice with the Command Query Responsability Segregation, event sourcing and saga patterns. Review the Event sourcing design pattern explanations and how it is tested with some integration tests: Review the CQRS pattern . Review the CQRS code in the order management microservice implementation Kafka Python API and some examples in our integration tests project Kafka Nodejs API used in the voyage microservice","title":"10 - Reading: Review the CQRS patterns implementation"},{"location":"training/eda-skill-journey/#lab-11-run-the-solution-on-ibm-cloud","text":"Deploying the solution on IBM Cloud Kubernetes Service Perform smokeTests locally on the solution running on IKS.","title":"Lab 11: Run the solution on IBM Cloud"},{"location":"training/eda-skill-journey/#lab-12-optional-run-the-solution-on-ibm-cloud-private","text":"Deploying the solution on IBM Cloud Private","title":"Lab 12: Optional - Run the solution on IBM Cloud Private"},{"location":"training/eda-skill-journey/#lab-13-data-replication-with-kafka","text":"One of the common usage of using Kafka is to combine it with a Change Data Capture component to get update from a \"legacy\" data base to the new microservice runtime environment. We are detailing an approach in this article .","title":"Lab 13: Data replication with Kafka"},{"location":"training/eda-skill-journey/#lab-14-real-time-analytics-and-machine-learning","text":"IBM Cloud Streaming Analytics introduction and getting started Apply predictive analytics on container metrics for predictive maintenance use case","title":"Lab 14: Real time analytics and Machine learning"},{"location":"training/eda-skill-journey/#other-subjects","text":"Develop a toolchain for one of the container manager service Our Kubernetes troubleshooting notes Kafka monitoring IBM Event Streams - stream analytics app Event detection on continuous feed using Streaming Analytics in IBM Cloud. Read how to process continuous streaming events Event-driven cloud-native applications The Cloud Private IBM Event Streams product running on private cloud Read introduction to act on events with IBM Cloud Functions","title":"Other subjects"},{"location":"training/eda-skill-journey/#slack-channel","text":"Contact us on '#eda-ac` channel under the ibmcase.slack.com workspace.","title":"Slack channel"}]}